<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.56" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://yuanchaofa.com/post/kimi-k2-and-kimi-k2-thinking-notes.html"><meta property="og:site_name" content="chaofa用代码打点酱油"><meta property="og:title" content="Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）"><meta property="og:description" content="深度解读 Kimi K2 和 K2 Thinking 技术细节：MuonClip 优化方案、大规模 Agentic 数据合成 pipeline、通用强化学习的 Self-Judging 机制，以及 200-300 步工具调用的 Test-Time Scaling。从预训练到后训练，揭秘月之暗面如何打造 SOTA 开源 Thinking 模型。"><meta property="og:type" content="article"><meta property="og:image" content="https://cfcdn.yuanchaofa.com/blog/2025/20251109144342.png"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-11-10T16:30:20.000Z"><meta property="article:tag" content="LLM"><meta property="article:tag" content="paper"><meta property="article:published_time" content="2025-11-09T17:26:00.000Z"><meta property="article:modified_time" content="2025-11-10T16:30:20.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）","image":["https://cfcdn.yuanchaofa.com/blog/2025/20251109144342.png","https://cfcdn.yuanchaofa.com/blog/2025/20251109161151.png","https://cfcdn.yuanchaofa.com/blog/2025/20251109163028.png","https://cfcdn.yuanchaofa.com/blog/2025/20251109175053.png","https://cfcdn.yuanchaofa.com/blog/2025/20251109163203.png","https://cfcdn.yuanchaofa.com/blog/2025/20251109164955.png","https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png"],"datePublished":"2025-11-09T17:26:00.000Z","dateModified":"2025-11-10T16:30:20.000Z","author":[{"@type":"Person","name":"Chaofa Yuan","url":"https://yuanchaofa.com"}]}</script><meta name="baidu-site-verification" content="codeva-y7Qplz9xAV"><meta name="360-site-verification" content="d65e0e26fb7ffa7c147867834f4d1475"><meta http-equiv="Content-Type" content="text/html;charset=gb2312"><meta name="sogou_site_verification" content="sS60nRna6W"><meta name="google-adsense-account" content="ca-pub-6733138658650037"><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6733138658650037" async crossorigin="anonymous"></script><link rel="alternate" type="application/rss+xml" href="https://yuanchaofa.com/rss.xml" title="chaofa用代码打点酱油 RSS Feed"><link rel="icon" href="/img/icon.webp"><title>Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等） | chaofa用代码打点酱油</title><meta name="description" content="深度解读 Kimi K2 和 K2 Thinking 技术细节：MuonClip 优化方案、大规模 Agentic 数据合成 pipeline、通用强化学习的 Self-Judging 机制，以及 200-300 步工具调用的 Test-Time Scaling。从预训练到后训练，揭秘月之暗面如何打造 SOTA 开源 Thinking 模型。">
    <link rel="preload" href="/assets/style-Bm-44Uv7.css" as="style"><link rel="stylesheet" href="/assets/style-Bm-44Uv7.css">
    <link rel="modulepreload" href="/assets/app-BSIteuMx.js"><link rel="modulepreload" href="/assets/kimi-k2-and-kimi-k2-thinking-notes.html-_a-fXtQi.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-CtTs29zS.js" as="script"><link rel="prefetch" href="/assets/about.html-BXmL-_Xl.js" as="script"><link rel="prefetch" href="/assets/blog-feed.html-BkNUgNF0.js" as="script"><link rel="prefetch" href="/assets/link.html-WmDgaaoN.js" as="script"><link rel="prefetch" href="/assets/from-self-attention-to-multi-head-self-attention.html-BkLXFV7G.js" as="script"><link rel="prefetch" href="/assets/hands-on-causallm-decoder.html-DuhuRjN3.js" as="script"><link rel="prefetch" href="/assets/hands-on-group-query-attention-and-multi-query-attention.html-64nAZyy-.js" as="script"><link rel="prefetch" href="/assets/hands-on-lora.html-DHurtTcN.js" as="script"><link rel="prefetch" href="/assets/index.html-Xmazw4LS.js" as="script"><link rel="prefetch" href="/assets/ad-nums.html-lfLWn06z.js" as="script"><link rel="prefetch" href="/assets/quick-tutorial.html-DTugzSav.js" as="script"><link rel="prefetch" href="/assets/activate-function-from-relu-gelu-to-swishglu.html-DB23pevi.js" as="script"><link rel="prefetch" href="/assets/the-way-of-moe-model-evolution.html-Bs-CVLku.js" as="script"><link rel="prefetch" href="/assets/2020-year-summary.html-UKCoc1rN.js" as="script"><link rel="prefetch" href="/assets/2024-year-summary.html-Dmj4kxFv.js" as="script"><link rel="prefetch" href="/assets/2021-year-summary.html-BMmFTij7.js" as="script"><link rel="prefetch" href="/assets/2022-year-summary.html-Cg-2jHmP.js" as="script"><link rel="prefetch" href="/assets/2023-year-summary.html-Ds0rZT2e.js" as="script"><link rel="prefetch" href="/assets/growth-strategies-for-ordinary-people-starting-from-scratch.html-BLdUNGQX.js" as="script"><link rel="prefetch" href="/assets/2025-02-month-summary.html-Bc87VuZm.js" as="script"><link rel="prefetch" href="/assets/2025-03-month-summary.html-DbujuoXF.js" as="script"><link rel="prefetch" href="/assets/2025-04-month-summary.html-DBBgEcVd.js" as="script"><link rel="prefetch" href="/assets/2025-05-month-summary.html-CR3Bc0NS.js" as="script"><link rel="prefetch" href="/assets/2025-06-month-summary.html-Cik1JrjF.js" as="script"><link rel="prefetch" href="/assets/2025-07-month-summary.html-BLNkFrW5.js" as="script"><link rel="prefetch" href="/assets/2025-08-month-summary.html-b3FXqUPG.js" as="script"><link rel="prefetch" href="/assets/2025-09-month-summary.html-gcFMwoBm.js" as="script"><link rel="prefetch" href="/assets/2025-10-month-summary.html-DpRRsPvD.js" as="script"><link rel="prefetch" href="/assets/2025-11-month-summary.html-DUDYvM5u.js" as="script"><link rel="prefetch" href="/assets/11.html-Dl-MGfgv.js" as="script"><link rel="prefetch" href="/assets/12.html-CiFnhuTc.js" as="script"><link rel="prefetch" href="/assets/13.html-Bz54FxNu.js" as="script"><link rel="prefetch" href="/assets/14.html-CDC2sARe.js" as="script"><link rel="prefetch" href="/assets/16.html-C9DNcnnQ.js" as="script"><link rel="prefetch" href="/assets/18.html-BmSCy1ye.js" as="script"><link rel="prefetch" href="/assets/19.html-CjXzh7qq.js" as="script"><link rel="prefetch" href="/assets/20.html-tWP7b3Hb.js" as="script"><link rel="prefetch" href="/assets/21.html-DQtj_Odk.js" as="script"><link rel="prefetch" href="/assets/22.html-BgpMjj2Y.js" as="script"><link rel="prefetch" href="/assets/23.html-E1sdRLMP.js" as="script"><link rel="prefetch" href="/assets/24.html-WMhZLIQj.js" as="script"><link rel="prefetch" href="/assets/bai-fei-li-shang-jin-ji.html-5KpSWa8U.js" as="script"><link rel="prefetch" href="/assets/hub-of-fu-lan-ke-yang.html-61BfpqwD.js" as="script"><link rel="prefetch" href="/assets/27.html-DPL2381a.js" as="script"><link rel="prefetch" href="/assets/blind-date-from-bruce.html-DllzjfLl.js" as="script"><link rel="prefetch" href="/assets/blind-date-from-miss-cui.html-Df5FVrzB.js" as="script"><link rel="prefetch" href="/assets/joke-with-miss-cui.html-BFIqqMoW.js" as="script"><link rel="prefetch" href="/assets/how-i-met-bruce.html-JTVaqUPd.js" as="script"><link rel="prefetch" href="/assets/life-influenced-by-point.html-BsXaEwFQ.js" as="script"><link rel="prefetch" href="/assets/2020-emnlp-submition.html-TxV8jPCv.js" as="script"><link rel="prefetch" href="/assets/ten-years-after-the-college-entrance-examination.html-Dk4fl5zA.js" as="script"><link rel="prefetch" href="/assets/how-to-keep-mental-health-working-in-bytedance.html-DM241bEk.js" as="script"><link rel="prefetch" href="/assets/talk-with-dayu.html-CsqcczgF.js" as="script"><link rel="prefetch" href="/assets/work-i-can-insist-more-time.html-Jq3cVftU.js" as="script"><link rel="prefetch" href="/assets/from-native-rag-to-agentic-rag.html-BFbxQ5cI.js" as="script"><link rel="prefetch" href="/assets/hands-on-deepseek-mla.html-DG2rZJh2.js" as="script"><link rel="prefetch" href="/assets/hands-on-deepseek-mla-projection-absorption.html-DnisVCbG.js" as="script"><link rel="prefetch" href="/assets/hands-on-dpo-direct-preference-optimization.html-zWACHUL2.js" as="script"><link rel="prefetch" href="/assets/hands-on-rope-position-embedding.html-BRF-W2t_.js" as="script"><link rel="prefetch" href="/assets/three-ways-of-deploy-deepseek-r1-and-llm.html-B-JoqaDY.js" as="script"><link rel="prefetch" href="/assets/llm-train-infer-memoery-usage-calculation.html-CiMO1tF5.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-advanced.html-BnKbdzm-.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-basic.html-D4GcA3NK.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-intermediate.html-BRTZJ7wk.js" as="script"><link rel="prefetch" href="/assets/make-flomo-better.html-D1mSemmF.js" as="script"><link rel="prefetch" href="/assets/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html-iq_0-CUW.js" as="script"><link rel="prefetch" href="/assets/deepseek-grm-paper-reading-notes.html-oB1NYdHx.js" as="script"><link rel="prefetch" href="/assets/deepseek-r1-paper-reading-notes.html-CkjqUuIe.js" as="script"><link rel="prefetch" href="/assets/gemini-2.5-tech-report-reading-note.html-CZuY7YZ8.js" as="script"><link rel="prefetch" href="/assets/kimi-k1.5-paper-reading-notes.html-5vK_wL6Q.js" as="script"><link rel="prefetch" href="/assets/raycast-tutorial-1.html-BqJ0Gel3.js" as="script"><link rel="prefetch" href="/assets/1.html-D_msQ1_O.js" as="script"><link rel="prefetch" href="/assets/10.html-BfZkLCH9.js" as="script"><link rel="prefetch" href="/assets/17.html-J0ZqpTD3.js" as="script"><link rel="prefetch" href="/assets/2.html-DCtdX5nk.js" as="script"><link rel="prefetch" href="/assets/29.html-7s7EjMOn.js" as="script"><link rel="prefetch" href="/assets/3.html-yUQSwI-G.js" as="script"><link rel="prefetch" href="/assets/5.html-Bdsu1Dwv.js" as="script"><link rel="prefetch" href="/assets/7.html-Q3Lg0_jA.js" as="script"><link rel="prefetch" href="/assets/8.html-DKC_ZlSL.js" as="script"><link rel="prefetch" href="/assets/9.html-KFaOfSCi.js" as="script"><link rel="prefetch" href="/assets/git-config-path-seperation.html-DpdHBakW.js" as="script"><link rel="prefetch" href="/assets/404.html-bRyfLzQ3.js" as="script"><link rel="prefetch" href="/assets/index.html-pvFbNMES.js" as="script"><link rel="prefetch" href="/assets/index.html-DG1dFLV6.js" as="script"><link rel="prefetch" href="/assets/index.html-DF9hQktQ.js" as="script"><link rel="prefetch" href="/assets/index.html-DePYxLE0.js" as="script"><link rel="prefetch" href="/assets/index.html-movoCPUx.js" as="script"><link rel="prefetch" href="/assets/index.html-BMbYv8_H.js" as="script"><link rel="prefetch" href="/assets/index.html-B9B4h8gs.js" as="script"><link rel="prefetch" href="/assets/index.html-vjo-yi-U.js" as="script"><link rel="prefetch" href="/assets/index.html-CH_bLri2.js" as="script"><link rel="prefetch" href="/assets/index.html-CkGmTcMj.js" as="script"><link rel="prefetch" href="/assets/index.html-CEIVNbad.js" as="script"><link rel="prefetch" href="/assets/index.html-B3YvpcGJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BKwnbFiB.js" as="script"><link rel="prefetch" href="/assets/index.html-D21AJ9RF.js" as="script"><link rel="prefetch" href="/assets/index.html-Bl-mzhL4.js" as="script"><link rel="prefetch" href="/assets/index.html-iBo14R4Z.js" as="script"><link rel="prefetch" href="/assets/index.html-DsqL7jVA.js" as="script"><link rel="prefetch" href="/assets/index.html-CrM_7mH2.js" as="script"><link rel="prefetch" href="/assets/index.html-DUsbeTsr.js" as="script"><link rel="prefetch" href="/assets/index.html-T9sm1JEl.js" as="script"><link rel="prefetch" href="/assets/index.html-BQ7Acdmu.js" as="script"><link rel="prefetch" href="/assets/index.html-PuXffTa6.js" as="script"><link rel="prefetch" href="/assets/index.html-BTWIOYCj.js" as="script"><link rel="prefetch" href="/assets/index.html-CQeTFN-H.js" as="script"><link rel="prefetch" href="/assets/index.html-DmL5zDgz.js" as="script"><link rel="prefetch" href="/assets/index.html-DxGNM2jt.js" as="script"><link rel="prefetch" href="/assets/index.html-U8s8by8l.js" as="script"><link rel="prefetch" href="/assets/index.html-C6XQCPkX.js" as="script"><link rel="prefetch" href="/assets/index.html-P7UYBMen.js" as="script"><link rel="prefetch" href="/assets/index.html-D9VWkJ6f.js" as="script"><link rel="prefetch" href="/assets/index.html-CRSEVDsh.js" as="script"><link rel="prefetch" href="/assets/index.html-Mugu_eKH.js" as="script"><link rel="prefetch" href="/assets/index.html-BBX00jHJ.js" as="script"><link rel="prefetch" href="/assets/index.html-C-KrcYIl.js" as="script"><link rel="prefetch" href="/assets/index.html-COAsFjxc.js" as="script"><link rel="prefetch" href="/assets/index.html-CQG2oOnN.js" as="script"><link rel="prefetch" href="/assets/index.html-cPmUlOj6.js" as="script"><link rel="prefetch" href="/assets/index.html-Qxm96gVq.js" as="script"><link rel="prefetch" href="/assets/index.html-DD6BRPPe.js" as="script"><link rel="prefetch" href="/assets/index.html-Clncf_WS.js" as="script"><link rel="prefetch" href="/assets/index.html-snNkmy2C.js" as="script"><link rel="prefetch" href="/assets/index.html-BRmWpWny.js" as="script"><link rel="prefetch" href="/assets/index.html-qgdk_2Pg.js" as="script"><link rel="prefetch" href="/assets/index.html-Cfx5tcm9.js" as="script"><link rel="prefetch" href="/assets/index.html-CmrIP4Lo.js" as="script"><link rel="prefetch" href="/assets/index.html-R_bx4-br.js" as="script"><link rel="prefetch" href="/assets/index.html-Cbu_iyoq.js" as="script"><link rel="prefetch" href="/assets/index.html-CM79JhQ5.js" as="script"><link rel="prefetch" href="/assets/index.html-BO0qY9YJ.js" as="script"><link rel="prefetch" href="/assets/index.html-D5J6aZaB.js" as="script"><link rel="prefetch" href="/assets/index.html-Dm5c-L5U.js" as="script"><link rel="prefetch" href="/assets/index.html-C1CGpNSD.js" as="script"><link rel="prefetch" href="/assets/index.html-ymkq3a4d.js" as="script"><link rel="prefetch" href="/assets/index.html-DSn-77Oi.js" as="script"><link rel="prefetch" href="/assets/giscus-By3eXuXR.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-GXRgw7eJ.js" as="script"><link rel="prefetch" href="/assets/SearchResult-D0F-UJ0J.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container external-link-icon pure has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/img/icon.webp" alt><!----><span class="vp-site-name hide-in-pad">chaofa用代码打点酱油</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://moacode.org/register?ref=bbruceyu" aria-label="ClaudeCode/CodeX API 代理「推荐」" rel="noopener noreferrer" target="_blank"><!---->ClaudeCode/CodeX API 代理「推荐」<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://yuanchaofa.com/llms-zero-to-hero/" aria-label="LLMs-Zero-to-Hero" rel="noopener noreferrer" target="_blank"><!---->LLMs-Zero-to-Hero<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://www.youtube.com/@bbruceyuan" aria-label="YouTube" rel="noopener noreferrer" target="_blank"><!--[--><span class="font-icon icon Youtube" style=""></span><!--]-->YouTube<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://space.bilibili.com/12420432" aria-label="B 站" rel="noopener noreferrer" target="_blank"><!---->B 站<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://yuanchaofa.com/rss.xml" aria-label="RSS订阅" rel="noopener noreferrer" target="_blank"><!---->RSS订阅<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/about.html" aria-label="关于我"><!---->关于我<!----></a></div></nav><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/bbruceyuan" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!--[--><button type="button" class="search-pro-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Data Export</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">LLM</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Paper</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/post/deepseek-grm-paper-reading-notes.html" aria-label="DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)"><!---->DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/gemini-2.5-tech-report-reading-note.html" aria-label="Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考"><!---->Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/post/kimi-k2-and-kimi-k2-thinking-notes.html" aria-label="Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）"><!---->Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/kimi-k1.5-paper-reading-notes.html" aria-label="深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的"><!---->深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html" aria-label="自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking"><!---->自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/deepseek-r1-paper-reading-notes.html" aria-label="自顶向下方式深度解读 DeepSeek-R1，内含大量细节"><!---->自顶向下方式深度解读 DeepSeek-R1，内含大量细节<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Python 类型体操</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Raycast Tutorial</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">技术</span><span class="vp-arrow end"></span></button><!----></section></li></ul><!----></aside><!--[--><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）</h1><div class="page-info"><span class="page-author-info" aria-label="作者"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://yuanchaofa.com" target="_blank" rel="noopener noreferrer">Chaofa Yuan</a></span><span property="author" content="Chaofa Yuan"></span></span><!----><span class="page-date-info" aria-label="写作日期"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年11月9日</span><meta property="datePublished" content="2025-11-09T17:26:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 16 分钟</span><meta property="timeRequired" content="PT16M"></span><span class="page-category-info" aria-label="分类"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item clickable" role="navigation">paper-reading</span><!--]--><meta property="articleSection" content="paper-reading"></span><span class="page-tag-info" aria-label="标签"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item clickable" role="navigation">LLM</span><span class="page-tag-item clickable" role="navigation">paper</span><!--]--><meta property="keywords" content="LLM,paper"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">此页内容<!----><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-背景">0. 背景</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-整体架构-从-k2-到-k2-thinking">1. 整体架构：从 K2 到 K2 Thinking</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-1-k2-open-agentic-intelligence-的基座">1.1 K2：Open Agentic Intelligence 的基座</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-2-k2-thinking-加入-test-time-scaling">1.2 K2 Thinking：加入 Test-Time Scaling</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-预训练">2. 预训练</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-1-基于-muonclip-优化器的-token-效率优化">2.1 基于 MuonClip 优化器的 Token 效率优化</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_2-1-1-为什么需要更好的优化器">2.1.1 为什么需要更好的优化器？</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_2-1-2-muonclip-直接控制-attention-logits">2.1.2 MuonClip：直接控制 Attention Logits</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_2-1-3-实验结果-零训练尖峰">2.1.3 实验结果：零训练尖峰</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-2-文本的改写优化">2.2 文本的改写优化</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_2-2-1-知识领域数据改写">2.2.1 知识领域数据改写</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_2-2-2-数学领域数据改写">2.2.2 数学领域数据改写</a></li><!----><!--]--></ul></li><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-后训练-重点">3. 后训练(重点)</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-1-大规模-agentic-数据合成-教会模型使用工具">3.1 大规模 Agentic 数据合成：教会模型使用工具</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-1-数据合成流程">3.1.1 数据合成流程</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-2-为什么这个方法有效">3.1.2 为什么这个方法有效？</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-2-通用强化学习-不可验证奖励">3.2 通用强化学习：不可验证奖励</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-2-1-self-judging-机制">3.2.1 Self-Judging 机制</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-2-2-用可验证奖励改进-critic">3.2.2 用可验证奖励改进 Critic</a></li><!----><!--]--></ul></li><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-k2-thinking">4. K2 Thinking</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-1-什么是-test-time-scaling">4.1 什么是 Test-Time Scaling？</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-2-边思考边使用工具-interleaved-reasoning">4.2 边思考边使用工具：Interleaved Reasoning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-3-简要看看-benchmark">4.3 简要看看 benchmark</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_4-3-1-agentic-search-超越人类基线">4.3.1 Agentic Search：超越人类基线</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_4-3-2-agentic-coding-构建完整的应用">4.3.2 Agentic Coding：构建完整的应用</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-4-小结">4.4 小结</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_5-技术细节对比-k2-vs-k2-thinking">5. 技术细节对比：K2 vs K2 Thinking</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_6-核心启发-我们能学到什么">6. 核心启发：我们能学到什么？</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_7-更多内容">7. 更多内容</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h2 id="_0-背景" tabindex="-1"><a class="header-anchor" href="#_0-背景"><span>0. 背景</span></a></h2><p>月之暗面发布的 <strong><a href="https://moonshotai.github.io/Kimi-K2/thinking.html" target="_blank" rel="noopener noreferrer">Kimi K2 Thinking</a></strong>，在 Humanity&#39;s Last Exam (HLE) 上达到了 44.9% 的成绩，在多个基准测试中表现优异，不过榜单简单看一眼即可；让我比较惊喜的是，K2 Thinking 可以执行 200-300 步连续的工具调用，有类似于 <code>claude</code> 一样的长程规划和自适应推理能力。</p><p>但是，K2 Thinking 的官方 blog 只展示了 benchmark 数据和 demo，并没有透露具体的技术细节。作为一个大模型从业者，看到 Twitter/知乎大家都在聊这个模型，所以我就比较好奇「模型的训练方法」以及「给我们工作学习中的启发」。</p><p>好在今年早些时候发布了 <strong>Kimi K2</strong> 的完整<a href="https://arxiv.org/abs/2507.20534" target="_blank" rel="noopener noreferrer">技术报告</a>和 <a href="https://moonshotai.github.io/Kimi-K2/" target="_blank" rel="noopener noreferrer">技术 blog</a>。而 <strong>K2 Thinking 和 K2 师出同源</strong>，只是在 K2 的基础上增加了 thinking 能力，更强的工具调用能力，通过 test-time scaling 实现一个更强的 Thinking Agent。因此，通过深入研究 K2 的技术细节，我们就能理解 K2 Thinking 是如何炼成的。</p><p>我是朝发（CHAOFA）这篇文章会从 K2 的技术报告出发，结合 K2 Thinking 的特点，了解这个 SOTA 开源 thinking 模型是怎么训出来的。<strong>核心关注三个问题</strong>：</p><ol><li><strong>预训练阶段</strong>：如何用 MuonClip 优化器实现更高的 token 效率？</li><li><strong>后训练阶段</strong>：如何通过大规模 Agentic 数据合成和通用强化学习，让模型学会使用工具？</li><li><strong>Test-Time Scaling</strong>：如何让模型在推理时进行长程思考和工具调用？</li></ol><blockquote><p>历史上此比较相关文章：</p><ul><li><a href="https://yuanchaofa.com/post/kimi-k1.5-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的</a></li><li><a href="https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">自顶向下方式深度解读 DeepSeek-R1，内含大量细节</a></li><li><a href="https://yuanchaofa.com/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html" target="_blank" rel="noopener noreferrer">自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking</a></li></ul></blockquote><blockquote><p>如果不喜欢看文字可以看视频解读，<a href="https://www.bilibili.com/video/BV1yikRBvEwy/" target="_blank" rel="noopener noreferrer">B 站-chaofa用代码打点酱油</a>和 <a href="https://www.youtube.com/@bbruceyuan" target="_blank" rel="noopener noreferrer">YouTube</a></p><p><a href="https://www.bilibili.com/video/BV1yikRBvEwy/" target="_blank" rel="noopener noreferrer">算法视角深度解读 Kimi K2 和 K2 Thinking，从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据 --bilibili</a></p></blockquote><h2 id="_1-整体架构-从-k2-到-k2-thinking" tabindex="-1"><a class="header-anchor" href="#_1-整体架构-从-k2-到-k2-thinking"><span>1. 整体架构：从 K2 到 K2 Thinking</span></a></h2><p><img src="https://cfcdn.yuanchaofa.com/blog/2025/20251109144342.png" alt="image.png|700x366" loading="lazy"></p><blockquote><p>Archiecture from: <a href="https://x.com/rasbt/status/1986511951141441648?s=20" target="_blank" rel="noopener noreferrer">Sebastian Raschka</a></p></blockquote><p>先来看一下上面的整体结构图，然后在深入技术细节之前，我们有必要先理解 K2 和 K2 Thinking 的关系。</p><h3 id="_1-1-k2-open-agentic-intelligence-的基座" tabindex="-1"><a class="header-anchor" href="#_1-1-k2-open-agentic-intelligence-的基座"><span>1.1 K2：Open Agentic Intelligence 的基座</span></a></h3><p>Kimi K2 是一个 <a href="https://yuanchaofa.com/llms-zero-to-hero/the-way-of-moe-model-evolution.html" target="_blank" rel="noopener noreferrer">MoE (Mixture-of-Experts)</a> 模型，拥有 <strong>32B 激活参数和 1T 总参数</strong>。它在非 thinking 模型中，在前沿知识、数学和编码任务上达到了 SOTA 性能。</p><p>K2 的核心特点是<strong>有比较强的 Agentic 能力</strong>。什么是 Agentic 任务？就是模型不仅要回答问题，还要主动使用工具、执行操作、完成复杂的多步骤任务。比如：</p><ul><li>用 Python 分析数据、生成可视化网页</li><li>在命令行中编辑文件、运行命令</li><li>通过搜索和浏览器收集信息、验证假设、构建答案</li></ul><p>K2 发布了两个版本：</p><ul><li>Kimi-K2-Base：基础模型，适合研究者/开发者/企业用户进行微调</li><li>Kimi-K2-Instruct：后训练模型，适合直接使用，是一个非推理模式（Non-Reasoning Model）</li></ul><h3 id="_1-2-k2-thinking-加入-test-time-scaling" tabindex="-1"><a class="header-anchor" href="#_1-2-k2-thinking-加入-test-time-scaling"><span>1.2 K2 Thinking：加入 Test-Time Scaling</span></a></h3><p>Kimi K2 Thinking 是在 K2 的基础上，通过额外的训练，让模型具备了 thinking 能力。它的核心特点是：</p><ol><li>边思考边使用工具：模型在推理过程中，会进行 <code>think → search → browse → think → code</code> 的循环，动态生成和验证假设</li><li>长程推理：可以执行 200-300 步连续的工具调用，保持推理的连贯性。（这点是让人比较惊喜的）</li><li>Test-Time Scaling：通过增加推理时的 thinking tokens 和工具调用步数，提升模型性能</li></ol><p>从架构上看，<code>K2 Thinking = K2 + Thinking Ability + Test-Time Scaling</code>。因此，<strong>理解 K2 的训练方法，就能理解 K2 Thinking 的 80%</strong>。</p><p>下面我们按照训练流程，依次讲解预训练、后训练和 test-time scaling 的关键技术。</p><h2 id="_2-预训练" tabindex="-1"><a class="header-anchor" href="#_2-预训练"><span>2. 预训练</span></a></h2><h3 id="_2-1-基于-muonclip-优化器的-token-效率优化" tabindex="-1"><a class="header-anchor" href="#_2-1-基于-muonclip-优化器的-token-效率优化"><span>2.1 基于 MuonClip 优化器的 Token 效率优化</span></a></h3><p>预训练是 Agentic Intelligence 的关键基础，它建立了让强化学习探索变得可行、高效和可泛化的先验知识。但是，正如 Ilya Sutskever 所说，数据是有限的&quot;化石燃料&quot;，其增长速度远远落后于算力的增长。这使得<strong>预训练阶段的 token 利用效率</strong>成为 AI scaling laws 中的新关键系数。</p><h4 id="_2-1-1-为什么需要更好的优化器" tabindex="-1"><a class="header-anchor" href="#_2-1-1-为什么需要更好的优化器"><span>2.1.1 为什么需要更好的优化器？</span></a></h4><p>给定一个大致有限的预训练数据集和固定的模型配置，更 token 高效的优化器能产生更多的智能。Moonshot 之前的工作 <a href="https://github.com/MoonshotAI/Moonlight" target="_blank" rel="noopener noreferrer">Moonlight</a> 已经证明，<a href="https://kellerjordan.github.io/posts/muon/" target="_blank" rel="noopener noreferrer">Muon</a> 优化器在 LLM 训练中显著优于广泛使用的 AdamW 优化器，即“相同配置训练下有更低的 loss”。</p><p>K2 的设计目标是进一步扩展 Moonlight，它采用了类似 DeepSeek-V3 的架构。基于 scaling-law 分析，他们做了两点改进（看图更清晰）：</p><ul><li>减少了 attention heads 的数量，以提高长上下文效率。</li><li>增加了 MoE 的稀疏性，以获得更高的 token 效率</li></ul><blockquote><p>原文这么写的：Based on scaling-law analysis, we reduce the number of heads for long-context efficiency, and increase MoE sparsity for greater token efficiency。</p></blockquote><p>但在扩展过程中，他们遇到了一个持续的挑战：<strong>由 attention logits 爆炸引起的训练不稳定</strong>。这个问题在使用 Muon 时更频繁，而在 AdamW 中较少。现有的解决方案（如 Qwen3 用的 query-key normalization）都不够充分（防止数值溢出）。</p><h4 id="_2-1-2-muonclip-直接控制-attention-logits" tabindex="-1"><a class="header-anchor" href="#_2-1-2-muonclip-直接控制-attention-logits"><span>2.1.2 MuonClip：直接控制 Attention Logits</span></a></h4><p>为了解决这个问题，kimi 提出了 MuonClip 优化器，它通过 <strong>qk-clip 技术</strong>改进了 Muon。</p><p><strong>核心思想</strong>：qk-clip 通过在 Muon 更新后<strong>直接重新缩放 query 和 key 投影的权重矩阵</strong>，从源头控制 attention logits 的规模，从而稳定训练。（注意：这里是更新完之后，所以不会改变这一次更新的 forward/backward 操作，影响的是下一步）。</p><p>具体来说，query 和 key 投影按如下方式缩放：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>=</mo><msup><mi>η</mi><mi>α</mi></msup><msub><mi>W</mi><mi>q</mi></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> q_i = \eta^{\alpha} W_q x_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0005em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>=</mo><msup><mi>η</mi><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup><msub><mi>W</mi><mi>k</mi></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> k_i = \eta^{1-\alpha} W_k x_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0585em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 是一个平衡超参数，因此 attention logit 变为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>η</mi><mi>α</mi></msup><msub><mi>q</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mi mathvariant="normal">⊤</mi></msup><mo stretchy="false">(</mo><msup><mi>η</mi><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup><msub><mi>k</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>η</mi><mtext> </mtext><msubsup><mi>q</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> (\eta^{\alpha} q_i)^\top (\eta^{1-\alpha} k_j) = \eta\, q_i^\top k_j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1852em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1852em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span></p><p>自适应因子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span>（阈值为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>）在每一步之后根据该步的最大 attention logit 设置：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>η</mi><mo>=</mo><mi>min</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mi>t</mi><mstyle scriptlevel="0" displaystyle="true"><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></munder><mo fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><msubsup><mi>q</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>k</mi><mi>j</mi></msub><mo fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo></mstyle></mfrac><mo separator="true">,</mo><mn>1</mn><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \eta = \min\left(\frac{t}{\displaystyle\max_{i,j}\bigl(q_i^\top k_j\bigr)}, 1\right) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.7029em;vertical-align:-1.6529em;"></span><span class="mop">min</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.875em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="3.600em" viewBox="0 0 875 3600"><path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2921em;"><span style="top:-2.2109em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3723em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8638em;"><span></span></span></span></span></span><span class="mopen"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing size1">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.6529em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.875em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="3.600em" viewBox="0 0 875 3600"><path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 是预设的阈值。这是一个通用技术，可能适用于其他稳定化场景。这里其实还有一些其他的细节，比如 每个 head 有不同的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span>。</p><h4 id="_2-1-3-实验结果-零训练尖峰" tabindex="-1"><a class="header-anchor" href="#_2-1-3-实验结果-零训练尖峰"><span>2.1.3 实验结果：零训练尖峰</span></a></h4><p>实验表明，MuonClip 有效地防止了 logit 爆炸，同时保持了下游任务性能。在实践中，K2 使用 MuonClip 在 15.5T tokens 上进行预训练，实现了零训练尖峰（zero loss spike），证明了 MuonClip 是大规模 LLM 训练的稳健解决方案。</p><p><img src="https://cfcdn.yuanchaofa.com/blog/2025/20251109161151.png" alt="image.png|700x420" loading="lazy"></p><p>从 loss 曲线可以看出，MuonClip 的训练过程非常平滑，没有出现任何不稳定的情况。这为后续的 Agentic 能力训练打下了坚实的基础。</p><p><strong>小结</strong>：MuonClip 优化器通过 qk-clip 技术，在保持 Muon 高 token 效率的同时，解决了训练不稳定问题，使得在同等条件下获得比 AdamW 更低的 loss，使得 K2 能够在有限的数据上训练出更强的基础模型。</p><h3 id="_2-2-文本的改写优化" tabindex="-1"><a class="header-anchor" href="#_2-2-文本的改写优化"><span>2.2 文本的改写优化</span></a></h3><p>K2 相比 K1.5 的一个关键进步是引入了<strong>合成数据生成策略</strong>来提高 token 利用率。核心思想是：通过精心设计的改写 pipeline，在不引入显著过拟合的情况下，扩大高质量 tokens 的数量。改写（Rephrasing） 就是数据合成的一种方式，主要是为了提高「高质量数据的占比」，尤其是「知识领域」和「数学领域」。：</p><h4 id="_2-2-1-知识领域数据改写" tabindex="-1"><a class="header-anchor" href="#_2-2-1-知识领域数据改写"><span>2.2.1 知识领域数据改写</span></a></h4><p>在知识密集型文本上进行预训练面临一个权衡：单次 epoch 不足以全面吸收知识，而多次 epoch 重复会导致收益递减并增加过拟合风险。为了提高高质量知识 tokens 的利用率，K2 提出了一个合成改写框架，每个语料库最多改写两次，包含三个关键组件：</p><p><strong>A. 风格和视角多样化的提示（Style- and perspective-diverse prompting）</strong></p><p>通过精心设计的 prompts，引导大语言模型以不同的风格和视角生成原文的忠实改写。这样做的好处是：</p><ul><li>增强语言多样性</li><li>保持事实完整性</li><li>避免简单的同义词替换</li></ul><p><strong>B. 分块自回归生成（Chunk-wise autoregressive generation）</strong></p><p>为了在长文档中保持全局连贯性并避免信息丢失，采用基于分块的自回归改写策略，一图胜千言：</p><p><img src="https://cfcdn.yuanchaofa.com/blog/2025/20251109163028.png" alt="image.png|700x344" loading="lazy"></p><p><strong>C. 保真度验证（Fidelity verification）</strong></p><p>为了确保原文和改写内容之间的一致性，进行保真度检查，比较每个改写段落与其源文本的语义对齐。这是训练前的初步质量控制步骤。</p><h4 id="_2-2-2-数学领域数据改写" tabindex="-1"><a class="header-anchor" href="#_2-2-2-数学领域数据改写"><span>2.2.2 数学领域数据改写</span></a></h4><p>为了增强数学推理能力，K2 采用了两种策略：</p><p><strong>A. &quot;学习笔记&quot;风格改写</strong></p><p>将高质量的数学文档改写成&quot;学习笔记&quot;风格，遵循 <a href="https://arxiv.org/abs/2501.01926" target="_blank" rel="noopener noreferrer">SwallowMath</a> 中引入的方法。这种风格更接近人类学习数学的方式，包含：</p><ul><li>逐步推导过程</li><li>关键概念解释</li><li>示例和练习</li></ul><p><strong>B. 多语言翻译</strong></p><p>将其他语言的高质量数学材料翻译成英语，以增加数据多样性。这样可以：</p><ul><li>利用非英语世界的优质数学资源</li><li>增加数学表达的多样性</li><li>扩大训练数据规模</li></ul><p><strong>小结</strong>：通过针对知识和数学领域的专门改写技术，K2 在不显著增加过拟合风险的情况下，大幅提高了高质量 tokens 的利用率。这种受控的数据增强策略是 K2 预训练成功的关键因素之一。</p><h2 id="_3-后训练-重点" tabindex="-1"><a class="header-anchor" href="#_3-后训练-重点"><span>3. 后训练(重点)</span></a></h2><p>K2 的增强 Agentic 能力源于两个重要方面：</p><ul><li><strong>大规模 Agentic 数据合成</strong></li><li><strong>通用强化学习</strong></li></ul><h3 id="_3-1-大规模-agentic-数据合成-教会模型使用工具" tabindex="-1"><a class="header-anchor" href="#_3-1-大规模-agentic-数据合成-教会模型使用工具"><span>3.1 大规模 Agentic 数据合成：教会模型使用工具</span></a></h3><p>为了教会模型复杂的工具使用能力，kimi 是基于<strong>大规模模拟真实世界的工具使用场景</strong>，构建了数据 pipeline。</p><h4 id="_3-1-1-数据合成流程" tabindex="-1"><a class="header-anchor" href="#_3-1-1-数据合成流程"><span>3.1.1 数据合成流程</span></a></h4><p>这个管道的核心思想是：<strong>系统地演化数百个领域，包含数千个工具</strong>（包括真实的 MCP 工具和合成工具），然后生成数百个具有不同工具集的 agents。</p><p><img src="https://cfcdn.yuanchaofa.com/blog/2025/20251109175053.png" alt="image.png|700x232" loading="lazy"></p><p>辅助看这个图：</p><p><img src="https://cfcdn.yuanchaofa.com/blog/2025/20251109163203.png" alt="image.png|700x233" loading="lazy"></p><p>具体流程如下：</p><ol><li>定义领域和工具：涵盖各种真实场景，如数据分析、网页开发、系统管理等</li><li>生成任务：所有任务都是基于 rubric 的（有明确的评分标准），确保一致的评估</li><li>模拟交互：Agents 与模拟环境和用户 agents 交互，创建真实的多轮工具使用场景</li><li>LLM 评判(LLM as judge)：根据任务 rubrics 评估模拟结果，过滤出高质量的训练数据</li></ol><p>这个可扩展的 pipeline 生成了<strong>多样化、高质量的数据</strong>，为大规模拒绝采样和强化学习铺平了道路。</p><h4 id="_3-1-2-为什么这个方法有效" tabindex="-1"><a class="header-anchor" href="#_3-1-2-为什么这个方法有效"><span>3.1.2 为什么这个方法有效？</span></a></h4><p>传统的工具使用训练依赖于人工标注的数据，成本高、规模小、多样性有限。而 k2 的方法通过<strong>自动化合成</strong>，可以：</p><ul><li>无限扩展：只要定义新的领域和工具，就能生成新的训练数据</li><li>保证质量：通过 rubric-based 评估和 LLM judge，确保数据质量</li><li>覆盖长尾场景：可以模拟各种罕见但重要的工具使用场景</li></ul><h3 id="_3-2-通用强化学习-不可验证奖励" tabindex="-1"><a class="header-anchor" href="#_3-2-通用强化学习-不可验证奖励"><span>3.2 通用强化学习：不可验证奖励</span></a></h3><p>传统的强化学习主要应用于<strong>可验证奖励</strong>的任务，比如数学题（答案对错明确）和竞赛编程（能否通过测试用例）。但对于<strong>不可验证奖励</strong>的任务（如写研究报告、创意写作），传统 RL 就无能为力了。</p><blockquote><p>是不是突然想起了 <a href="https://yuanchaofa.com/post/deepseek-grm-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-GRM（通用奖励模型）</a>。</p></blockquote><h4 id="_3-2-1-self-judging-机制" tabindex="-1"><a class="header-anchor" href="#_3-2-1-self-judging-机制"><span>3.2.1 Self-Judging 机制</span></a></h4><p>核心思想是：<strong>模型作为自己的评判者</strong>，为不可验证的任务提供可扩展的、基于 rubric 的反馈。</p><p>具体做法：</p><ol><li>对于不可验证的任务，模型生成多个候选答案</li><li>模型自己根据 rubric 评估这些答案，给出分数</li><li>使用这些分数作为奖励信号，进行强化学习</li></ol><blockquote><p>但这里有个问题：模型的自我评估准确吗？这不还是 LLM as Judge 那一套吗？</p></blockquote><h4 id="_3-2-2-用可验证奖励改进-critic" tabindex="-1"><a class="header-anchor" href="#_3-2-2-用可验证奖励改进-critic"><span>3.2.2 用可验证奖励改进 Critic</span></a></h4><p>kimi 的解决方案是：<strong>在可验证奖励的 on-policy rollouts 中，持续更新 critic</strong>，使 critic 在最新策略上不断提高评估准确性。</p><p>这可以看作是<strong>用可验证奖励来改进不可验证奖励的估计</strong>。通过这种方式，模型的自我评估能力会随着训练不断提升，从而支持更广泛的任务。</p><p><strong>小结</strong>：通过大规模 Agentic 数据合成和通用强化学习，K2 学会了在各种场景下使用工具，并且能够处理可验证和不可验证的任务。这为 K2 Thinking 的长程推理能力打下了基础。</p><h2 id="_4-k2-thinking" tabindex="-1"><a class="header-anchor" href="#_4-k2-thinking"><span>4. K2 Thinking</span></a></h2><p>K2 Thinking 在 K2 的基础上，增加了 <strong>thinking 能力</strong>、<strong>更强的工具调用能力</strong>和 <strong>test-time scaling</strong>。这使得模型能够在推理时进行长程思考和工具调用，从而解决更复杂的问题。</p><h3 id="_4-1-什么是-test-time-scaling" tabindex="-1"><a class="header-anchor" href="#_4-1-什么是-test-time-scaling"><span>4.1 什么是 Test-Time Scaling？</span></a></h3><p>Test-Time Scaling 是指<strong>在推理时增加计算量，以提升模型性能</strong>。对于 K2 Thinking，这体现在两个方面：</p><ol><li>增加 thinking tokens：模型在生成答案前，会先生成大量的思考过程（类似 OpenAI o1，这其实就是 Long-CoT，这种技术在 Kimi-k1.5 就已经开始做了）</li><li>增加工具调用步数：模型可以执行 200-300 步连续的工具调用，进行长程规划（这是新增的，为了 Agentic 能力的提升）</li></ol><p>这两者结合，使得 K2 Thinking 能够解决需要深度推理和多步操作的复杂问题。</p><h3 id="_4-2-边思考边使用工具-interleaved-reasoning" tabindex="-1"><a class="header-anchor" href="#_4-2-边思考边使用工具-interleaved-reasoning"><span>4.2 边思考边使用工具：Interleaved Reasoning</span></a></h3><p>K2 Thinking 的核心能力是<strong>边思考边使用工具</strong>。它会进行动态的 <code>think → search → browse → think → code</code> 循环，这个循环可以重复数百次，直到找到答案：</p><ol><li>Think：分析问题，生成假设</li><li>Search：搜索相关信息</li><li>Browse：浏览网页，提取关键信息</li><li>Think：验证假设，调整策略</li><li>Code：编写代码，执行计算</li></ol><h3 id="_4-3-简要看看-benchmark" tabindex="-1"><a class="header-anchor" href="#_4-3-简要看看-benchmark"><span>4.3 简要看看 benchmark</span></a></h3><h4 id="_4-3-1-agentic-search-超越人类基线" tabindex="-1"><a class="header-anchor" href="#_4-3-1-agentic-search-超越人类基线"><span>4.3.1 Agentic Search：超越人类基线</span></a></h4><p>在 BrowseComp benchmark 上，K2 Thinking 达到了 <strong>60.2%</strong> 的成绩，显著超越了 <strong>29.2%</strong> 的人类基线。</p><p>BrowseComp 是一个挺具有挑战性的 benchmark，旨在评估模型<strong>持续浏览、搜索和推理难以找到的真实世界网络信息</strong>的能力。</p><h4 id="_4-3-2-agentic-coding-构建完整的应用" tabindex="-1"><a class="header-anchor" href="#_4-3-2-agentic-coding-构建完整的应用"><span>4.3.2 Agentic Coding：构建完整的应用</span></a></h4><p>K2 Thinking 在编码任务上也表现出色：</p><p><img src="https://cfcdn.yuanchaofa.com/blog/2025/20251109164955.png" alt="image.png|700x469" loading="lazy"></p><p>从官网看，K2 Thinking 可以<strong>从单个 prompt 构建完整的应用</strong>，包括：</p><ul><li>组件密集的网站</li><li>Word 克隆应用</li><li>交互式数据分析工具</li></ul><h3 id="_4-4-小结" tabindex="-1"><a class="header-anchor" href="#_4-4-小结"><span>4.4 小结</span></a></h3><p>通过 test-time scaling，K2 Thinking 能够在推理时进行长程思考和工具调用，从而解决需要深度推理和多步操作的复杂问题。这使得它在 Agentic Reasoning、Agentic Search 和 Agentic Coding 任务上都达到了 SOTA 性能。（有点 claude 那味道了）</p><h2 id="_5-技术细节对比-k2-vs-k2-thinking" tabindex="-1"><a class="header-anchor" href="#_5-技术细节对比-k2-vs-k2-thinking"><span>5. 技术细节对比：K2 vs K2 Thinking</span></a></h2><p>让我们总结一下 K2 和 K2 Thinking 的关键区别：</p><table><thead><tr><th>维度</th><th>K2 (Instruct)</th><th>K2 Thinking</th></tr></thead><tbody><tr><td>模型类型</td><td>Non-thinking（无长思考）</td><td>Thinking model（有长思考）</td></tr><tr><td>推理方式</td><td>直接生成答案</td><td>边思考边使用工具</td></tr><tr><td>工具调用</td><td>支持，但步数有限（其实也挺好的）</td><td>200-300 步连续调用</td></tr><tr><td>Test-Time Scaling</td><td>不支持</td><td>支持（thinking tokens + 工具调用）</td></tr><tr><td>适用场景</td><td>通用对话、快速响应</td><td>复杂推理、长程规划</td></tr></tbody></table><p>从技术角度看，K2 Thinking 是在 K2 的基础上：</p><ul><li>增加了 thinking 能力：通过额外的训练，让模型学会生成长思考过程</li><li>优化了工具调用：支持更长的工具调用链，保持推理连贯性</li><li>引入了 test-time scaling：在推理时增加计算量，提升性能</li></ul><h2 id="_6-核心启发-我们能学到什么" tabindex="-1"><a class="header-anchor" href="#_6-核心启发-我们能学到什么"><span>6. 核心启发：我们能学到什么？</span></a></h2><p>从 K2 和 K2 Thinking 的技术报告中，我觉得比较重要的，可能能在我们实际业务中用上的点有：</p><ol><li>数据的改写策略，写的很详细，尤其是在做「创意写作」方面工作的同学。</li><li>Agentic 训练数据构建的 pipeline。别扯没用的，就是要「真实环境模拟运行」获取大量的 trajectory 然后用 LLM 做筛选。（以前我只想着去构造，迭代去筛选更有效）</li><li>rubric-based 评估（不过这个其实一两年前大家就在用了，为什么突然又改头换面火了一下，这个真的太考验业务敏感度和怎么使用了，能直接在 k2 这种级别的开源模型上搞出来，还是挺佩服的）</li><li>test-time scaling 还是很有必要的，梦回年初 Long-CoT，想要效果好牺牲点时间绝对是值得的。（尽管可能会导致过度生成、倾向于用工具的问题）</li></ol><blockquote><p>个人碎碎念：</p><ol><li><p>从 K1.5 发 paper 开始，就感觉 KIMI 突然开始醒悟做社区了，OpenSource 真的是比较博好感，现在中国的开源模型真的牛皮🐂🍺</p></li><li><p>好像 <code>claude</code> 在 coding 上的爆火让大家都领悟到了 <code>agentic</code> 能力的重要性。希望把 Claude 价格打下来，加油～</p></li></ol></blockquote><h2 id="_7-更多内容" tabindex="-1"><a class="header-anchor" href="#_7-更多内容"><span>7. 更多内容</span></a></h2><p>欢迎关注我，我是朝发（chaofa），基本全网同名 <a href="https://yuanchaofa.com/" target="_blank" rel="noopener noreferrer">chaofa用代码打点酱油</a></p><ul><li>公众号：<img src="https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png" alt="chaofa用代码打点酱油" loading="lazy"></li><li><a href="https://space.bilibili.com/12420432" target="_blank" rel="noopener noreferrer">B站-chaofa用代码打点酱油</a></li><li><a href="https://www.youtube.com/@bbruceyuan" target="_blank" rel="noopener noreferrer">YouTube-chaofa用代码打点酱油</a></li><li><a href="https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54" target="_blank" rel="noopener noreferrer">chaofa 的 notion 简介</a></li></ul></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/bbruceyuan/bbruceyuan.github.io/edit/main/docs/post/paper/kimi-k2-and-k2-thinking.md" aria-label="编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->编辑此页<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/11/10 16:30:20</span></div><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/post/gemini-2.5-tech-report-reading-note.html" aria-label="Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考</div></a><a class="route-link auto-link next" href="/post/kimi-k1.5-paper-reading-notes.html" aria-label="深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的<!----></div></a></nav><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!----><!--]--></main><!--]--><!--]--><!----></div><!--]--><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-BSIteuMx.js" defer></script>
  </body>
</html>

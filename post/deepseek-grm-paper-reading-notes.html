<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.56" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://yuanchaofa.com/post/deepseek-grm-paper-reading-notes.html"><meta property="og:site_name" content="chaofa用代码打点酱油"><meta property="og:title" content="DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)"><meta property="og:description" content="DeepSeek团队提出全新通用奖励模型DeepSeek-GRM，通过Self-Principled Critique Tuning（SPCT）方法实现推理时动态扩展能力。该研究突破传统规则奖励模型的局限，在角色扮演、创意写作等开放领域展现卓越性能。27B小模型效果超越340B大模型，且具备更少领域偏差。文章详解训练策略（RFT+在线强化学习）和推理优化（投票机制+元奖励引导），实验结果证实推理时扩展可显著提升效果，这是 DeepSeek-R2 的前兆吗？"><meta property="og:type" content="article"><meta property="og:image" content="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123401789.webp"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-06-21T06:46:04.000Z"><meta property="article:tag" content="LLM"><meta property="article:tag" content="paper"><meta property="article:published_time" content="2025-05-03T23:00:20.000Z"><meta property="article:modified_time" content="2025-06-21T06:46:04.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)","image":["https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123401789.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123435456.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502130525536.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503212538082.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503213929051.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503220321571.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503222004215.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503223555628.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224457640.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503225713996.webp","https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224750765.webp","https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png"],"datePublished":"2025-05-03T23:00:20.000Z","dateModified":"2025-06-21T06:46:04.000Z","author":[{"@type":"Person","name":"Chaofa Yuan","url":"https://yuanchaofa.com"}]}</script><meta name="baidu-site-verification" content="codeva-y7Qplz9xAV"><meta name="360-site-verification" content="d65e0e26fb7ffa7c147867834f4d1475"><meta http-equiv="Content-Type" content="text/html;charset=gb2312"><meta name="sogou_site_verification" content="sS60nRna6W"><meta name="google-adsense-account" content="ca-pub-6733138658650037"><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6733138658650037" async crossorigin="anonymous"></script><link rel="alternate" type="application/rss+xml" href="https://yuanchaofa.com/rss.xml" title="chaofa用代码打点酱油 RSS Feed"><link rel="icon" href="/img/icon.webp"><title>DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型) | chaofa用代码打点酱油</title><meta name="description" content="DeepSeek团队提出全新通用奖励模型DeepSeek-GRM，通过Self-Principled Critique Tuning（SPCT）方法实现推理时动态扩展能力。该研究突破传统规则奖励模型的局限，在角色扮演、创意写作等开放领域展现卓越性能。27B小模型效果超越340B大模型，且具备更少领域偏差。文章详解训练策略（RFT+在线强化学习）和推理优化（投票机制+元奖励引导），实验结果证实推理时扩展可显著提升效果，这是 DeepSeek-R2 的前兆吗？">
    <link rel="preload" href="/assets/style-Bm-44Uv7.css" as="style"><link rel="stylesheet" href="/assets/style-Bm-44Uv7.css">
    <link rel="modulepreload" href="/assets/app-Q4Ay_VEF.js"><link rel="modulepreload" href="/assets/deepseek-grm-paper-reading-notes.html-C8RMoJFv.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-DOq-GCmf.js" as="script"><link rel="prefetch" href="/assets/about.html-BEkwbPaU.js" as="script"><link rel="prefetch" href="/assets/blog-feed.html-upN0XAJP.js" as="script"><link rel="prefetch" href="/assets/link.html-B5cKYHTR.js" as="script"><link rel="prefetch" href="/assets/from-self-attention-to-multi-head-self-attention.html-DjEO4djK.js" as="script"><link rel="prefetch" href="/assets/hands-on-causallm-decoder.html-ChTK4EM2.js" as="script"><link rel="prefetch" href="/assets/hands-on-group-query-attention-and-multi-query-attention.html-CKHe3z6v.js" as="script"><link rel="prefetch" href="/assets/hands-on-lora.html-CpohPPc_.js" as="script"><link rel="prefetch" href="/assets/index.html-C0npt-nf.js" as="script"><link rel="prefetch" href="/assets/ad-nums.html-Dn15bcIe.js" as="script"><link rel="prefetch" href="/assets/quick-tutorial.html-3yFcDHOP.js" as="script"><link rel="prefetch" href="/assets/activate-function-from-relu-gelu-to-swishglu.html-O1OBG2wD.js" as="script"><link rel="prefetch" href="/assets/the-way-of-moe-model-evolution.html-CFZpjdu_.js" as="script"><link rel="prefetch" href="/assets/2020-year-summary.html-jOeANM_s.js" as="script"><link rel="prefetch" href="/assets/2024-year-summary.html-VJy5D62Q.js" as="script"><link rel="prefetch" href="/assets/2021-year-summary.html-Ca6MR6Jf.js" as="script"><link rel="prefetch" href="/assets/2022-year-summary.html-CaRGte5U.js" as="script"><link rel="prefetch" href="/assets/2023-year-summary.html-BssH5nEu.js" as="script"><link rel="prefetch" href="/assets/growth-strategies-for-ordinary-people-starting-from-scratch.html-uDEVR4LN.js" as="script"><link rel="prefetch" href="/assets/2025-02-month-summary.html-Bm_B2w-p.js" as="script"><link rel="prefetch" href="/assets/2025-03-month-summary.html-6sWpsbJW.js" as="script"><link rel="prefetch" href="/assets/2025-04-month-summary.html-2JvcDbET.js" as="script"><link rel="prefetch" href="/assets/2025-05-month-summary.html-CuYHNWt3.js" as="script"><link rel="prefetch" href="/assets/2025-06-month-summary.html-BHm4LztV.js" as="script"><link rel="prefetch" href="/assets/2025-07-month-summary.html-BvuXR2JP.js" as="script"><link rel="prefetch" href="/assets/2025-08-month-summary.html-_79cZ1OX.js" as="script"><link rel="prefetch" href="/assets/2025-09-month-summary.html-ClJqI2B8.js" as="script"><link rel="prefetch" href="/assets/11.html-tvpjAJQ1.js" as="script"><link rel="prefetch" href="/assets/12.html-BMteCZN3.js" as="script"><link rel="prefetch" href="/assets/13.html-Boaifl4I.js" as="script"><link rel="prefetch" href="/assets/14.html-BHq5VdWS.js" as="script"><link rel="prefetch" href="/assets/16.html-CBKEUKd2.js" as="script"><link rel="prefetch" href="/assets/18.html-DJMY_n7r.js" as="script"><link rel="prefetch" href="/assets/19.html-DHl75Rym.js" as="script"><link rel="prefetch" href="/assets/20.html-Duar5JvL.js" as="script"><link rel="prefetch" href="/assets/21.html-BbxbK48L.js" as="script"><link rel="prefetch" href="/assets/22.html-MObDbzar.js" as="script"><link rel="prefetch" href="/assets/23.html-C34BX9Ux.js" as="script"><link rel="prefetch" href="/assets/24.html-C9PEujEj.js" as="script"><link rel="prefetch" href="/assets/bai-fei-li-shang-jin-ji.html-_4gQwF-6.js" as="script"><link rel="prefetch" href="/assets/hub-of-fu-lan-ke-yang.html-hrOezcR8.js" as="script"><link rel="prefetch" href="/assets/27.html-CJ4Ut5z9.js" as="script"><link rel="prefetch" href="/assets/blind-date-from-bruce.html-CcPwPtIz.js" as="script"><link rel="prefetch" href="/assets/blind-date-from-miss-cui.html-DTc3fukV.js" as="script"><link rel="prefetch" href="/assets/joke-with-miss-cui.html-BzzHyfX4.js" as="script"><link rel="prefetch" href="/assets/how-i-met-bruce.html-Bb5gzUYD.js" as="script"><link rel="prefetch" href="/assets/life-influenced-by-point.html-BKsLIcXm.js" as="script"><link rel="prefetch" href="/assets/2020-emnlp-submition.html-BrpWfQQC.js" as="script"><link rel="prefetch" href="/assets/ten-years-after-the-college-entrance-examination.html-BeS6-e8B.js" as="script"><link rel="prefetch" href="/assets/how-to-keep-mental-health-working-in-bytedance.html-Cx4uygqV.js" as="script"><link rel="prefetch" href="/assets/talk-with-dayu.html-Cu0hp--K.js" as="script"><link rel="prefetch" href="/assets/work-i-can-insist-more-time.html-B11clDiS.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-advanced.html-CMg4VthC.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-basic.html-BJrIMRIl.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-intermediate.html-BxDNOT34.js" as="script"><link rel="prefetch" href="/assets/make-flomo-better.html-tdHCHhgJ.js" as="script"><link rel="prefetch" href="/assets/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html-Ddw1K9Xw.js" as="script"><link rel="prefetch" href="/assets/deepseek-r1-paper-reading-notes.html-bv_eXf6X.js" as="script"><link rel="prefetch" href="/assets/gemini-2.5-tech-report-reading-note.html-BlbDq06f.js" as="script"><link rel="prefetch" href="/assets/kimi-k1.5-paper-reading-notes.html-BH-pOU0w.js" as="script"><link rel="prefetch" href="/assets/raycast-tutorial-1.html-CBO72dzy.js" as="script"><link rel="prefetch" href="/assets/from-native-rag-to-agentic-rag.html-Aai6xSua.js" as="script"><link rel="prefetch" href="/assets/hands-on-deepseek-mla.html-Tg5-2L8k.js" as="script"><link rel="prefetch" href="/assets/hands-on-deepseek-mla-projection-absorption.html-Cld8F8bs.js" as="script"><link rel="prefetch" href="/assets/three-ways-of-deploy-deepseek-r1-and-llm.html-DWZDYTFH.js" as="script"><link rel="prefetch" href="/assets/llm-train-infer-memoery-usage-calculation.html-B93x6PUD.js" as="script"><link rel="prefetch" href="/assets/1.html-D6ilWexB.js" as="script"><link rel="prefetch" href="/assets/10.html-CxNTf62Z.js" as="script"><link rel="prefetch" href="/assets/17.html-DxMumTeF.js" as="script"><link rel="prefetch" href="/assets/2.html-BhqIMQZ8.js" as="script"><link rel="prefetch" href="/assets/29.html-Ctluh_RQ.js" as="script"><link rel="prefetch" href="/assets/3.html-DmCLZwCl.js" as="script"><link rel="prefetch" href="/assets/5.html-CWiFNMHN.js" as="script"><link rel="prefetch" href="/assets/7.html-B5a2J-xd.js" as="script"><link rel="prefetch" href="/assets/8.html-BosUEwIp.js" as="script"><link rel="prefetch" href="/assets/9.html-CFncdyCj.js" as="script"><link rel="prefetch" href="/assets/git-config-path-seperation.html-Dmu5z8IJ.js" as="script"><link rel="prefetch" href="/assets/404.html-IgW5kjn_.js" as="script"><link rel="prefetch" href="/assets/index.html-2rUA5tEI.js" as="script"><link rel="prefetch" href="/assets/index.html-Bf6lSLOs.js" as="script"><link rel="prefetch" href="/assets/index.html-CqmbhQFF.js" as="script"><link rel="prefetch" href="/assets/index.html-CK7Q0VDc.js" as="script"><link rel="prefetch" href="/assets/index.html-xJn2DF8L.js" as="script"><link rel="prefetch" href="/assets/index.html-BUmtBWzz.js" as="script"><link rel="prefetch" href="/assets/index.html-C0cr7iv9.js" as="script"><link rel="prefetch" href="/assets/index.html-CefyjE7e.js" as="script"><link rel="prefetch" href="/assets/index.html-xmJp6X6V.js" as="script"><link rel="prefetch" href="/assets/index.html-BIFgPDn2.js" as="script"><link rel="prefetch" href="/assets/index.html-BQzvqyOH.js" as="script"><link rel="prefetch" href="/assets/index.html-DuHM_Ba5.js" as="script"><link rel="prefetch" href="/assets/index.html-KLxfmFDx.js" as="script"><link rel="prefetch" href="/assets/index.html-WYrfKQjN.js" as="script"><link rel="prefetch" href="/assets/index.html-CafYf9CZ.js" as="script"><link rel="prefetch" href="/assets/index.html-BqKOIc7l.js" as="script"><link rel="prefetch" href="/assets/index.html-Ba4mhDtZ.js" as="script"><link rel="prefetch" href="/assets/index.html-C5JuIr2N.js" as="script"><link rel="prefetch" href="/assets/index.html-B5mPBcNo.js" as="script"><link rel="prefetch" href="/assets/index.html-Bqy6vmQv.js" as="script"><link rel="prefetch" href="/assets/index.html-Dh3876wJ.js" as="script"><link rel="prefetch" href="/assets/index.html-wGW0Kvhz.js" as="script"><link rel="prefetch" href="/assets/index.html-B3zd9Vg_.js" as="script"><link rel="prefetch" href="/assets/index.html-C6h8Zilr.js" as="script"><link rel="prefetch" href="/assets/index.html-CPmNGxFm.js" as="script"><link rel="prefetch" href="/assets/index.html-3gAclkfK.js" as="script"><link rel="prefetch" href="/assets/index.html-8F3gUMT1.js" as="script"><link rel="prefetch" href="/assets/index.html-DvqX5YBG.js" as="script"><link rel="prefetch" href="/assets/index.html-Ba-eW_OY.js" as="script"><link rel="prefetch" href="/assets/index.html-CUNtXCRc.js" as="script"><link rel="prefetch" href="/assets/index.html-DunvDKNb.js" as="script"><link rel="prefetch" href="/assets/index.html-KkFkyZu1.js" as="script"><link rel="prefetch" href="/assets/index.html-D-kJfKtv.js" as="script"><link rel="prefetch" href="/assets/index.html-D7vMuVXN.js" as="script"><link rel="prefetch" href="/assets/index.html-BLJvbw2-.js" as="script"><link rel="prefetch" href="/assets/index.html-CaDyXrli.js" as="script"><link rel="prefetch" href="/assets/index.html-CJ1Ule_F.js" as="script"><link rel="prefetch" href="/assets/index.html-9bJo3-tv.js" as="script"><link rel="prefetch" href="/assets/index.html-9o6yXki1.js" as="script"><link rel="prefetch" href="/assets/index.html-CYrMNsjo.js" as="script"><link rel="prefetch" href="/assets/index.html-WUg46XrP.js" as="script"><link rel="prefetch" href="/assets/index.html-DdSygBxn.js" as="script"><link rel="prefetch" href="/assets/index.html-CW0ODvLM.js" as="script"><link rel="prefetch" href="/assets/index.html-Con1Mh8u.js" as="script"><link rel="prefetch" href="/assets/index.html-BYCzsm4W.js" as="script"><link rel="prefetch" href="/assets/index.html-8qCuW6Wv.js" as="script"><link rel="prefetch" href="/assets/index.html-D_K6unYR.js" as="script"><link rel="prefetch" href="/assets/index.html-By40XwAP.js" as="script"><link rel="prefetch" href="/assets/index.html-CQW72owf.js" as="script"><link rel="prefetch" href="/assets/index.html-BsF6-bnA.js" as="script"><link rel="prefetch" href="/assets/index.html-C9rNkk3D.js" as="script"><link rel="prefetch" href="/assets/giscus-By3eXuXR.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-GXRgw7eJ.js" as="script"><link rel="prefetch" href="/assets/SearchResult-Ds3vMyae.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container external-link-icon pure has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/img/icon.webp" alt><!----><span class="vp-site-name hide-in-pad">chaofa用代码打点酱油</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://yuanchaofa.com/llms-zero-to-hero/" aria-label="LLMs-Zero-to-Hero" rel="noopener noreferrer" target="_blank"><!---->LLMs-Zero-to-Hero<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://www.youtube.com/@bbruceyuan" aria-label="YouTube" rel="noopener noreferrer" target="_blank"><!--[--><span class="font-icon icon Youtube" style=""></span><!--]-->YouTube<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://space.bilibili.com/12420432" aria-label="B 站" rel="noopener noreferrer" target="_blank"><!---->B 站<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://yuanchaofa.com/rss.xml" aria-label="RSS订阅" rel="noopener noreferrer" target="_blank"><!---->RSS订阅<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/about.html" aria-label="关于我"><!---->关于我<!----></a></div></nav><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/bbruceyuan" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!--[--><button type="button" class="search-pro-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Data Export</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Paper</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/post/deepseek-grm-paper-reading-notes.html" aria-label="DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)"><!---->DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/gemini-2.5-tech-report-reading-note.html" aria-label="Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考"><!---->Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/kimi-k1.5-paper-reading-notes.html" aria-label="深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的"><!---->深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html" aria-label="自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking"><!---->自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/deepseek-r1-paper-reading-notes.html" aria-label="自顶向下方式深度解读 DeepSeek-R1，内含大量细节"><!---->自顶向下方式深度解读 DeepSeek-R1，内含大量细节<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Python 类型体操</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Raycast Tutorial</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">大模型</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">技术</span><span class="vp-arrow end"></span></button><!----></section></li></ul><!----></aside><!--[--><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)</h1><div class="page-info"><span class="page-author-info" aria-label="作者"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://yuanchaofa.com" target="_blank" rel="noopener noreferrer">Chaofa Yuan</a></span><span property="author" content="Chaofa Yuan"></span></span><!----><span class="page-date-info" aria-label="写作日期"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年5月3日</span><meta property="datePublished" content="2025-05-03T23:00:20.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 10 分钟</span><meta property="timeRequired" content="PT10M"></span><span class="page-category-info" aria-label="分类"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item clickable" role="navigation">paper-reading</span><!--]--><meta property="articleSection" content="paper-reading"></span><span class="page-tag-info" aria-label="标签"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item clickable" role="navigation">LLM</span><span class="page-tag-item clickable" role="navigation">paper</span><!--]--><meta property="keywords" content="LLM,paper"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">此页内容<!----><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-结论-take-away">1. 结论(take away)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-前提-preliminaries">2. 前提(Preliminaries)</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-1-rm-模型训练分类">2.1 RM 模型训练分类</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-2-principle可以提升rm效果">2.2 Principle可以提升RM效果</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-self-principled-critique-tuning-spct">3. Self-Principled Critique Tuning(SPCT)</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-1-训练">3.1 训练</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-1-rft-reject-fine-tuning">3.1.1 RFT (Reject fine-tuning)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-2-online-rl">3.1.2 online RL</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-2-推理">3.2 推理</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-实验结果">4. 实验结果</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-1-主实验分析">4.1 主实验分析</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-2-inference-scaling">4.2 Inference Scaling</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-3-消融实验">4.3 消融实验</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#其他">其他</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h2 id="_1-结论-take-away" tabindex="-1"><a class="header-anchor" href="#_1-结论-take-away"><span>1. 结论(take away)</span></a></h2><p>Training Scaling 和 Inference Scaling 在 Base-Model 都取得了巨大的成功。那么在强化学习（Reinforcement Learning, RL）过程中需要的 Reward-Model（RM） 是不是也可以通过 <strong>Inference-Time Scaling 来优化 RM</strong> 呢？因此 DeepSeek 团队提出一种方法叫做：Self-Principed Critique Tuning (SPCT) 的方法来训练一个通用型的 RM（Generalist RM）。</p><p>RL 在推理模型中取得了巨大的成功，如 OpenAI 的 O系列、DeepSeek R系列（<a href="https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-R1</a>），但这些模型都采用了 Rule-Base Reward Model，因此 Reward Model 具有一定的局限性，在很多场景不够通用，因此本文的 <strong>DeepSeek-GRM 是旨在利用 SPCT 的方式来训练一个通用型的 Reward Model，并且能够很好得 Inference-Time Scale，以此得到一个在通用任务（非数学、代码等有精确 Reward）也能有很好效果的模型</strong>。 s</p><blockquote><p>[!NOTE] 本文首发于<a href="https://yuanchaofa.com/" target="_blank" rel="noopener noreferrer">chaofa用代码打点酱油</a>的个人 Blog，后续有更新会优先更新于 Blog 中，原文链接<a href="https://yuanchaofa.com/post/deepseek-grm-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)</a>，也会同步到同名<a href="https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png" target="_blank" rel="noopener noreferrer">公众号-chaofa用代码打点酱油</a>（仅同步）</p><p>如果不喜欢看文字的朋友，也可以看 <a href="https://www.bilibili.com/video/BV17cVdzTEac/" target="_blank" rel="noopener noreferrer">B站</a>、<a href="https://youtu.be/NlIKow850w8?si=r2GFKqGl4GfsJQvw" target="_blank" rel="noopener noreferrer">YouTube</a> 上的视频解读。</p></blockquote><h2 id="_2-前提-preliminaries" tabindex="-1"><a class="header-anchor" href="#_2-前提-preliminaries"><span>2. 前提(Preliminaries)</span></a></h2><h3 id="_2-1-rm-模型训练分类" tabindex="-1"><a class="header-anchor" href="#_2-1-rm-模型训练分类"><span>2.1 RM 模型训练分类</span></a></h3><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123401789.webp" alt="deepseek-grm-20250502123401789" loading="lazy"></p><p>这里的分类非常的清晰，先分成两个大类：（1）打分模式（Scoring Patterns），分为 Pointwise, Pairwise；（2）生成打分的模式：Scalar（标量数值型），Semi-Scalar（半数值型），Generative（生成式）。</p><blockquote><p>我甚至觉得这个划分是本文最重要的贡献之一</p></blockquote><p>先对输入进行区分（Scoring Patterns）包含两种类型的输入：</p><ul><li>（i）Pointwise，输入是一条样本（或多个样本），但是要对每一个样本都输出对应的分数。这里解释一下为什么 pointwise 的 Scoring Patterns 可以支持多种输入形式？原因为：训练完之后，你的输入可以是一条样本，也可以是两条样本，也可以是多条样本，而下方的 pairwise 形式的 Scoring Pattens 训练完之后，只能给成对的样本评估，如果要支持多个、单个样本，则需要其他的操作。</li><li>（ii）Pairwise，输入要是成对的样本，输入是一个值。假设是两个样本 a, b，那么输出是一个值，大于0 表示 a 好，小于0表示 b好；或者直接输出 a / b 来表示 a 好还是 b 好。</li></ul><p>然后可以对模型的输出方式（Reward Generation Paradigms）进行区分，</p><ul><li>（a）Scalar：让模型一个 Head 数出一个浮点数</li><li>（b）Semi-Scalar：先让模型一个 Head 输出一段分析（Critique），然后再用<strong>另外一个 Head</strong> 输出一个浮点数（或者直接计算某 token的 logit 值）</li><li>（c）Generative：模型只有一个 Head，这个 Head 是通过生成的方式输出 Critique 以及最终的分数，最终的分数要自己抽取出来。</li></ul><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123435456.webp" alt="deepseek-grm-20250502123435456" loading="lazy"></p><p>然后我们可以对此进行组合：</p><ul><li>(a) + (i)。没有思维链，多次推理结果都是一样的，因此没法 Inference-time Scaling。这里说的 Bradley-Terry 指的是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>y</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(y_1 &gt; y_2 | x) = \frac{r(y_1)}{r(y_1) + r(y_2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> ，因此 Loss 可以定义成 pairwise loss，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>loss</mtext><mo>=</mo><mo>−</mo><msup><mo>∑</mo><mi>N</mi></msup><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>−</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{loss} = -\sum^{N} log(\sigma(r(y_1) - r(y_2)))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">loss</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2312em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)))</span></span></span></span> ，N 指的是数据集中的样本。</li><li>(a) + (ii)，模型输出的是 &gt;0 / &lt; 0 浮点数，因此也不 Scaling，训练 loss 是 Pointwise Loss。</li><li>(b) + (i)，这里因为有 Critique 的存在，每次采样都会有不同的结果，所以可以 Scaling。</li><li>(b/c) +（ii)，可以 Scaling，但是训练完之后只能成对输入。</li><li>(c) + (i)，通过生成的方式生成【critique 和 每个样本的 Score】，然后自行解析抽取结果。</li></ul><p>但是从实际的结果结果来看，(c) + (i) 在 inference-time scaling 的效果要好于 (b) + (i)，具体见下图的绿色线（CLoud），多次采样提升不明显，所以最终采用了 (c) + (i)，也就是 PointWise-GRM。 <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502130525536.webp" alt="deepseek-grm-20250502130525536|366" loading="lazy"></p><h3 id="_2-2-principle可以提升rm效果" tabindex="-1"><a class="header-anchor" href="#_2-2-principle可以提升rm效果"><span>2.2 Principle可以提升RM效果</span></a></h3><p>前面提到过，部分领域可以有精确的规则，比如数学、代码，但是针对于一些通用的领域，比如角色扮演、写作等，评判的规则就会变得更复杂、并且通常都没有一个固定的标准答案（golden truth），因此我们可以指定一些准则（principles）来进行打分。</p><p>当然<strong>这些准则可以是模型自己生成的</strong>。下面解释一下这个表格，以 GPT4o-2024-0806为例，Gemma-2-27B-it 同理，从表格的数据可以看出：</p><ul><li>2 / 3 行对比，增加了自我增加的评估准则（principles）对于指标没有什么提升（效果差不多）。</li><li>2 / 4 行对比，通过一些过滤规则，相对于没有规则有一定的提升。3 / 4 行对比也同样说明如此。</li></ul><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503212538082.webp" alt="deepseek-grm-20250503212538082|409" loading="lazy"></p><p>上面的结果可以让我们得出结论：<strong>经过筛选的自我生成的评估准则可以提升 Reward-Model 的效果。</strong></p><h2 id="_3-self-principled-critique-tuning-spct" tabindex="-1"><a class="header-anchor" href="#_3-self-principled-critique-tuning-spct"><span>3. Self-Principled Critique Tuning(SPCT)</span></a></h2><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503213929051.webp" alt="deepseek-grm-20250503213929051" loading="lazy"></p><p>从这个图可以看出，SPCT 一共包含两个大部分部分</p><ul><li>训练 <ul><li>RFT（rejective fine-tuning）作为冷启动</li><li>rule-based online RL（reinforcement learning） 用于强化模型生成评估准则（principle）和推理批判（critique）的能力，后面都用 principle 和 critique 表示。</li></ul></li><li>推理 <ul><li>通过 inference-time scaling 的方式增加 RM 的最终能力。</li></ul></li></ul><h3 id="_3-1-训练" tabindex="-1"><a class="header-anchor" href="#_3-1-训练"><span>3.1 训练</span></a></h3><h4 id="_3-1-1-rft-reject-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_3-1-1-rft-reject-fine-tuning"><span>3.1.1 RFT (Reject fine-tuning)</span></a></h4><p>前面提到了，一个通用的 GRM 要做到输入自由，所以我们最终采用的是 Pointwise-GRM，用同一个模型生成 principle 和 critique。样本构造方式如下：</p><ul><li>给定一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 是一句话（包含模型的 instruction 和 output）。有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>r</mi><mi>l</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\{r_l\}_{l=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0331em;vertical-align:-0.2831em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span> 个 golden 结果（response 1/2 的打分），以及 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mi>l</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\{y_l\}_{l=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0331em;vertical-align:-0.2831em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span> 表示 GRM 预测的结果，这里格式样例为每一个： &quot;principle 1: xxx, principle 2: xxx。Analysis: xxxx, ，response 1 、2 FinalScore: [2, 3]&quot;，我们可以抽取出 final score，然后与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">r_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 进行对比。对于每一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 我们都要过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>R</mi><mi>F</mi><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{RFT}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">RFT</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次 GRM（因此也就会有多个结果，但是我们要过滤掉一些结果，这个过滤的过程就是我们拒绝采样的过程。</li><li>首先我们会把预测结果和 golden label 不一致的过滤</li><li>其次会过滤过于简单的样本，也就是在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>N</mi><mi>F</mi><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{NFT}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">NFT</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次采样的过程中，都和 golden label 结果一样的样本。</li><li>具体的过滤要求是，对于有多条 response 输入的样本（如上面样例中的 response 1/2 就是有 2 个 response 输入），那么最终要求人工标注的最优 response 具有最大的 score；而如果 response 只有一条，那么要求预测的奖励值等于 goden label。(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示对于第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 条 response 的 score) <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503220321571.webp" alt="deepseek-grm-20250503220321571" loading="lazy"></li><li>上面的看似美好，但是没有考虑到，GRM 有限次采样的过程中，可能无法生成符合 golden label（具体某一条response 好以及打分），因此会加入一条人工打分最大的 response 到 prompt 中，这条样本被叫做 hinted sampling，<strong>如果使用 hinted sampling 则只采样一次</strong>，实验结果发现 hinted sampling 的样本 Critique 结果更短（这样效果可能受限），因此只适合做冷启动，更多的效果提升还是需要 online RL</li></ul><h4 id="_3-1-2-online-rl" tabindex="-1"><a class="header-anchor" href="#_3-1-2-online-rl"><span>3.1.2 online RL</span></a></h4><p>强化学习部分因为使用 rule-based reward，因此细节反而相对比较简单。具体是使用 GRPO 算法做训练，输入是：一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> (instruction) 以及 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\{y_i\}_{i=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> （n 条 response），输出是：GRM 生成的 principle 和 critique，然后抽出去对应的分数，计为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，下面的公式总结起来就一句话：预测对了给 1 分，预测错了给 -1 分，没有其他的格式分。 <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503222004215.webp" alt="deepseek-grm-20250503222004215" loading="lazy"></p><p>其他细节：KL 散度约束的系数比 DeepSeek-R1 更大，然后设置 GRPO 中 rollout 次数为 4 来平衡效率和效果。</p><h3 id="_3-2-推理" tabindex="-1"><a class="header-anchor" href="#_3-2-推理"><span>3.2 推理</span></a></h3><p>SPCT 最重要的初衷是什么？是【inference-time scaling】。因此推理的时候有两个方式来 Scaling 达到提升效果的目的。</p><ul><li>Voting with Generated Rewards。具体解释为：推理的时候 sample 多条结果，那么就会有多个结果的 score，把每个 response 的结果加起来作为最终的结果。</li><li>Meta Reward Model Guided Voting。也就是说不是对多次采样的 Score 直接加起来，因为有时候 GRM 可能生成一些质量低的 principle 和 critique，而是通过训练一个 meta rewrd model 来引导投票过程。具体就是训练一个二分类，表示当前的 principle 和 critique 是否要被用于投票，也就是过滤掉一些低质量的采样结果。比如 Figure 3 中的 meta RM 就过滤掉了 2 4 两个结果，只用 1 3 用于投票。一般设置保留一半的采样结果。</li></ul><h2 id="_4-实验结果" tabindex="-1"><a class="header-anchor" href="#_4-实验结果"><span>4. 实验结果</span></a></h2><h3 id="_4-1-主实验分析" tabindex="-1"><a class="header-anchor" href="#_4-1-主实验分析"><span>4.1 主实验分析</span></a></h3><ul><li>27B的 spct-GRM模型比 340B 大模型效果还要好，并且不像 scalar 和 semi-scalar 的模型一样比较大的 bias（比如 ppe 任务，可验证奖励任务就表现更好）。</li><li>相对于 LLM-as-a-Judge 的方式，带有 spct 的GRM 因为有 principle 的生成，GRM相对更好一些。</li><li>整体上说就是：SPCT 提升了 GRM 在通用任务的评估能力，并且具有更少的领域偏差（不一定非得是可验证奖励的领域才表现好）。 <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503223555628.webp" alt="deepseek-grm-20250503223555628" loading="lazy"></li></ul><h3 id="_4-2-inference-scaling" tabindex="-1"><a class="header-anchor" href="#_4-2-inference-scaling"><span>4.2 Inference Scaling</span></a></h3><p>Inference-time Scaling，Voting 从 1 -&gt; 32，效果逐步提升，并且 MetaRM 会进一步带来效果，这个无需多说。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224457640.webp" alt="deepseek-grm-20250503224457640|445" loading="lazy"></p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503225713996.webp" alt="deepseek-grm-20250503225713996" loading="lazy"></p><h3 id="_4-3-消融实验" tabindex="-1"><a class="header-anchor" href="#_4-3-消融实验"><span>4.3 消融实验</span></a></h3><ul><li>各个组件效均有效果提升，其实比较重要的是 Princeple Generation 以及 General Instruction Data（这告诉我们混合通用数据的重要性）。 <ul><li>non-hinted sampling 更重要，比较合理，给了模型更多的探索和采样空间。hinted sampling 更多是为了防止模型训练过程中学不到东西，是保下线的东西。</li><li>最终重要的，RL 比 RFT 更重要， <strong>RL is all we need</strong>（🤣66.1 -&gt; 68.7）。</li></ul></li></ul><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224750765.webp" alt="deepseek-grm-20250503224750765|575" loading="lazy"></p><h2 id="其他" tabindex="-1"><a class="header-anchor" href="#其他"><span>其他</span></a></h2><p>最后欢迎关注我，基本全网同名 <a href="https://yuanchaofa.com/" target="_blank" rel="noopener noreferrer">chaofa用代码打点酱油</a></p><ul><li>公众号： <img src="https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png" alt="chaofa用代码打点酱油" loading="lazy"></li><li><a href="https://space.bilibili.com/12420432" target="_blank" rel="noopener noreferrer">B站-chaofa用代码打点酱油</a></li><li><a href="https://www.youtube.com/@bbruceyuan" target="_blank" rel="noopener noreferrer">YouTube-chaofa用代码打点酱油</a></li><li><a href="https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54" target="_blank" rel="noopener noreferrer">chaofa 的 notion 简介</a></li></ul></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/bbruceyuan/bbruceyuan.github.io/edit/main/docs/post/paper/deepseek-grm.md" aria-label="编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->编辑此页<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/6/21 06:46:04</span></div><!----></div></footer><nav class="vp-page-nav"><!----><a class="route-link auto-link next" href="/post/gemini-2.5-tech-report-reading-note.html" aria-label="Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考<!----></div></a></nav><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!----><!--]--></main><!--]--><!--]--><!----></div><!--]--><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-Q4Ay_VEF.js" defer></script>
  </body>
</html>

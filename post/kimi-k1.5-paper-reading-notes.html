<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.56" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://yuanchaofa.com/post/kimi-k1.5-paper-reading-notes.html"><meta property="og:site_name" content="chaofa用代码打点酱油"><meta property="og:title" content="深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的"><meta property="og:description" content="深度解读 Kimi K1.5 论文，介绍其多模态推理模型的技术原理与发展路线，涵盖预训练、监督微调、强化学习及其核心启发，提供详细的算法处理细节和数据构建方法。"><meta property="og:type" content="article"><meta property="og:image" content="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250222174441801.webp"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-11-09T09:11:59.000Z"><meta property="article:tag" content="LLM"><meta property="article:tag" content="paper"><meta property="article:published_time" content="2025-03-01T17:06:20.000Z"><meta property="article:modified_time" content="2025-11-09T09:11:59.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的","image":["https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250222174441801.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250222201009410.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301143553668.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301150759507.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301161202020.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301164738438.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301164824425.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165047951.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165154316.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165315712.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165522083.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165633911.webp","https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301170806060.webp","https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png"],"datePublished":"2025-03-01T17:06:20.000Z","dateModified":"2025-11-09T09:11:59.000Z","author":[{"@type":"Person","name":"Chaofa Yuan","url":"https://yuanchaofa.com"}]}</script><meta name="baidu-site-verification" content="codeva-y7Qplz9xAV"><meta name="360-site-verification" content="d65e0e26fb7ffa7c147867834f4d1475"><meta http-equiv="Content-Type" content="text/html;charset=gb2312"><meta name="sogou_site_verification" content="sS60nRna6W"><meta name="google-adsense-account" content="ca-pub-6733138658650037"><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6733138658650037" async crossorigin="anonymous"></script><link rel="alternate" type="application/rss+xml" href="https://yuanchaofa.com/rss.xml" title="chaofa用代码打点酱油 RSS Feed"><link rel="icon" href="/img/icon.webp"><title>深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的 | chaofa用代码打点酱油</title><meta name="description" content="深度解读 Kimi K1.5 论文，介绍其多模态推理模型的技术原理与发展路线，涵盖预训练、监督微调、强化学习及其核心启发，提供详细的算法处理细节和数据构建方法。">
    <link rel="preload" href="/assets/style-Bm-44Uv7.css" as="style"><link rel="stylesheet" href="/assets/style-Bm-44Uv7.css">
    <link rel="modulepreload" href="/assets/app-BSIteuMx.js"><link rel="modulepreload" href="/assets/kimi-k1.5-paper-reading-notes.html-5vK_wL6Q.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-CtTs29zS.js" as="script"><link rel="prefetch" href="/assets/about.html-BXmL-_Xl.js" as="script"><link rel="prefetch" href="/assets/blog-feed.html-BkNUgNF0.js" as="script"><link rel="prefetch" href="/assets/link.html-WmDgaaoN.js" as="script"><link rel="prefetch" href="/assets/from-self-attention-to-multi-head-self-attention.html-BkLXFV7G.js" as="script"><link rel="prefetch" href="/assets/hands-on-causallm-decoder.html-DuhuRjN3.js" as="script"><link rel="prefetch" href="/assets/hands-on-group-query-attention-and-multi-query-attention.html-64nAZyy-.js" as="script"><link rel="prefetch" href="/assets/hands-on-lora.html-DHurtTcN.js" as="script"><link rel="prefetch" href="/assets/index.html-Xmazw4LS.js" as="script"><link rel="prefetch" href="/assets/ad-nums.html-lfLWn06z.js" as="script"><link rel="prefetch" href="/assets/quick-tutorial.html-DTugzSav.js" as="script"><link rel="prefetch" href="/assets/activate-function-from-relu-gelu-to-swishglu.html-DB23pevi.js" as="script"><link rel="prefetch" href="/assets/the-way-of-moe-model-evolution.html-Bs-CVLku.js" as="script"><link rel="prefetch" href="/assets/2020-year-summary.html-UKCoc1rN.js" as="script"><link rel="prefetch" href="/assets/2024-year-summary.html-Dmj4kxFv.js" as="script"><link rel="prefetch" href="/assets/2021-year-summary.html-BMmFTij7.js" as="script"><link rel="prefetch" href="/assets/2022-year-summary.html-Cg-2jHmP.js" as="script"><link rel="prefetch" href="/assets/2023-year-summary.html-Ds0rZT2e.js" as="script"><link rel="prefetch" href="/assets/growth-strategies-for-ordinary-people-starting-from-scratch.html-BLdUNGQX.js" as="script"><link rel="prefetch" href="/assets/2025-02-month-summary.html-Bc87VuZm.js" as="script"><link rel="prefetch" href="/assets/2025-03-month-summary.html-DbujuoXF.js" as="script"><link rel="prefetch" href="/assets/2025-04-month-summary.html-DBBgEcVd.js" as="script"><link rel="prefetch" href="/assets/2025-05-month-summary.html-CR3Bc0NS.js" as="script"><link rel="prefetch" href="/assets/2025-06-month-summary.html-Cik1JrjF.js" as="script"><link rel="prefetch" href="/assets/2025-07-month-summary.html-BLNkFrW5.js" as="script"><link rel="prefetch" href="/assets/2025-08-month-summary.html-b3FXqUPG.js" as="script"><link rel="prefetch" href="/assets/2025-09-month-summary.html-gcFMwoBm.js" as="script"><link rel="prefetch" href="/assets/2025-10-month-summary.html-DpRRsPvD.js" as="script"><link rel="prefetch" href="/assets/2025-11-month-summary.html-DUDYvM5u.js" as="script"><link rel="prefetch" href="/assets/11.html-Dl-MGfgv.js" as="script"><link rel="prefetch" href="/assets/12.html-CiFnhuTc.js" as="script"><link rel="prefetch" href="/assets/13.html-Bz54FxNu.js" as="script"><link rel="prefetch" href="/assets/14.html-CDC2sARe.js" as="script"><link rel="prefetch" href="/assets/16.html-C9DNcnnQ.js" as="script"><link rel="prefetch" href="/assets/18.html-BmSCy1ye.js" as="script"><link rel="prefetch" href="/assets/19.html-CjXzh7qq.js" as="script"><link rel="prefetch" href="/assets/20.html-tWP7b3Hb.js" as="script"><link rel="prefetch" href="/assets/21.html-DQtj_Odk.js" as="script"><link rel="prefetch" href="/assets/22.html-BgpMjj2Y.js" as="script"><link rel="prefetch" href="/assets/23.html-E1sdRLMP.js" as="script"><link rel="prefetch" href="/assets/24.html-WMhZLIQj.js" as="script"><link rel="prefetch" href="/assets/bai-fei-li-shang-jin-ji.html-5KpSWa8U.js" as="script"><link rel="prefetch" href="/assets/hub-of-fu-lan-ke-yang.html-61BfpqwD.js" as="script"><link rel="prefetch" href="/assets/27.html-DPL2381a.js" as="script"><link rel="prefetch" href="/assets/blind-date-from-bruce.html-DllzjfLl.js" as="script"><link rel="prefetch" href="/assets/blind-date-from-miss-cui.html-Df5FVrzB.js" as="script"><link rel="prefetch" href="/assets/joke-with-miss-cui.html-BFIqqMoW.js" as="script"><link rel="prefetch" href="/assets/how-i-met-bruce.html-JTVaqUPd.js" as="script"><link rel="prefetch" href="/assets/life-influenced-by-point.html-BsXaEwFQ.js" as="script"><link rel="prefetch" href="/assets/2020-emnlp-submition.html-TxV8jPCv.js" as="script"><link rel="prefetch" href="/assets/ten-years-after-the-college-entrance-examination.html-Dk4fl5zA.js" as="script"><link rel="prefetch" href="/assets/how-to-keep-mental-health-working-in-bytedance.html-DM241bEk.js" as="script"><link rel="prefetch" href="/assets/talk-with-dayu.html-CsqcczgF.js" as="script"><link rel="prefetch" href="/assets/work-i-can-insist-more-time.html-Jq3cVftU.js" as="script"><link rel="prefetch" href="/assets/from-native-rag-to-agentic-rag.html-BFbxQ5cI.js" as="script"><link rel="prefetch" href="/assets/hands-on-deepseek-mla.html-DG2rZJh2.js" as="script"><link rel="prefetch" href="/assets/hands-on-deepseek-mla-projection-absorption.html-DnisVCbG.js" as="script"><link rel="prefetch" href="/assets/hands-on-dpo-direct-preference-optimization.html-zWACHUL2.js" as="script"><link rel="prefetch" href="/assets/hands-on-rope-position-embedding.html-BRF-W2t_.js" as="script"><link rel="prefetch" href="/assets/three-ways-of-deploy-deepseek-r1-and-llm.html-B-JoqaDY.js" as="script"><link rel="prefetch" href="/assets/llm-train-infer-memoery-usage-calculation.html-CiMO1tF5.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-advanced.html-BnKbdzm-.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-basic.html-D4GcA3NK.js" as="script"><link rel="prefetch" href="/assets/python-type-challenge-intermediate.html-BRTZJ7wk.js" as="script"><link rel="prefetch" href="/assets/make-flomo-better.html-D1mSemmF.js" as="script"><link rel="prefetch" href="/assets/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html-iq_0-CUW.js" as="script"><link rel="prefetch" href="/assets/deepseek-grm-paper-reading-notes.html-oB1NYdHx.js" as="script"><link rel="prefetch" href="/assets/deepseek-r1-paper-reading-notes.html-CkjqUuIe.js" as="script"><link rel="prefetch" href="/assets/gemini-2.5-tech-report-reading-note.html-CZuY7YZ8.js" as="script"><link rel="prefetch" href="/assets/kimi-k2-and-kimi-k2-thinking-notes.html-_a-fXtQi.js" as="script"><link rel="prefetch" href="/assets/raycast-tutorial-1.html-BqJ0Gel3.js" as="script"><link rel="prefetch" href="/assets/1.html-D_msQ1_O.js" as="script"><link rel="prefetch" href="/assets/10.html-BfZkLCH9.js" as="script"><link rel="prefetch" href="/assets/17.html-J0ZqpTD3.js" as="script"><link rel="prefetch" href="/assets/2.html-DCtdX5nk.js" as="script"><link rel="prefetch" href="/assets/29.html-7s7EjMOn.js" as="script"><link rel="prefetch" href="/assets/3.html-yUQSwI-G.js" as="script"><link rel="prefetch" href="/assets/5.html-Bdsu1Dwv.js" as="script"><link rel="prefetch" href="/assets/7.html-Q3Lg0_jA.js" as="script"><link rel="prefetch" href="/assets/8.html-DKC_ZlSL.js" as="script"><link rel="prefetch" href="/assets/9.html-KFaOfSCi.js" as="script"><link rel="prefetch" href="/assets/git-config-path-seperation.html-DpdHBakW.js" as="script"><link rel="prefetch" href="/assets/404.html-bRyfLzQ3.js" as="script"><link rel="prefetch" href="/assets/index.html-pvFbNMES.js" as="script"><link rel="prefetch" href="/assets/index.html-DG1dFLV6.js" as="script"><link rel="prefetch" href="/assets/index.html-DF9hQktQ.js" as="script"><link rel="prefetch" href="/assets/index.html-DePYxLE0.js" as="script"><link rel="prefetch" href="/assets/index.html-movoCPUx.js" as="script"><link rel="prefetch" href="/assets/index.html-BMbYv8_H.js" as="script"><link rel="prefetch" href="/assets/index.html-B9B4h8gs.js" as="script"><link rel="prefetch" href="/assets/index.html-vjo-yi-U.js" as="script"><link rel="prefetch" href="/assets/index.html-CH_bLri2.js" as="script"><link rel="prefetch" href="/assets/index.html-CkGmTcMj.js" as="script"><link rel="prefetch" href="/assets/index.html-CEIVNbad.js" as="script"><link rel="prefetch" href="/assets/index.html-B3YvpcGJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BKwnbFiB.js" as="script"><link rel="prefetch" href="/assets/index.html-D21AJ9RF.js" as="script"><link rel="prefetch" href="/assets/index.html-Bl-mzhL4.js" as="script"><link rel="prefetch" href="/assets/index.html-iBo14R4Z.js" as="script"><link rel="prefetch" href="/assets/index.html-DsqL7jVA.js" as="script"><link rel="prefetch" href="/assets/index.html-CrM_7mH2.js" as="script"><link rel="prefetch" href="/assets/index.html-DUsbeTsr.js" as="script"><link rel="prefetch" href="/assets/index.html-T9sm1JEl.js" as="script"><link rel="prefetch" href="/assets/index.html-BQ7Acdmu.js" as="script"><link rel="prefetch" href="/assets/index.html-PuXffTa6.js" as="script"><link rel="prefetch" href="/assets/index.html-BTWIOYCj.js" as="script"><link rel="prefetch" href="/assets/index.html-CQeTFN-H.js" as="script"><link rel="prefetch" href="/assets/index.html-DmL5zDgz.js" as="script"><link rel="prefetch" href="/assets/index.html-DxGNM2jt.js" as="script"><link rel="prefetch" href="/assets/index.html-U8s8by8l.js" as="script"><link rel="prefetch" href="/assets/index.html-C6XQCPkX.js" as="script"><link rel="prefetch" href="/assets/index.html-P7UYBMen.js" as="script"><link rel="prefetch" href="/assets/index.html-D9VWkJ6f.js" as="script"><link rel="prefetch" href="/assets/index.html-CRSEVDsh.js" as="script"><link rel="prefetch" href="/assets/index.html-Mugu_eKH.js" as="script"><link rel="prefetch" href="/assets/index.html-BBX00jHJ.js" as="script"><link rel="prefetch" href="/assets/index.html-C-KrcYIl.js" as="script"><link rel="prefetch" href="/assets/index.html-COAsFjxc.js" as="script"><link rel="prefetch" href="/assets/index.html-CQG2oOnN.js" as="script"><link rel="prefetch" href="/assets/index.html-cPmUlOj6.js" as="script"><link rel="prefetch" href="/assets/index.html-Qxm96gVq.js" as="script"><link rel="prefetch" href="/assets/index.html-DD6BRPPe.js" as="script"><link rel="prefetch" href="/assets/index.html-Clncf_WS.js" as="script"><link rel="prefetch" href="/assets/index.html-snNkmy2C.js" as="script"><link rel="prefetch" href="/assets/index.html-BRmWpWny.js" as="script"><link rel="prefetch" href="/assets/index.html-qgdk_2Pg.js" as="script"><link rel="prefetch" href="/assets/index.html-Cfx5tcm9.js" as="script"><link rel="prefetch" href="/assets/index.html-CmrIP4Lo.js" as="script"><link rel="prefetch" href="/assets/index.html-R_bx4-br.js" as="script"><link rel="prefetch" href="/assets/index.html-Cbu_iyoq.js" as="script"><link rel="prefetch" href="/assets/index.html-CM79JhQ5.js" as="script"><link rel="prefetch" href="/assets/index.html-BO0qY9YJ.js" as="script"><link rel="prefetch" href="/assets/index.html-D5J6aZaB.js" as="script"><link rel="prefetch" href="/assets/index.html-Dm5c-L5U.js" as="script"><link rel="prefetch" href="/assets/index.html-C1CGpNSD.js" as="script"><link rel="prefetch" href="/assets/index.html-ymkq3a4d.js" as="script"><link rel="prefetch" href="/assets/index.html-DSn-77Oi.js" as="script"><link rel="prefetch" href="/assets/giscus-By3eXuXR.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-GXRgw7eJ.js" as="script"><link rel="prefetch" href="/assets/SearchResult-D0F-UJ0J.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container external-link-icon pure has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/img/icon.webp" alt><!----><span class="vp-site-name hide-in-pad">chaofa用代码打点酱油</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://moacode.org/register?ref=bbruceyu" aria-label="ClaudeCode/CodeX API 代理「推荐」" rel="noopener noreferrer" target="_blank"><!---->ClaudeCode/CodeX API 代理「推荐」<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://yuanchaofa.com/llms-zero-to-hero/" aria-label="LLMs-Zero-to-Hero" rel="noopener noreferrer" target="_blank"><!---->LLMs-Zero-to-Hero<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://www.youtube.com/@bbruceyuan" aria-label="YouTube" rel="noopener noreferrer" target="_blank"><!--[--><span class="font-icon icon Youtube" style=""></span><!--]-->YouTube<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://space.bilibili.com/12420432" aria-label="B 站" rel="noopener noreferrer" target="_blank"><!---->B 站<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://yuanchaofa.com/rss.xml" aria-label="RSS订阅" rel="noopener noreferrer" target="_blank"><!---->RSS订阅<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/about.html" aria-label="关于我"><!---->关于我<!----></a></div></nav><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/bbruceyuan" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><!--[--><button type="button" class="search-pro-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Data Export</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">LLM</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Paper</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/post/deepseek-grm-paper-reading-notes.html" aria-label="DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)"><!---->DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/gemini-2.5-tech-report-reading-note.html" aria-label="Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考"><!---->Gemini 2.5 Pro 是怎么炼成的？-- gemini 2.5 技术报告阅读笔记与思考<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/kimi-k2-and-kimi-k2-thinking-notes.html" aria-label="Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）"><!---->Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/post/kimi-k1.5-paper-reading-notes.html" aria-label="深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的"><!---->深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html" aria-label="自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking"><!---->自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/post/deepseek-r1-paper-reading-notes.html" aria-label="自顶向下方式深度解读 DeepSeek-R1，内含大量细节"><!---->自顶向下方式深度解读 DeepSeek-R1，内含大量细节<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Python 类型体操</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Raycast Tutorial</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">技术</span><span class="vp-arrow end"></span></button><!----></section></li></ul><!----></aside><!--[--><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的</h1><div class="page-info"><span class="page-author-info" aria-label="作者"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://yuanchaofa.com" target="_blank" rel="noopener noreferrer">Chaofa Yuan</a></span><span property="author" content="Chaofa Yuan"></span></span><!----><span class="page-date-info" aria-label="写作日期"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年3月1日</span><meta property="datePublished" content="2025-03-01T17:06:20.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 19 分钟</span><meta property="timeRequired" content="PT19M"></span><span class="page-category-info" aria-label="分类"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item clickable" role="navigation">paper-reading</span><!--]--><meta property="articleSection" content="paper-reading"></span><span class="page-tag-info" aria-label="标签"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item clickable" role="navigation">LLM</span><span class="page-tag-item clickable" role="navigation">paper</span><!--]--><meta property="keywords" content="LLM,paper"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">此页内容<!----><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-背景">0. 背景</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-整体架构">1. 整体架构</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-预训练">2. 预训练</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-sft-训练">3. SFT 训练</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-1-常规的-sft">3.1 常规的 SFT</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-1-数据构建">3.1.1 数据构建</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-2-数据分布">3.1.2 数据分布</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_3-1-3-训练细节">3.1.3 训练细节</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-2-long-cot-sft-重点1">3.2. Long-CoT SFT（重点1）</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-强化学习-重点2">4. 强化学习（重点2）</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-1-rl-数据集构建">4.1 RL 数据集构建</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-2-问题定义">4.2 问题定义</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-3-策略优化">4.3 策略优化</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-4-采样策略">4.4 采样策略</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-5-long2short-重点3">4.5 Long2short （重点3）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-6-其他细节">4.6 其他细节</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_4-6-1-代码">4.6.1 代码</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_4-6-2-数学">4.6.2  数学</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_4-6-3-视觉数据">4.6.3 视觉数据</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level4" href="#_4-6-4-rl-框架中的优化">4.6.4  RL 框架中的优化</a></li><!----><!--]--></ul></li><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_5-实验结论">5. 实验结论</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_5-1-主要实验">5.1 主要实验</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_5-2-自我进化实验">5.2 自我进化实验</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_5-3-课程学习效果">5.3 课程学习效果</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_5-4-负样本梯度是否有用">5.4 负样本梯度是否有用</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#其他">其他</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h2 id="_0-背景" tabindex="-1"><a class="header-anchor" href="#_0-背景"><span>0. 背景</span></a></h2><p>OpenAI O系列（reasoning）模型发布后（2024-09-12），全世界都在尝试解决 OpenAI 设置的谜题，而在同一时间（2025-01-20），Kimi-k1.5 和 deepseek-r1 都发布了自己的推理模型。并且都达到 openai-o1 模型的效果。这是两个出色的解题人，值得敬佩。</p><p>尽管 Kimi K1.5 效果很好，而且还是<strong>多模态</strong>的模型，但从网络声量上看，完全被 DeepSeek-R1 盖过去了，只能说时也命也。但事实上，对于科员人员以及算法工程是从业人员，以及想找算法工作的学生来说，<a href="https://arxiv.org/abs/2501.12599" target="_blank" rel="noopener noreferrer">Kimi k1.5 的技术报告</a>确实更值得 follow 的，核心是里面有更多算法处理的细节，这些细节是我们可以在工作中直接 follow 使用的，而 DeepSeek R1 的 paper 中还有很多细节没有透露，从我的上一篇文章<a href="https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">《自顶向下方式深度解读 DeepSeek-R1》</a>中就提到几个我们想知道却没说的关键点，Kimi k1.5 给了我们答案，让我们继续看下去，<strong>怎么处理数据能够让模型可以有更强的 Reasoning 能力</strong>。</p><blockquote><p>想看视频解读的可以看：<a href="https://www.bilibili.com/video/BV1uR9JYjEAp/" target="_blank" rel="noopener noreferrer">深度解读 Kimi K1.5 paper，算法工程师复现必读文章之一，了解 RL 数据如何筛选（B 站/YouTube/视频号-chaofa用代码打点酱油）</a></p></blockquote><h2 id="_1-整体架构" tabindex="-1"><a class="header-anchor" href="#_1-整体架构"><span>1. 整体架构</span></a></h2><p>Kimi K1.5 整体采用了和 chatGPT 一样的流程，先 pre-training，然后做 SFT，然后是强化学习。可以用<a href="https://www.zhihu.com/people/muyaostudio" target="_blank" rel="noopener noreferrer">知乎大佬木尧</a>画的一张图讲清楚。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250222174441801.webp" alt="kimi-k1.5-paper-reading-20250222174441801" loading="lazy"></p><p>本文不会按照原始 Paper 的流程解读，而是按照自己的理解，讲解每一个步骤中重要的细节，方便我们在工作和学习中 follow。</p><h2 id="_2-预训练" tabindex="-1"><a class="header-anchor" href="#_2-预训练"><span>2. 预训练</span></a></h2><p>kimi k1.5 基础模型，也就是我们常说的 base 模型，是在多样化、高质量的多模态语料库上进行训练的。语言数据涵盖五个领域：英语、中文、代码、数学推理和知识。多模态数据，包括图像描述、图文交错<sup class="footnote-ref"><a href="#footnote1">[1]</a><a class="footnote-anchor" id="footnote-ref1"></a></sup>、OCR数据、知识以及问答数据集，使我们的模型能够获得视觉 - 语言能力。严格的质量控制确保了整个预训练数据集的相关性、多样性和平衡性。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250222201009410.webp" alt="kimi-k1.5-paper-reading-20250222201009410" loading="lazy"></p><p>预训练分三个阶段进行</p><ul><li>第一阶段是视觉 - 语言预训练，在此阶段先训练语言基础（LLM），随后逐步进行多模态整合； <ul><li>数据处理方面很重要，一般来说就是规则过滤、去重、FastText 分类等，细节可以查看原始论文附录B，和其他论文说的预训练处理数据方式比较类似。</li><li>训练的方式也可以看一下 <ul><li>先训练语言模型，得到一个 LLM，然后再加入图文数据让模型获得 vison-language 的能力。其中 vison tower 是独立训练的，训练过程中不会更新 LLM 部分的参数。</li><li>然后慢慢的增加图文交织数据，从 0 -&gt; 30%，这个时候是需要更新 LLM 部分的参数。</li></ul></li></ul></li><li>第二阶段是冷却阶段，利用精心挑选的和合成的数据巩固模型能力，尤其针对推理和基于知识的任务；（注意这个时候也是有 vison-language 数据的） <ul><li>数据来自于两部分，精选的预训练子集，合成生成的内容。</li><li>具体做法是利用现有的数学、知识和代码语料库作为源材料，通过一个专有语言模型生成问答对，并采用拒绝采样技术来维持质量标准。</li><li>所以这也是为什么现在很多的 base 模型就已经有了基础的指令遵循能力。</li></ul></li><li>第三阶段是长上下文激活阶段，将序列处理能力扩展到 131,072 个 Token。 <ul><li>我们都知道，很早之前，kimi 就以长上下文能力著称，这里也是揭示了它们是怎么训练的，一共两个要点。 <ul><li>过采样 long-context 数据，其中 40% 采用全注意力，60% 采用 partial attention。（这个算法似乎没有讲是啥，没有引用我不太清楚具体是什么算法）</li><li>逐步训练，先 4k -&gt; 32k -&gt; 128k，这个倒是属于常规操作了。RoPE 中的频率设置为 1,000,000（上下文越长，一般设置会更大一些）。</li></ul></li></ul></li></ul><p>基于上面的方式，我们得到了 <code>kimi-k1.5-base</code> 模型。</p><h2 id="_3-sft-训练" tabindex="-1"><a class="header-anchor" href="#_3-sft-训练"><span>3. SFT 训练</span></a></h2><h3 id="_3-1-常规的-sft" tabindex="-1"><a class="header-anchor" href="#_3-1-常规的-sft"><span>3.1 常规的 SFT</span></a></h3><h4 id="_3-1-1-数据构建" tabindex="-1"><a class="header-anchor" href="#_3-1-1-数据构建"><span>3.1.1 数据构建</span></a></h4><p>数据集包含多个领域的基础监督微调（SFT）语料库。</p><ul><li>对于非推理任务，包括问答、写作和文本处理，首先通过人工标注构建一个种子数据集。这个种子数据集用于训练一个种子模型。随后收集各种各样的提示，并使用种子模型针对每个提示生成多个回复。接着，标注人员对这些回复进行排序，并对排名最高的回复进行优化，以得出最终版本。</li><li>对于诸如数学和编码问题这类推理任务，基于规则和基于奖励建模的验证比人工判断更准确、更高效，这里也同样利用了拒绝采样来扩充监督微调数据集。</li></ul><h4 id="_3-1-2-数据分布" tabindex="-1"><a class="header-anchor" href="#_3-1-2-数据分布"><span>3.1.2 数据分布</span></a></h4><p>SFT 数据集包含大约 100 万个样本。</p><ul><li>50万个示例用于一般性问答</li><li>20万个用于编码</li><li>20万个用于数学和科学领域</li><li>5000个用于创意写作</li><li>2万个用于长上下文任务，比如总结、文档问答、翻译以及写作等。</li><li>100万个文本-视觉示例，涵盖了各类范畴，包括图表解读、光学字符识别（OCR）、基于图像的对话、视觉编码、视觉推理，以及借助视觉辅助的数学/科学问题。</li></ul><h4 id="_3-1-3-训练细节" tabindex="-1"><a class="header-anchor" href="#_3-1-3-训练细节"><span>3.1.3 训练细节</span></a></h4><ul><li>epoch 1，序列长度设置为 32k，learning rate 从 2e-5 -&gt; 2e-6</li><li>epoch 2，序列长度设置为 128k，这里 lr 重新升高到 1e-5，然后再降到 1e-6。</li><li>为了训练效率，不同的 QA pair 是 pack 在一起的。 <ul><li>我有一个疑问：按理说不同的 sample 的 Attention 是分开的，具体可以看 Github issue，但如果 Attention 是分开的，那么这里的 Sequence length 设置这么长的意义是什么？</li><li>如果有好的解释，欢迎留言讨论。</li></ul></li></ul><h3 id="_3-2-long-cot-sft-重点1" tabindex="-1"><a class="header-anchor" href="#_3-2-long-cot-sft-重点1"><span>3.2. Long-CoT SFT（重点1）</span></a></h3><p>假设我们有了一个【精心挑选的数据集（具体见 4.1 小结）】，我们从中挑出一小部分，这些数据集是一些 Question（包含文本和图片），那么我们可以通过 Prompt Engineering 的方式生成答案，这部分数据是为了 Long-CoT 能力获得（其中 <a href="https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-R1 coldstart 数据</a> 也是这么处理的）。</p><p>这部分数据需要包含人类的思考过程：规划（生成执行大纲），评估（对中间步骤进行评估），反思（可以重新思考和修正方法/步骤）以及探索（鼓励尝试新的方法和思路），而这一步的 SFT 本质上和 3.1 中 SFT 是一样的，核心差别在于数据不一样，这部分数据是的 answer 部分比较长，并且蕴含了人类的思考过程。</p><h2 id="_4-强化学习-重点2" tabindex="-1"><a class="header-anchor" href="#_4-强化学习-重点2"><span>4. 强化学习（重点2）</span></a></h2><h3 id="_4-1-rl-数据集构建" tabindex="-1"><a class="header-anchor" href="#_4-1-rl-数据集构建"><span>4.1 RL 数据集构建</span></a></h3><p>我们都知道好的数据对于 RL 提升模型效果具有很重要的帮助，因此 kimi 定义了三个关键属性来衡量数据集的好坏。</p><ul><li>多样性。数据集应该具有很丰富的来源，包含自然科学、代码、推理等各种任务，这样可以帮助模型掌握各种各样的能力。</li><li>难度平衡。好的数据集应该平衡好简单、中等以及复杂的问题，并且通过课程学习的方式可以让模型更好地掌握复杂推理的能力，并且减少过拟合。</li><li>是否可以精确评估。在 <a href="https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-R1 的 paper 解读</a>中也提到了，RL 的数据需要可以精确地评估，这样可以更好地指引模型推理。</li></ul><p>这些东西除了第三点精确评估，都是以前（k1.5 和 r1出来之前）就经常提到的，但是具体怎么做的呢？</p><ul><li>关于多样性。 <ul><li>首先当然是数据原来要有不同的领域，尤其是这些数据最好是推理相关的问题并且要易于评估。然后 kimi 会构建一个标签系统给所有的数据打标来保证多样性（但是这里的标签系统是每个公司的核心资产，显然是不会说的）。</li></ul></li><li>关于难度平衡。 <ul><li>对问题进行难度分级，分级方式为：用一个 high temperature 回复问题，重复十次，计算通过率，通过率越高难度越小，否则难度越高。</li><li>这里采用课程学习的方式，先学简单的，然后逐步学习难得样本，这样可以更好的激发和对齐模型的能力。</li><li>但是这里有一个<strong>细节</strong><ul><li>对难度进行分级应该不是一次完成的，应该是每次都会用最新的 checkpoint 进行计算一次，这样算出来的难度分级才是有意义的。不然开始用很弱的模型选出来的难样本不能适应最新模型的能力。</li></ul></li></ul></li><li>关于精确评估。 <ul><li>我们要保证答案是可以精确评估的，这是核心宪法。</li><li>但是一些问题很难，可能答案形式很简单，比如选择题，模型很容易猜测，这种问题应该要被移除。最终移除了：多选题、对错题和给出证明的题，包括但不限于这些。</li><li>此外，Prompt 可能也存在 hack 的情况（模型并没有真正推理而得出了正确答案），我们要移除掉那些比较容易 hack 的 prompt。做法是不使用 CoT 的思考过程，运行 N（8） 次都对了，那么就认为是容易 hack 的 prompt。</li></ul></li></ul><h3 id="_4-2-问题定义" tabindex="-1"><a class="header-anchor" href="#_4-2-问题定义"><span>4.2 问题定义</span></a></h3><p>在论文的原文（2.3.1）中，写了一大段关于问题的定义，具有很多公式，但这些公式都不重要，核心就是一个点。当模型的 Reasoning 过程足够长的，<strong>我们可以假设</strong>这个思考过程（reasoning tokens）蕴含着巨大的搜索空间，这个搜索空间可以帮助模型到达最终的正确答案。因此最终变成了一个优化问题，公式为：</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301143553668.webp" alt="kimi-k1.5-paper-reading-20250301143553668" loading="lazy"></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 是 reward 评估函数，针对有固定答案的问题，用规则进行评估，对了就是 1，错了就是 0。针对于自由形式的问答，就用一个奖励模型预测答案是否和事实匹配。在这个公式中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 是问题输入，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 是模型预测答案，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∗</mo></mrow><annotation encoding="application/x-tex">y*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∗</span></span></span></span> 是正确答案。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是原始的模型，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">D</mi></mrow><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span></span> 是训练数据，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span></span> 是模型输出的思考过程。</p><h3 id="_4-3-策略优化" tabindex="-1"><a class="header-anchor" href="#_4-3-策略优化"><span>4.3 策略优化</span></a></h3><p>策略优化叫做 policy optimization（policy 就是我们要训练的模型）。这一段比较难写，我就直接截图原文，并且来解释一下这里是什么意思？</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301150759507.webp" alt="kimi-k1.5-paper-reading-20250301150759507" loading="lazy"></p><p>首先，公式 2 中，是告诉我们，我们优化的目标是什么，但是这样我们没法做训练。因此慢慢推导出公式 3，也就是说把 RL 期望最大化的东西变成 Loss，这样就可以用梯度上升的方式进行优化。</p><p>解释几个关键点：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>∗</mo></mrow><annotation encoding="application/x-tex">\pi*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mord">∗</span></span></span></span> 是最优策略，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi>θ</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\theta_{i}}(y, z | x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 表示当前模型输入为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 的情况下，具有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo separator="true">,</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">{y, z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> 的概率。在公式 2 下面第一个公式就是我们要优化的目标，对这个公式取对数就可以得到公式 2 下面的第二个公式，那么其实希望的时候两者尽量接近，因此设计一个平方的损失函数，让它尽量小，得到了公式2 下面的第三个公式，然后针对这个公式求导，就可以得到公式3。更详细的解释见 <a href="https://space.bilibili.com/12420432" target="_blank" rel="noopener noreferrer">bilibili</a> 或者 <a href="https://www.youtube.com/@bbruceyuan" target="_blank" rel="noopener noreferrer">YouTube</a> 视频。</p><p>这里还有另外一个重要的点，和 deepseek-r1 的 GRPO 算法一样，移除了 Value Model，核心的思考是这些错误的路径也是有助于模型获得思考的能力。</p><p>此外，在这样的训练过程中，我们观测到了模型会有过度思考的现象，因此又设计了一个长度惩罚的奖励给 RL 模型。公式很简单，就是越长奖励越小。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301161202020.webp" alt="kimi-k1.5-paper-reading-20250301161202020" loading="lazy"></p><h3 id="_4-4-采样策略" tabindex="-1"><a class="header-anchor" href="#_4-4-采样策略"><span>4.4 采样策略</span></a></h3><p>这里用的是 off-policy 的方式做训练，所以采样策略很重要，更好的采样方法可以得到更好的效果。这里有两种采样策略。<strong>课程采样</strong>和<strong>优先采样策略</strong>。</p><ul><li>课程采样。先从较简单的任务开始训练，逐渐过渡到更具挑战性的任务。由于前面构建的prompt set已经有了难易标签，所以课程采样比较容易实现。</li><li><strong>优先采样策略</strong>，让训练专注于模型表现不佳的问题上。这里跟踪每个问题 i 的成功率 si，并按 1 − si 的比例采样问题，这样成功率较低的问题获得更高的采样概率，模型就可以重点学习短板。</li></ul><h3 id="_4-5-long2short-重点3" tabindex="-1"><a class="header-anchor" href="#_4-5-long2short-重点3"><span>4.5 Long2short （重点3）</span></a></h3><p>kimi-k1.5 的 Long2Short，实现了长链推理（Long-CoT）到短链推理（Short-CoT）的高效迁移，既保留深度思考能力，又显著提升响应速度。具体做法有下面几个点</p><ul><li>模型融合。这里说的是权重融合，long/short 模型的权重直接融合。<strong>这种在业界已经用的很多了，可以在业务中尝试起来，而且不用训练</strong>。</li><li>最短拒绝采样。一次生成 x 条样本，把最短的那条选出来，前提是结果要对。</li><li>长短样本的 DPO。和上面类似（一条正样本），这里是构建的是正负 pair 样本（两条样本），短而正确的作为正样本，错误的是负样本或者1.5长于短样本的作为负样本。</li><li>long2short强化学习。在一阶段 RL 之后，使用长度惩罚来减少模型生成的长度。</li></ul><h3 id="_4-6-其他细节" tabindex="-1"><a class="header-anchor" href="#_4-6-其他细节"><span>4.6 其他细节</span></a></h3><h4 id="_4-6-1-代码" tabindex="-1"><a class="header-anchor" href="#_4-6-1-代码"><span>4.6.1 代码</span></a></h4><p>由于网络上的许多编程问题没有可用的测试用例，我们设计了一种方法来自动生成测试用例，这些测试用例将作为奖励，用于通过强化学习来训练我们的模型。我们主要关注那些不需要特殊评测程序的问题。我们还假定这些问题有正确的标准答案可用，这样我们就可以利用这些答案来生成更高质量的测试用例。</p><p>我们利用了被广泛认可的测试用例生成库 CYaRon1 来改进我们的方法。我们使用我们的基础模型 Kimi k1.5，根据问题描述来生成测试用例。CYaRon 的使用说明和问题描述会作为输入提供给生成器。</p><p>对于每个问题，我们首先使用生成器生成 50 个测试用例，并且为每个测试用例随机抽取 10 个正确的提交答案。我们针对这些提交答案运行测试用例。如果 10 个提交答案中至少有 7 个能得出匹配的结果，那么这个测试用例就被认为是有效的。</p><p>在这一轮筛选之后，我们得到了一组选定的测试用例。如果 10 个提交答案中至少有 9 个能通过这一整组选定的测试用例，那么这个问题及其相关的选定测试用例就会被添加到我们的训练集中。</p><p>从统计数据来看，在 1000 个在线竞赛问题的样本中，大约有 614 个问题不需要特殊评测程序。我们开发了 463 个测试用例生成器，这些生成器至少生成了 40 个有效的测试用例，最终使得 323 个问题被纳入了我们的训练集。</p><h4 id="_4-6-2-数学" tabindex="-1"><a class="header-anchor" href="#_4-6-2-数学"><span>4.6.2 数学</span></a></h4><p>在评估数学问题的解答时，一个挑战在于不同的书写形式可能表示相同的本质答案。例如，a2−4 和 (a+2)(a−2) 可能都是同一道题的正确答案。我们采用了两种方法来提高奖励模型的评分准确性：</p><ol><li><strong>经典奖励模型（Classic RM）</strong>：该模型最终将 “问题”、“参考答案” 和 “作答” 作为输入，并输出一个单一的标量，用以表明作答是否正确。</li><li><strong>思维链奖励模型（Chain-of-Thought RM）</strong>：整体和上面一样，采样了 800k 的带有 CoT 和 label 的数据，然后训练一个 CoT 模型，会在输出答案之前输出思考过程。当然没有疑问的，CoT 模型效果更好。这里也是 4.2 中提到的 RM。</li></ol><h4 id="_4-6-3-视觉数据" tabindex="-1"><a class="header-anchor" href="#_4-6-3-视觉数据"><span>4.6.3 视觉数据</span></a></h4><p>为了提升模型在现实世界中的图像推理能力，并使视觉输入与大型语言模型（LLMs）更有效地对齐，我们的视觉强化学习（Vision RL）数据主要来源于三个不同的类别：真实世界数据、合成视觉推理数据以及文本渲染数据。</p><ol><li><strong>真实世界数据</strong>：涵盖了一系列不同年级水平的科学问题，这些问题需要具备图形理解和推理能力；还包括需要视觉感知和推断的地点猜测任务，以及涉及理解复杂图表的数据分析等多种类型的数据。这些数据集提升了模型在现实世界场景中进行视觉推理的能力。</li><li><strong>合成视觉推理数据</strong>：是人工生成的，包括通过程序创建的图像和场景，旨在提高特定的视觉推理技能，例如理解空间关系、几何图案以及物体间的相互作用。这些合成数据集为测试模型的视觉推理能力提供了一个可控的环境，并能提供源源不断的训练示例。</li><li><strong>文本渲染数据</strong>：通过将文本内容转换为视觉格式而创建，使模型在处理不同模态下基于文本的查询时能够保持一致性。通过将文本文档、代码片段和结构化数据转换为图像，我们确保无论输入是纯文本还是渲染成图像（如屏幕截图或照片）的文本，模型都能给出一致的回复。这也有助于增强模型处理文字密集型图像的能力。 每一类数据对于构建一个全面的视觉语言模型都至关重要，该模型能够有效地处理广泛的现实世界应用，同时确保在各种输入模态下都能保持稳定的性能。</li></ol><h4 id="_4-6-4-rl-框架中的优化" tabindex="-1"><a class="header-anchor" href="#_4-6-4-rl-框架中的优化"><span>4.6.4 RL 框架中的优化</span></a></h4><p>由于我对于强化学习和推理相关的优化了解不多，主要是两个策略，一个是框架上的优化，train 和 推理用不同的框架。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301164738438.webp" alt="kimi-k1.5-paper-reading-20250301164738438" loading="lazy"></p><p>第二个 Rollout 采样上的优化，目标是为了更有效的利用数据。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301164824425.webp" alt="kimi-k1.5-paper-reading-20250301164824425" loading="lazy"></p><h2 id="_5-实验结论" tabindex="-1"><a class="header-anchor" href="#_5-实验结论"><span>5. 实验结论</span></a></h2><p>一起看一下比较有意思的实验结论。</p><h3 id="_5-1-主要实验" tabindex="-1"><a class="header-anchor" href="#_5-1-主要实验"><span>5.1 主要实验</span></a></h3><p>在各个主要的数据集上都表现的很好，这个基本毫无疑问，并且还是首个追上 openai-o1 的多模态大模型。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165047951.webp" alt="kimi-k1.5-paper-reading-20250301165047951" loading="lazy"></p><p>第二点，long2short 之后的 Short 模型表现也很好。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165154316.webp" alt="kimi-k1.5-paper-reading-20250301165154316" loading="lazy"></p><h3 id="_5-2-自我进化实验" tabindex="-1"><a class="header-anchor" href="#_5-2-自我进化实验"><span>5.2 自我进化实验</span></a></h3><p>随着模型训练，模型自发的输出越来越长的 CoT，并且模型效果越好。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165315712.webp" alt="kimi-k1.5-paper-reading-20250301165315712" loading="lazy"></p><p>模型输出的 CoT 越长，效果越好，并且模型越大，提升的幅度（斜率）越大。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165522083.webp" alt="kimi-k1.5-paper-reading-20250301165522083" loading="lazy"></p><h3 id="_5-3-课程学习效果" tabindex="-1"><a class="header-anchor" href="#_5-3-课程学习效果"><span>5.3 课程学习效果</span></a></h3><p>采用课程学习的采样策略，可以提升模型的能力。蓝色线很快就饱和了，但是橙色线还在继续提升。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301165633911.webp" alt="kimi-k1.5-paper-reading-20250301165633911" loading="lazy"></p><h3 id="_5-4-负样本梯度是否有用" tabindex="-1"><a class="header-anchor" href="#_5-4-负样本梯度是否有用"><span>5.4 负样本梯度是否有用</span></a></h3><p>结合 <a href="#4.3">4.3</a> 中的公式的定义，里面有一项 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>−</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(r - \hat{r})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ，其中小于平均值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 就是负样本。而 ReFT 是一个只用正样本，也就是只挑选好的样本的一个方法，最终结果是显示负样本也是有好处的。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/kimi-k1.5-paper-reading-20250301170806060.webp" alt="kimi-k1.5-paper-reading-20250301170806060" loading="lazy"></p><h2 id="其他" tabindex="-1"><a class="header-anchor" href="#其他"><span>其他</span></a></h2><p>最后欢迎关注我，基本全网同名 <a href="https://yuanchaofa.com/" target="_blank" rel="noopener noreferrer">chaofa用代码打点酱油</a></p><ul><li>公众号： <img src="https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png" alt="chaofa用代码打点酱油" loading="lazy"></li><li><a href="https://space.bilibili.com/12420432" target="_blank" rel="noopener noreferrer">B站-chaofa用代码打点酱油</a></li><li><a href="https://www.youtube.com/@bbruceyuan" target="_blank" rel="noopener noreferrer">YouTube-chaofa用代码打点酱油</a></li><li><a href="https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54" target="_blank" rel="noopener noreferrer">chaofa 的 notion 简介</a></li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="footnote1" class="footnote-item"><p>一张图像后面紧接着一段对该图像内容进行描述的文本，然后再是下一张图像及对应的文本。例如，当文本提到 “一只猫在沙发上睡觉” 时，搭配相应的图像，模型可以更准确地理解 “猫”“沙发” 的具体形态以及 “睡觉” 这一动作的实际表现，从而更深入地理解文本语义。 <a href="#footnote-ref1" class="footnote-backref">↩︎</a></p></li></ol></section></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/bbruceyuan/bbruceyuan.github.io/edit/main/docs/post/paper/kimi-k1.5.md" aria-label="编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->编辑此页<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/11/9 09:11:59</span></div><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/post/kimi-k2-and-kimi-k2-thinking-notes.html" aria-label="Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）</div></a><a class="route-link auto-link next" href="/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html" aria-label="自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaptThinking<!----></div></a></nav><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!----><!--]--></main><!--]--><!--]--><!----></div><!--]--><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-BSIteuMx.js" defer></script>
  </body>
</html>

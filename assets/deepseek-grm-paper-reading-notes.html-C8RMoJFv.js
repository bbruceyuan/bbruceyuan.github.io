import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as e,o as t}from"./app-Q4Ay_VEF.js";const p={};function l(i,s){return t(),n("div",null,s[0]||(s[0]=[e('<h2 id="_1-结论-take-away" tabindex="-1"><a class="header-anchor" href="#_1-结论-take-away"><span>1. 结论(take away)</span></a></h2><p>Training Scaling 和 Inference Scaling 在 Base-Model 都取得了巨大的成功。那么在强化学习（Reinforcement Learning, RL）过程中需要的 Reward-Model（RM） 是不是也可以通过 <strong>Inference-Time Scaling 来优化 RM</strong> 呢？因此 DeepSeek 团队提出一种方法叫做：Self-Principed Critique Tuning (SPCT) 的方法来训练一个通用型的 RM（Generalist RM）。</p><p>RL 在推理模型中取得了巨大的成功，如 OpenAI 的 O系列、DeepSeek R系列（<a href="https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-R1</a>），但这些模型都采用了 Rule-Base Reward Model，因此 Reward Model 具有一定的局限性，在很多场景不够通用，因此本文的 <strong>DeepSeek-GRM 是旨在利用 SPCT 的方式来训练一个通用型的 Reward Model，并且能够很好得 Inference-Time Scale，以此得到一个在通用任务（非数学、代码等有精确 Reward）也能有很好效果的模型</strong>。 s</p><blockquote><p>[!NOTE] 本文首发于<a href="https://yuanchaofa.com/" target="_blank" rel="noopener noreferrer">chaofa用代码打点酱油</a>的个人 Blog，后续有更新会优先更新于 Blog 中，原文链接<a href="https://yuanchaofa.com/post/deepseek-grm-paper-reading-notes.html" target="_blank" rel="noopener noreferrer">DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)</a>，也会同步到同名<a href="https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png" target="_blank" rel="noopener noreferrer">公众号-chaofa用代码打点酱油</a>（仅同步）</p><p>如果不喜欢看文字的朋友，也可以看 <a href="https://www.bilibili.com/video/BV17cVdzTEac/" target="_blank" rel="noopener noreferrer">B站</a>、<a href="https://youtu.be/NlIKow850w8?si=r2GFKqGl4GfsJQvw" target="_blank" rel="noopener noreferrer">YouTube</a> 上的视频解读。</p></blockquote><h2 id="_2-前提-preliminaries" tabindex="-1"><a class="header-anchor" href="#_2-前提-preliminaries"><span>2. 前提(Preliminaries)</span></a></h2><h3 id="_2-1-rm-模型训练分类" tabindex="-1"><a class="header-anchor" href="#_2-1-rm-模型训练分类"><span>2.1 RM 模型训练分类</span></a></h3><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123401789.webp" alt="deepseek-grm-20250502123401789" loading="lazy"></p><p>这里的分类非常的清晰，先分成两个大类：（1）打分模式（Scoring Patterns），分为 Pointwise, Pairwise；（2）生成打分的模式：Scalar（标量数值型），Semi-Scalar（半数值型），Generative（生成式）。</p><blockquote><p>我甚至觉得这个划分是本文最重要的贡献之一</p></blockquote><p>先对输入进行区分（Scoring Patterns）包含两种类型的输入：</p><ul><li>（i）Pointwise，输入是一条样本（或多个样本），但是要对每一个样本都输出对应的分数。这里解释一下为什么 pointwise 的 Scoring Patterns 可以支持多种输入形式？原因为：训练完之后，你的输入可以是一条样本，也可以是两条样本，也可以是多条样本，而下方的 pairwise 形式的 Scoring Pattens 训练完之后，只能给成对的样本评估，如果要支持多个、单个样本，则需要其他的操作。</li><li>（ii）Pairwise，输入要是成对的样本，输入是一个值。假设是两个样本 a, b，那么输出是一个值，大于0 表示 a 好，小于0表示 b好；或者直接输出 a / b 来表示 a 好还是 b 好。</li></ul><p>然后可以对模型的输出方式（Reward Generation Paradigms）进行区分，</p><ul><li>（a）Scalar：让模型一个 Head 数出一个浮点数</li><li>（b）Semi-Scalar：先让模型一个 Head 输出一段分析（Critique），然后再用<strong>另外一个 Head</strong> 输出一个浮点数（或者直接计算某 token的 logit 值）</li><li>（c）Generative：模型只有一个 Head，这个 Head 是通过生成的方式输出 Critique 以及最终的分数，最终的分数要自己抽取出来。</li></ul><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123435456.webp" alt="deepseek-grm-20250502123435456" loading="lazy"></p><p>然后我们可以对此进行组合：</p><ul><li>(a) + (i)。没有思维链，多次推理结果都是一样的，因此没法 Inference-time Scaling。这里说的 Bradley-Terry 指的是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>y</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(y_1 &gt; y_2 | x) = \\frac{r(y_1)}{r(y_1) + r(y_2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> ，因此 Loss 可以定义成 pairwise loss，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>loss</mtext><mo>=</mo><mo>−</mo><msup><mo>∑</mo><mi>N</mi></msup><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>−</mo><mi>r</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\text{loss} = -\\sum^{N} log(\\sigma(r(y_1) - r(y_2)))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">loss</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2312em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)))</span></span></span></span> ，N 指的是数据集中的样本。</li><li>(a) + (ii)，模型输出的是 &gt;0 / &lt; 0 浮点数，因此也不 Scaling，训练 loss 是 Pointwise Loss。</li><li>(b) + (i)，这里因为有 Critique 的存在，每次采样都会有不同的结果，所以可以 Scaling。</li><li>(b/c) +（ii)，可以 Scaling，但是训练完之后只能成对输入。</li><li>(c) + (i)，通过生成的方式生成【critique 和 每个样本的 Score】，然后自行解析抽取结果。</li></ul><p>但是从实际的结果结果来看，(c) + (i) 在 inference-time scaling 的效果要好于 (b) + (i)，具体见下图的绿色线（CLoud），多次采样提升不明显，所以最终采用了 (c) + (i)，也就是 PointWise-GRM。 <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502130525536.webp" alt="deepseek-grm-20250502130525536|366" loading="lazy"></p><h3 id="_2-2-principle可以提升rm效果" tabindex="-1"><a class="header-anchor" href="#_2-2-principle可以提升rm效果"><span>2.2 Principle可以提升RM效果</span></a></h3><p>前面提到过，部分领域可以有精确的规则，比如数学、代码，但是针对于一些通用的领域，比如角色扮演、写作等，评判的规则就会变得更复杂、并且通常都没有一个固定的标准答案（golden truth），因此我们可以指定一些准则（principles）来进行打分。</p><p>当然<strong>这些准则可以是模型自己生成的</strong>。下面解释一下这个表格，以 GPT4o-2024-0806为例，Gemma-2-27B-it 同理，从表格的数据可以看出：</p><ul><li>2 / 3 行对比，增加了自我增加的评估准则（principles）对于指标没有什么提升（效果差不多）。</li><li>2 / 4 行对比，通过一些过滤规则，相对于没有规则有一定的提升。3 / 4 行对比也同样说明如此。</li></ul><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503212538082.webp" alt="deepseek-grm-20250503212538082|409" loading="lazy"></p><p>上面的结果可以让我们得出结论：<strong>经过筛选的自我生成的评估准则可以提升 Reward-Model 的效果。</strong></p><h2 id="_3-self-principled-critique-tuning-spct" tabindex="-1"><a class="header-anchor" href="#_3-self-principled-critique-tuning-spct"><span>3. Self-Principled Critique Tuning(SPCT)</span></a></h2><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503213929051.webp" alt="deepseek-grm-20250503213929051" loading="lazy"></p><p>从这个图可以看出，SPCT 一共包含两个大部分部分</p><ul><li>训练 <ul><li>RFT（rejective fine-tuning）作为冷启动</li><li>rule-based online RL（reinforcement learning） 用于强化模型生成评估准则（principle）和推理批判（critique）的能力，后面都用 principle 和 critique 表示。</li></ul></li><li>推理 <ul><li>通过 inference-time scaling 的方式增加 RM 的最终能力。</li></ul></li></ul><h3 id="_3-1-训练" tabindex="-1"><a class="header-anchor" href="#_3-1-训练"><span>3.1 训练</span></a></h3><h4 id="_3-1-1-rft-reject-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_3-1-1-rft-reject-fine-tuning"><span>3.1.1 RFT (Reject fine-tuning)</span></a></h4><p>前面提到了，一个通用的 GRM 要做到输入自由，所以我们最终采用的是 Pointwise-GRM，用同一个模型生成 principle 和 critique。样本构造方式如下：</p><ul><li>给定一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 是一句话（包含模型的 instruction 和 output）。有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>r</mi><mi>l</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\\{r_l\\}_{l=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0331em;vertical-align:-0.2831em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span> 个 golden 结果（response 1/2 的打分），以及 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mi>l</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\\{y_l\\}_{l=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0331em;vertical-align:-0.2831em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span> 表示 GRM 预测的结果，这里格式样例为每一个： &quot;principle 1: xxx, principle 2: xxx。Analysis: xxxx, ，response 1 、2 FinalScore: [2, 3]&quot;，我们可以抽取出 final score，然后与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">r_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 进行对比。对于每一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 我们都要过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>R</mi><mi>F</mi><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{RFT}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">RFT</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次 GRM（因此也就会有多个结果，但是我们要过滤掉一些结果，这个过滤的过程就是我们拒绝采样的过程。</li><li>首先我们会把预测结果和 golden label 不一致的过滤</li><li>其次会过滤过于简单的样本，也就是在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>N</mi><mi>F</mi><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{NFT}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">NFT</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次采样的过程中，都和 golden label 结果一样的样本。</li><li>具体的过滤要求是，对于有多条 response 输入的样本（如上面样例中的 response 1/2 就是有 2 个 response 输入），那么最终要求人工标注的最优 response 具有最大的 score；而如果 response 只有一条，那么要求预测的奖励值等于 goden label。(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示对于第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 条 response 的 score) <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503220321571.webp" alt="deepseek-grm-20250503220321571" loading="lazy"></li><li>上面的看似美好，但是没有考虑到，GRM 有限次采样的过程中，可能无法生成符合 golden label（具体某一条response 好以及打分），因此会加入一条人工打分最大的 response 到 prompt 中，这条样本被叫做 hinted sampling，<strong>如果使用 hinted sampling 则只采样一次</strong>，实验结果发现 hinted sampling 的样本 Critique 结果更短（这样效果可能受限），因此只适合做冷启动，更多的效果提升还是需要 online RL</li></ul><h4 id="_3-1-2-online-rl" tabindex="-1"><a class="header-anchor" href="#_3-1-2-online-rl"><span>3.1.2 online RL</span></a></h4><p>强化学习部分因为使用 rule-based reward，因此细节反而相对比较简单。具体是使用 GRPO 算法做训练，输入是：一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> (instruction) 以及 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\\{y_i\\}_{i=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> （n 条 response），输出是：GRM 生成的 principle 和 critique，然后抽出去对应的分数，计为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，下面的公式总结起来就一句话：预测对了给 1 分，预测错了给 -1 分，没有其他的格式分。 <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503222004215.webp" alt="deepseek-grm-20250503222004215" loading="lazy"></p><p>其他细节：KL 散度约束的系数比 DeepSeek-R1 更大，然后设置 GRPO 中 rollout 次数为 4 来平衡效率和效果。</p><h3 id="_3-2-推理" tabindex="-1"><a class="header-anchor" href="#_3-2-推理"><span>3.2 推理</span></a></h3><p>SPCT 最重要的初衷是什么？是【inference-time scaling】。因此推理的时候有两个方式来 Scaling 达到提升效果的目的。</p><ul><li>Voting with Generated Rewards。具体解释为：推理的时候 sample 多条结果，那么就会有多个结果的 score，把每个 response 的结果加起来作为最终的结果。</li><li>Meta Reward Model Guided Voting。也就是说不是对多次采样的 Score 直接加起来，因为有时候 GRM 可能生成一些质量低的 principle 和 critique，而是通过训练一个 meta rewrd model 来引导投票过程。具体就是训练一个二分类，表示当前的 principle 和 critique 是否要被用于投票，也就是过滤掉一些低质量的采样结果。比如 Figure 3 中的 meta RM 就过滤掉了 2 4 两个结果，只用 1 3 用于投票。一般设置保留一半的采样结果。</li></ul><h2 id="_4-实验结果" tabindex="-1"><a class="header-anchor" href="#_4-实验结果"><span>4. 实验结果</span></a></h2><h3 id="_4-1-主实验分析" tabindex="-1"><a class="header-anchor" href="#_4-1-主实验分析"><span>4.1 主实验分析</span></a></h3><ul><li>27B的 spct-GRM模型比 340B 大模型效果还要好，并且不像 scalar 和 semi-scalar 的模型一样比较大的 bias（比如 ppe 任务，可验证奖励任务就表现更好）。</li><li>相对于 LLM-as-a-Judge 的方式，带有 spct 的GRM 因为有 principle 的生成，GRM相对更好一些。</li><li>整体上说就是：SPCT 提升了 GRM 在通用任务的评估能力，并且具有更少的领域偏差（不一定非得是可验证奖励的领域才表现好）。 <img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503223555628.webp" alt="deepseek-grm-20250503223555628" loading="lazy"></li></ul><h3 id="_4-2-inference-scaling" tabindex="-1"><a class="header-anchor" href="#_4-2-inference-scaling"><span>4.2 Inference Scaling</span></a></h3><p>Inference-time Scaling，Voting 从 1 -&gt; 32，效果逐步提升，并且 MetaRM 会进一步带来效果，这个无需多说。</p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224457640.webp" alt="deepseek-grm-20250503224457640|445" loading="lazy"></p><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503225713996.webp" alt="deepseek-grm-20250503225713996" loading="lazy"></p><h3 id="_4-3-消融实验" tabindex="-1"><a class="header-anchor" href="#_4-3-消融实验"><span>4.3 消融实验</span></a></h3><ul><li>各个组件效均有效果提升，其实比较重要的是 Princeple Generation 以及 General Instruction Data（这告诉我们混合通用数据的重要性）。 <ul><li>non-hinted sampling 更重要，比较合理，给了模型更多的探索和采样空间。hinted sampling 更多是为了防止模型训练过程中学不到东西，是保下线的东西。</li><li>最终重要的，RL 比 RFT 更重要， <strong>RL is all we need</strong>（🤣66.1 -&gt; 68.7）。</li></ul></li></ul><p><img src="https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224750765.webp" alt="deepseek-grm-20250503224750765|575" loading="lazy"></p><h2 id="其他" tabindex="-1"><a class="header-anchor" href="#其他"><span>其他</span></a></h2><p>最后欢迎关注我，基本全网同名 <a href="https://yuanchaofa.com/" target="_blank" rel="noopener noreferrer">chaofa用代码打点酱油</a></p><ul><li>公众号： <img src="https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png" alt="chaofa用代码打点酱油" loading="lazy"></li><li><a href="https://space.bilibili.com/12420432" target="_blank" rel="noopener noreferrer">B站-chaofa用代码打点酱油</a></li><li><a href="https://www.youtube.com/@bbruceyuan" target="_blank" rel="noopener noreferrer">YouTube-chaofa用代码打点酱油</a></li><li><a href="https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54" target="_blank" rel="noopener noreferrer">chaofa 的 notion 简介</a></li></ul>',50)]))}const c=a(p,[["render",l],["__file","deepseek-grm-paper-reading-notes.html.vue"]]),o=JSON.parse('{"path":"/post/deepseek-grm-paper-reading-notes.html","title":"DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)","lang":"zh-CN","frontmatter":{"title":"DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)","date":"2025-05-03T23:00:20.000Z","tag":["LLM","paper"],"category":["paper-reading"],"description":"DeepSeek团队提出全新通用奖励模型DeepSeek-GRM，通过Self-Principled Critique Tuning（SPCT）方法实现推理时动态扩展能力。该研究突破传统规则奖励模型的局限，在角色扮演、创意写作等开放领域展现卓越性能。27B小模型效果超越340B大模型，且具备更少领域偏差。文章详解训练策略（RFT+在线强化学习）和推理优化（投票机制+元奖励引导），实验结果证实推理时扩展可显著提升效果，这是 DeepSeek-R2 的前兆吗？","publish":true,"permalink":"/post/deepseek-grm-paper-reading-notes.html","head":[["meta",{"property":"og:url","content":"https://yuanchaofa.com/post/deepseek-grm-paper-reading-notes.html"}],["meta",{"property":"og:site_name","content":"chaofa用代码打点酱油"}],["meta",{"property":"og:title","content":"DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)"}],["meta",{"property":"og:description","content":"DeepSeek团队提出全新通用奖励模型DeepSeek-GRM，通过Self-Principled Critique Tuning（SPCT）方法实现推理时动态扩展能力。该研究突破传统规则奖励模型的局限，在角色扮演、创意写作等开放领域展现卓越性能。27B小模型效果超越340B大模型，且具备更少领域偏差。文章详解训练策略（RFT+在线强化学习）和推理优化（投票机制+元奖励引导），实验结果证实推理时扩展可显著提升效果，这是 DeepSeek-R2 的前兆吗？"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123401789.webp"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-21T06:46:04.000Z"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:tag","content":"paper"}],["meta",{"property":"article:published_time","content":"2025-05-03T23:00:20.000Z"}],["meta",{"property":"article:modified_time","content":"2025-06-21T06:46:04.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DeepSeek-GRM：Inferene-time Scaling 的 Generalist Reward Model(通用奖励模型)\\",\\"image\\":[\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123401789.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502123435456.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250502130525536.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503212538082.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503213929051.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503220321571.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503222004215.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503223555628.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224457640.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503225713996.webp\\",\\"https://cfcdn.bruceyuan.com/blog/2025/deepseek-grm-20250503224750765.webp\\",\\"https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png\\"],\\"datePublished\\":\\"2025-05-03T23:00:20.000Z\\",\\"dateModified\\":\\"2025-06-21T06:46:04.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Chaofa Yuan\\",\\"url\\":\\"https://yuanchaofa.com\\"}]}"]]},"headers":[{"level":2,"title":"1. 结论(take away)","slug":"_1-结论-take-away","link":"#_1-结论-take-away","children":[]},{"level":2,"title":"2. 前提(Preliminaries)","slug":"_2-前提-preliminaries","link":"#_2-前提-preliminaries","children":[{"level":3,"title":"2.1 RM 模型训练分类","slug":"_2-1-rm-模型训练分类","link":"#_2-1-rm-模型训练分类","children":[]},{"level":3,"title":"2.2 Principle可以提升RM效果","slug":"_2-2-principle可以提升rm效果","link":"#_2-2-principle可以提升rm效果","children":[]}]},{"level":2,"title":"3. Self-Principled Critique Tuning(SPCT)","slug":"_3-self-principled-critique-tuning-spct","link":"#_3-self-principled-critique-tuning-spct","children":[{"level":3,"title":"3.1 训练","slug":"_3-1-训练","link":"#_3-1-训练","children":[{"level":4,"title":"3.1.1 RFT (Reject fine-tuning)","slug":"_3-1-1-rft-reject-fine-tuning","link":"#_3-1-1-rft-reject-fine-tuning","children":[]},{"level":4,"title":"3.1.2 online RL","slug":"_3-1-2-online-rl","link":"#_3-1-2-online-rl","children":[]}]},{"level":3,"title":"3.2 推理","slug":"_3-2-推理","link":"#_3-2-推理","children":[]}]},{"level":2,"title":"4. 实验结果","slug":"_4-实验结果","link":"#_4-实验结果","children":[{"level":3,"title":"4.1 主实验分析","slug":"_4-1-主实验分析","link":"#_4-1-主实验分析","children":[]},{"level":3,"title":"4.2 Inference Scaling","slug":"_4-2-inference-scaling","link":"#_4-2-inference-scaling","children":[]},{"level":3,"title":"4.3 消融实验","slug":"_4-3-消融实验","link":"#_4-3-消融实验","children":[]}]},{"level":2,"title":"其他","slug":"其他","link":"#其他","children":[]}],"git":{"createdTime":1746285097000,"updatedTime":1750488364000,"contributors":[{"name":"Chaofa Yuan","email":"bruceyuan123@gmail.com","commits":5}]},"readingTime":{"minutes":9.5,"words":2849},"filePathRelative":"post/paper/deepseek-grm.md","localizedDate":"2025年5月3日"}');export{c as comp,o as data};

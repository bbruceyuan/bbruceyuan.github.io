---
title: LLM MOEçš„è¿›åŒ–ä¹‹è·¯ï¼Œä»æ™®é€šç®€åŒ– MOEï¼Œåˆ° spare_moeï¼Œå†åˆ° deepseek ä½¿ç”¨çš„ share_xpert_spare_moe
date: 2025-01-27T19:30:00
star: true
tag:
  - transformer
  - LLM
category:
  - hands-on-code
  - llms-zero-to-hero
description: "æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†ä¸‰ç§ä¸åŒç‰ˆæœ¬çš„æ··åˆä¸“å®¶æ¨¡å‹(Mixture of Experts, MoE)å®ç°ï¼Œä»åŸºç¡€åˆ°è¿›é˜¶ï¼Œå¸®åŠ©è¯»è€…å…¨é¢ç†è§£ MoE åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š1. åŸºç¡€ç‰ˆ MoEï¼šä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚ä½œä¸ºä¸“å®¶ï¼Œç†è§£ MoE çš„åŸºæœ¬å·¥ä½œåŸç†; 2. SparseMoEï¼šå¤§æ¨¡å‹è®­ç»ƒä¸­å¸¸ç”¨çš„ç¨€ç– MoE å®ç°ï¼ŒåŸºäº Switch Transformers çš„è®¾è®¡;3. SharedExpert SparseMoEï¼šå‚è€ƒ DeepSeek çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¼•å…¥å…±äº«ä¸“å®¶æœºåˆ¶ã€‚æ–‡ç« æä¾›äº†å®Œæ•´çš„ PyTorch å®ç°ä»£ç ï¼ŒåŒ…æ‹¬æ¨¡å‹å®šä¹‰ã€è·¯ç”±æœºåˆ¶ã€è´Ÿè½½å‡è¡¡æŸå¤±è®¡ç®—ä»¥åŠè®­ç»ƒæµç¨‹ã€‚åŒæ—¶è®¾ç½®äº†æ‰©å±•æ€§ç»ƒä¹ ï¼Œå¸®åŠ©è¯»è€…æ·±å…¥ç†è§£ MoE çš„å·¥ä½œæœºåˆ¶å’Œä¼˜åŒ–æ–¹å‘ã€‚"
publish: true
permalink: /llms-zero-to-hero/the-way-of-moe-model-evolution.html
banner: https://bruceyuan.com/img/huggingface.png
---


## 1. é˜…è¯»å‰æ
æœ¬æ¬¡è¯¾ä¸€å…±è®²è§£ä¸‰ä¸ªä¸åŒç‰ˆæœ¬çš„ MOEï¼Œåˆ†åˆ«æ˜¯åŸºç¡€ç‰ˆMOEï¼Œå¤§æ¨¡å‹è®­ç»ƒç”¨çš„ SparseMoEï¼Œè¿˜æœ‰ DeepSeek ç”¨çš„æ¯”è¾ƒå¤šçš„ shared_expert çš„ SparseMoEã€‚
- åŸºç¡€ç‰ˆMOEã€‚ç†è§£ MOE åœ¨å¹²ä»€ä¹ˆï¼Ÿ
- å¤§æ¨¡å‹è®­ç»ƒç”¨çš„ SparseMoEã€‚äº†è§£å¤§æ¨¡å‹æ€ä¹ˆåš MOE è®­ç»ƒï¼Ÿ
- Deepseek ç”¨çš„ shared_expert SparseMoEã€‚äº†è§£ MOE æ¨¡å‹å¦‚ä½•è¿›åŒ–ï¼Ÿ

## 2. ç‰ˆæœ¬1ï¼šåŸºç¡€ç‰ˆæœ¬MOE
è¾“å…¥æ˜¯ä¸€ä¸ª Token, è¾“å‡ºæ˜¯ä¸€ä¸ª Token Embeddingã€‚æš‚æ—¶å…ˆä¸è€ƒè™‘ MOE å¾—åˆ°çš„  Embedding æ€ä¹ˆä½¿ç”¨ã€‚

å› ä¸º MOE ç½‘ç»œå¯¹åº”ç€ Expertï¼Œè¿™ä¸ª Expert ä¸€èˆ¬æ˜¯ä¸€ä¸ª FeadFoward Networkï¼ŒFFNã€‚è€Œä¸ºäº†ç®€åŒ–ï¼Œåç»­æˆ‘ä»¬éƒ½ç”¨ä¸€å±‚çš„ Linear ä»£æ›¿ï¼Œæ›´é«˜çº§ç‰ˆæœ¬çš„ Expert ç•™ç»™å¤§å®¶å½“åšè¯¾åä½œä¸šã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¸“å®¶çš„å®šä¹‰ã€‚
```python
class BasicExpert(nn.Module):
    # ä¸€ä¸ª Expert å¯ä»¥æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„ï¼Œ linear å±‚å³å¯
    # ä¹Ÿå¯ä»¥æ˜¯ MLP å±‚
    # ä¹Ÿå¯ä»¥æ˜¯ æ›´å¤æ‚çš„ MLP å±‚ï¼ˆactive function è®¾ç½®ä¸º swigluï¼‰
    def __init__(self, feature_in, feature_out):
        super().__init__()
        self.linear = nn.Linear(feature_in, feature_out)
    
    def forward(self, x):
        return self.linear(x)
```

åŸºç¡€ç‰ˆæœ¬çš„ MOE å¯ä»¥çœ‹è¿™ä¸ªå›¾ï¼Œéå¸¸çš„ç®€å•ã€‚

![llms-zero-to-hero-basic-moe-model](/llms-zero-to-hero/basic-moe-model.png)

```python

class BasicMOE(nn.Module):
    def __init__(self, feature_in, feature_out, expert_number):
        super().__init__()
        self.experts = nn.ModuleList(
            [
                BasicExpert(feature_in, feature_out) for _ in range(expert_number)
            ]
        )
        # gate å°±æ˜¯é€‰ä¸€ä¸ª expert 
        self.gate = nn.Linear(feature_in, expert_number)
    
    def forward(self, x):
        # x çš„ shape æ˜¯ ï¼ˆbatch, feature_in)
        expert_weight = self.gate(x)  # shape æ˜¯ (batch, expert_number)
        expert_out_list = [
            expert(x).unsqueeze(1) for expert in self.experts
        ]  # é‡Œé¢æ¯ä¸€ä¸ªå…ƒç´ çš„ shape æ˜¯ï¼š (batch, ) ??

        # concat èµ·æ¥ (batch, expert_number, feature_out)
        expert_output = torch.cat(expert_out_list, dim=1)

        # print(expert_output.size())

        expert_weight = expert_weight.unsqueeze(1) # (batch, 1, expert_nuber)

        # expert_weight * expert_out_list
        output = expert_weight @ expert_output  # (batch, 1, feature_out)
        
        return output.squeeze()


def test_basic_moe():
    x = torch.rand(2, 4)

    basic_moe = BasicMOE(4, 3, 2)
    out = basic_moe(x)
    print(out)


test_basic_moe()

```

## 2. ç‰ˆæœ¬2ï¼šSparseMoE ï¼ˆå¤§æ¨¡å‹è®­ç»ƒä½¿ç”¨ï¼‰
è¿™ä¸ªä¸€èˆ¬æˆ‘ä»¬ç”¨ switch transformers è¿™ç¯‡æ–‡ç« çš„å›¾ä½œä¸ºæ¼”ç¤ºï¼Œè¯¦æƒ…çœ‹ï¼š

![llms-zero-to-hero-switch-transformers-moe-model](/llms-zero-to-hero/switch-transformers-moe-model.png)


å’Œ Basic åŒºåˆ«æ˜¯ï¼ŒMOE é€‰æ‹© topK ä¸ªä¸“å®¶ï¼Œç„¶åå¯¹è¿™ topK ä¸ªä¸“å®¶çš„è¾“å‡ºè¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¹¶ä¸”æŠŠè¾“å…¥æ ·æœ¬å˜æˆäº†å¤§æ¨¡å‹ä¸­çœŸå®çš„è¾“å…¥ Shapeï¼Œ(batch, seq_len, hidden_dim)

```python

# ä¸»è¦å‚è€ƒè‡ª mistral MOE çš„å®ç°

class MOERouter(nn.Module):
    def __init__(self, hidden_dim, expert_number, top_k):
        super().__init__()
        self.gate = nn.Linear(hidden_dim, expert_number)
        self.expert_number = expert_number
        self.top_k = top_k
    
    def forward(self, hidden_states):
        # è®¡ç®—è·¯ç”±logits
        router_logits = self.gate(hidden_states)  # shape is (b * s, expert_number)
        
        # è®¡ç®—ä¸“å®¶ç»è¿‡softmaxä¹‹åçš„æ¦‚ç‡
        routing_probs = F.softmax(router_logits, dim=-1, dtype=torch.float)
        
        # è®¡ç®—topkçš„ä¸“å®¶çš„è¾“å‡º
        router_weights, selected_experts = torch.topk(
            routing_probs, self.top_k, dim=-1
        )  # shapeéƒ½æ˜¯ (b * s, top_k)
        
        # ä¸“å®¶æƒé‡å½’ä¸€åŒ–
        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)
        router_weights = router_weights.to(hidden_states.dtype)
        
        # ç”Ÿæˆä¸“å®¶æ©ç 
        expert_mask = F.one_hot(
            selected_experts,
            num_classes=self.expert_number
        )  # shapeæ˜¯ (b * s, top_k, expert_number)
        expert_mask = expert_mask.permute(2, 1, 0)  # (expert_number, top_k, b * s)
        
        return router_logits, router_weights, selected_experts, expert_mask


class MOEConfig:
    def __init__(
            self, 
            hidden_dim, 
            expert_number, 
            top_k, 
            shared_experts_number=2,
        ):
        self.hidden_dim = hidden_dim
        self.expert_number = expert_number
        self.top_k = top_k
        self.shared_experts_number = shared_experts_number

class SparseMOE(nn.Module):
    # ç¨€ç– MOE æ¨¡å‹ï¼Œè¿™é‡Œæ¯ä¸€ä¸ª token éƒ½ä¼šè¿‡ topk ä¸ªä¸“å®¶ï¼Œå¾—åˆ°å¯¹åº”token çš„ hidden_embeddings
    def __init__(self, config):
        super().__init__()

        self.hidden_dim = config.hidden_dim

        self.expert_number = config.expert_number
        self.top_k = config.top_k

        self.experts = nn.ModuleList(
            [
                BasicExpert(self.hidden_dim, self.hidden_dim) for _ in range(self.expert_number)
            ]
        )

        self.router = MOERouter(self.hidden_dim, self.expert_number, self.top_k)
    
    def forward(self, x):
        # x shape is (b, s, hidden_dim)
        batch_size, seq_len, hidden_dim = x.size()

        # åˆå¹¶å‰ä¸¤ä¸ªç»´åº¦ï¼Œå› ä¸ºä¸æ˜¯ Sample ç»´åº¦äº†ï¼Œè€Œæ˜¯ token ç»´åº¦
        hidden_states = x.view(-1, hidden_dim) # shape is(b * s, hidden_dim)

        router_logits, router_weights, selected_experts_indices, expert_mask = self.router(hidden_states)
        # å…¶ä¸­ selected_experts_indices shape æ˜¯ (b * s, top_k)
        # å…¶ä¸­ expert_mask shape æ˜¯ (expert_number, top_k, b * s)
        
        final_hidden_states = torch.zeros(
            (batch_size * seq_len, hidden_dim),
            dtype=hidden_states.dtype,
            device=hidden_states.device
        )

        for expert_idx in range(self.expert_number):
            expert_layer = self.experts[expert_idx]
            # expert_mask[expert_idx] shape æ˜¯ (top_k, b * s)
            idx, top_x = torch.where(expert_mask[expert_idx]) 
            # idx å’Œ top_x éƒ½æ˜¯ä¸€ç»´ tensor
            # idx çš„å€¼æ˜¯ 0 æˆ– 1, è¡¨ç¤ºè¿™ä¸ª token æ˜¯ä½œä¸ºå½“å‰ä¸“å®¶çš„ top1 è¿˜æ˜¯ top2
            # top_x çš„å€¼æ˜¯ token åœ¨ batch*seq_len ä¸­çš„ä½ç½®ç´¢å¼•
            # ä¾‹å¦‚å¯¹äº batch_size=2, seq_len=4 çš„è¾“å…¥:
            # top_x çš„å€¼èŒƒå›´æ˜¯ 0-7, è¡¨ç¤ºåœ¨å±•å¹³åçš„ 8 ä¸ª token ä¸­çš„ä½ç½®
            # idx çš„å€¼æ˜¯ 0/1, è¡¨ç¤ºè¿™ä¸ª token æŠŠå½“å‰ä¸“å®¶ä½œä¸ºå…¶ top1/top2 ä¸“å®¶

            # hidden_states çš„ shape æ˜¯ (b * s, hidden_dim)
            # éœ€è¦å–åˆ° top_x å¯¹åº”çš„ hidden_states
            current_state = hidden_states.unsqueeze(
                0
            )[:, top_x, :].reshape(-1, hidden_dim) # ï¼ˆselected_token_number, hidden_dimï¼‰

            # router_weight çš„ shape æ˜¯ (b * s, top_k)
            current_hidden_states = expert_layer(
                current_state
            ) * router_weights[top_x, idx].unsqueeze(-1)  # ï¼ˆselected_token_number, 1ï¼‰ è¿™é‡Œæœ‰å¹¿æ’­

            # æŠŠå½“å‰ä¸“å®¶çš„è¾“å‡ºåŠ åˆ° final_hidden_states ä¸­
            # æ–¹å¼1 çš„å†™æ³•æ€§èƒ½æ›´å¥½ï¼Œå¹¶ä¸”æ–¹å¼1å®¹æ˜“å‡ºç°
            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
            # æ–¹å¼2
            # final_hidden_states[top_x] += current_hidden_states.to(hidden_states.dtype)
            # æ–¹å¼1 çš„å†™æ³•æ€§èƒ½æ›´å·®ï¼Œå¹¶ä¸”æ–¹å¼1å®¹æ˜“å‡ºç°é”™è¯¯ï¼Œ+= æ“ä½œåœ¨å¤„ç†é‡å¤ç´¢å¼•æ—¶éœ€è¦å¤šæ¬¡è¯»å†™å†…å­˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´ç«äº‰æ¡ä»¶

        # æŠŠ final_hidden_states è¿˜åŸåˆ°åŸæ¥çš„ shape
        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)

        return final_hidden_states, router_logits # shape æ˜¯ (b * s, expert_number)


def test_token_level_moe():
    x = torch.rand(2, 4, 16)
    config = MOEConfig(16, 2, 2)
    token_level_moe = SparseMOE(config)
    out = token_level_moe(x)
    print(out[0].shape, out[1].shape)


test_token_level_moe()
```


## 3. ç‰ˆæœ¬3ï¼šShareExpert SparseMoE ï¼ˆdeepseek ç‰ˆæœ¬ï¼‰
> å¤‡æ³¨ï¼šè¿™é‡Œæ˜¯å‚è€ƒ deepseek moe æ€æƒ³ï¼Œå†™çš„ä¸€ä¸ªå…±äº« expert çš„ MOE ç½‘ç»œï¼Œæœ‰ä¸€å®šçš„ç®€åŒ–ï¼Œä½†æ˜¯å¯ä»¥æ–¹ä¾¿ç†è§£è®­ç»ƒè¿‡ç¨‹ã€‚

å’Œ ç‰ˆæœ¬2 çš„ SparseMOE åŒºåˆ«æ˜¯ï¼Œè¿™é‡Œå¤šäº†ä¸€ä¸ª shared experts çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯æ‰€æœ‰ token å…±äº«çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰€æœ‰ token éƒ½è¿‡è¿™ä¸ª shared experts æ¨¡å‹ï¼Œç„¶åæ¯ä¸ª token ä¼šç”¨è®¡ç®—çš„ Router æƒé‡ï¼Œæ¥é€‰æ‹© topK ä¸ªä¸“å®¶ï¼Œç„¶åå’Œå…±äº«çš„ä¸“å®¶çš„è¾“å‡ºä¸€èµ·åŠ æƒæ±‚å’Œã€‚

å…·ä½“ç»“æ„å›¾ä¸ºï¼š

![llms-zero-to-hero-deepseek-v3-model-architecture](/llms-zero-to-hero/deepseek-v3-model-architecture.png)

```python
class ShareExpertMOE(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.moe_model = SparseMOE(config)
        self.shared_experts = nn.ModuleList(
            [
                BasicExpert(
                    config.hidden_dim, config.hidden_dim
                ) for _ in range(config.shared_experts_number)
            ]
        )

    def forward(self, x):
        # x shape æ˜¯ (b, s, hidden_dim)
        # é¦–å…ˆè¿‡ moe æ¨¡å‹
        sparse_moe_out, router_logits = self.moe_model(x)
        
        # é’ˆå¯¹çš„è¿˜æ˜¯ x çš„æ¯ä¸€ä¸ª 
        # ç„¶åè¿‡ shared experts
        shared_experts_out = [
            expert(x) for expert in self.shared_experts
        ] # æ¯ä¸€ä¸ª expert çš„è¾“å‡º shape æ˜¯ (b, s, hidden_dim)
        
        shared_experts_out = torch.stack(
            shared_experts_out, dim=0
        ).sum(dim=0)
        
        # æŠŠ sparse_moe_out å’Œ shared_experts_out åŠ èµ·æ¥
        return sparse_moe_out + shared_experts_out, router_logits


def test_share_expert_moe():
    x = torch.rand(2, 4, 16)
    config = MOEConfig(16, 2, 2)
    share_expert_moe = ShareExpertMOE(config)
    out = share_expert_moe(x)
    print(out[0].shape, out[1].shape)


test_share_expert_moe()

```



## 4. æ¨¡å‹è®­ç»ƒæµ‹è¯•
ç”¨äºæµ‹è¯•ä¸Šé¢çš„ä»£ç æ˜¯å¦å¯ä»¥è·‘é€šï¼Ÿ

```python

def switch_load_balancing_loss(router_logits: torch.Tensor, num_experts: int) -> torch.Tensor:
    """
    è®¡ç®— Switch Transformers çš„è´Ÿè½½å‡è¡¡æŸå¤±
    
    Args:
        router_logits: shape [batch_size * sequence_length, num_experts]
        num_experts: ä¸“å®¶æ•°é‡
    
    Returns:
        total_loss: æ€»æŸå¤± = auxiliary_loss + z_loss
    """
    # è®¡ç®—è·¯ç”±æ¦‚ç‡
    router_probs = torch.softmax(router_logits, dim=-1)  # [b*s, num_experts]
    
    # è·å–æ¯ä¸ªtokençš„æœ€ä¼˜ä¸“å®¶
    _, selected_experts = torch.topk(router_probs, k=2, dim=-1)  # [b*s]
    
    # åˆ›å»ºone-hotçŸ©é˜µè¡¨ç¤ºé€‰ä¸­çš„ä¸“å®¶
    mask = torch.nn.functional.one_hot(selected_experts, num_experts).float()  # [b*s, num_experts]
    
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„æœŸæœ›è´Ÿè½½ (ç†æƒ³æƒ…å†µä¸‹åº”è¯¥æ˜¯ 1/num_experts)
    expected_load = torch.ones_like(router_probs) / num_experts
    
    # è®¡ç®—å®é™…è´Ÿè½½ (æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡é™¤ä»¥æ€»tokenæ•°é‡)
    # åœ¨batchç»´åº¦ä¸Šè®¡ç®—å¹³å‡å€¼
    actual_load = mask.mean(dim=0)  # [num_experts]
    
    # è®¡ç®—auxiliary loss
    # è¿™ä¼šæƒ©ç½šè´Ÿè½½åˆ†å¸ƒä¸æœŸæœ›è´Ÿè½½çš„å·®å¼‚
    aux_loss = torch.sum(actual_load * router_probs.mean(dim=0)) * num_experts
    
    # è®¡ç®—z_loss (å¯é€‰)
    # è¿™ä¼šæƒ©ç½šè¿‡å¤§çš„è·¯ç”±logits
    z_loss = torch.mean(torch.square(router_logits))
    z_loss_weight = 0.001  # å¯è°ƒæ•´çš„è¶…å‚æ•°
    
    # æ€»æŸå¤±
    total_loss = aux_loss + z_loss * z_loss_weight
    
    return total_loss

def test_moe_training():
    # Create a simple dataset
    batch_size = 32
    seq_len = 16
    hidden_dim = 32
    num_batches = 100
    
    # Initialize model and optimizer
    config = MOEConfig(hidden_dim=hidden_dim, 
                      expert_number=4,
                      top_k=2,
                      shared_experts_number=2)
    model = ShareExpertMOE(config)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Training loop
    model.train()
    for batch in range(num_batches):
        # Generate random input data
        x = torch.randn(batch_size, seq_len, hidden_dim)
        target = torch.randn(batch_size, seq_len, hidden_dim)
        
        # Forward pass
        output, router_logits = model(x)

        # Compute losses
        # MSE loss for prediction
        mse_loss = F.mse_loss(output, target)
        
        aux_loss = switch_load_balancing_loss(router_logits, config.expert_number)
        # Combined loss
        total_loss = mse_loss + 0.01 * aux_loss
        
        # Backward pass and optimize
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        if batch % 10 == 0:
            print(f"Batch {batch}, Loss: {total_loss.item():.4f} "
                  f"(MSE: {mse_loss.item():.4f}, Aux: {aux_loss.item():.4f})")

# Run the training test
test_moe_training()
```


## 5. è¯¾åä½œä¸š

1. æŠŠ expert æ”¹æˆ swishGLU ç‰ˆæœ¬çš„ FFN ä¸“å®¶
   - å¯ä»¥å‚è€ƒæ–‡ç« ï¼š[LLM activate function æ¿€æ´»å‡½æ•°çš„è¿›åŒ–ä¹‹è·¯ï¼Œä» ReLUï¼ŒGELU åˆ° swishGLU](https://bruceyuan.com/llms-zero-to-hero/activate-function-from-relu-gelu-to-swishglu.html)
   - ä»¥åŠ [æ‰‹å†™ transformer decoderï¼ˆCausalLMï¼‰](https://bruceyuan.com/hands-on-code/hands-on-causallm-decoder.html)
2. æŠŠ MOE åº”ç”¨åˆ°ä¸Šä¸€æ¬¡çš„ build_nanoGPT ä¸­ï¼Œä¹Ÿå°±æ˜¯æ›¿æ¢æ‰åŸæ¥çš„ FFNå±‚ï¼Œæ³¨æ„è¿™é‡Œè´Ÿè½½å‡è¡¡ loss è¦åŒ…å«æ¯ä¸€å±‚çš„ MOE çš„ router_logits
   - å‚è€ƒ GitHub ä»“åº“ï¼Œ ã€[LLMs-Zero-to-Hero](https://github.com/bbruceyuan/LLMs-Zero-to-Hero)ã€‘
3. è‡ªå·±é—®ä¸€ä¸‹ GPT topK æ˜¯æ€ä¹ˆå®ç°çš„åå‘ä¼ æ’­ï¼Œäº†è§£åå‘ä¼ æ’­çš„æ¢¯åº¦æ€ä¹ˆæµè½¬çš„ï¼Ÿ


## äº¤ä¸ªæœ‹å‹ğŸ¤£
æœ€åæ¬¢è¿å…³æ³¨æˆ‘ï¼ŒåŸºæœ¬å…¨ç½‘åŒå [chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://bruceyuan.com/)
- å…¬ä¼—å·ï¼š ![chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](/llms-zero-to-hero/chaofa-wechat-official-account.png)
- [Bç«™-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://space.bilibili.com/12420432)
- [YouTube-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://www.youtube.com/@bbruceyuan)
- [chaofa çš„ notion ç®€ä»‹](https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54)
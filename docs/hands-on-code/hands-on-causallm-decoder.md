---
title: æ‰‹å†™ transformer decoderï¼ˆCausalLMï¼‰
date: 2024-08-18T13:00:00
star: true
tag:
  - transformer
  - LLM
category:
  - hands-on-code
description: æ‰‹å†™ä¸€ä¸ª Causal Language Modelï¼Œæˆ–è€…è¯´ç®€åŒ–ç‰ˆçš„ transformer ä¸­çš„ decoderã€‚
publish: true
permalink: /hands-on-code/hands-on-causallm-decoder.html
banner: https://yuanchaofa.com/img/huggingface.png
---

## é˜…è¯»é¡»çŸ¥

é¢è¯•è¿‡ç¨‹ä¸­è®©å†™ transformers Decoder ä¸€å®šè¦æ²Ÿé€šæ¸…æ¥šæ˜¯å†™ä¸€ä¸ª CausalLM decoder è¿˜æ˜¯åŸç‰ˆçš„ï¼ŒåŸç‰ˆçš„æ¯”è¾ƒå¤æ‚ï¼Œä¸€èˆ¬ä¹Ÿä¸ä¼šè®©å†™ã€‚è¿™é‡Œçš„ Decoder ä¸€èˆ¬æŒ‡çš„æ˜¯ **CausalLM**ï¼Œå…·ä½“å˜åŒ–æ˜¯å°‘äº† encoder éƒ¨åˆ†çš„è¾“å…¥ï¼Œæ‰€ä»¥ä¹Ÿå°±æ²¡æœ‰äº† encoder and decoder cross attentionã€‚

- å› ä¸ºé‡ç‚¹å¸Œæœ›å†™ CausalLMï¼Œæ‰€ä»¥æ²¡æœ‰ Cross attention å’Œ ä¹Ÿçœç•¥äº† token embedding è¿™ä¸€æ­¥ã€‚

> å¦‚æœå¯¹äºæ–‡å­—ä¸æ„Ÿå†’ï¼Œå¯ä»¥æŸ¥çœ‹**YouTube å’Œ B ç«™è§†é¢‘** > [Youtube é“¾æ¥](https://www.youtube.com/watch?v=yzEotGJaQ74)-- [bilibili é“¾æ¥](https://www.bilibili.com/video/BV1Nh1QYCEsS/)

## çŸ¥è¯†ç‚¹

- transformers decoder çš„æµç¨‹æ˜¯ï¼šinput -> self-attention -> cross-attention -> FFN
- causalLM decoder çš„æµç¨‹æ˜¯ input -> self-attention -> FFN
  - å…¶ä»– `[self-attention, FFN]` æ˜¯ä¸€ä¸ª blockï¼Œä¸€èˆ¬ä¼šæœ‰å¾ˆå¤šçš„ block
- FFN çŸ©é˜µæœ‰ä¸¤æ¬¡å˜åŒ–ï¼Œä¸€æ¬¡å‡ç»´åº¦ï¼Œä¸€æ¬¡é™ç»´åº¦ã€‚å…¶ä¸­ LLaMA å¯¹äº GPT çš„æ”¹è¿›è¿˜æœ‰æŠŠ GeLU å˜æˆäº† SwishGLUï¼Œå¤šäº†ä¸€ä¸ªçŸ©é˜µã€‚æ‰€ä»¥ä¸€èˆ¬å‡ç»´ä¼šä» `4h -> 4h * 2 / 3`
- åŸç‰ˆçš„ transformers ç”¨ post-norm, åé¢ gpt2, llama ç³»åˆ—ç”¨çš„æ˜¯ pre-normã€‚å…¶ä¸­ llama ç³»åˆ—ä¸€èˆ¬ç”¨ RMSNorm ä»£æ›¿ GPT and transformers decoder ä¸­çš„ LayerNormã€‚

å…·ä½“å®ç°ï¼š

```python
# å¯¼å…¥ç›¸å…³éœ€è¦çš„åŒ…
import math
import torch
import torch.nn as nn

import warnings
warnings.filterwarnings(action="ignore")

# å†™ä¸€ä¸ª Block
class SimpleDecoder(nn.Module):
    def __init__(self, hidden_dim, nums_head, dropout=0.1):
        super().__init__()

        self.nums_head = nums_head
        self.head_dim = hidden_dim // nums_head

        self.dropout = dropout

        # è¿™é‡ŒæŒ‰ç…§ transformers ä¸­çš„ decoder æ¥å†™ï¼Œç”¨ post_norm çš„æ–¹å¼å®ç°æ³¨æ„æœ‰ æ®‹å·®é“¾æ¥
        # eps æ˜¯ä¸ºäº†é˜²æ­¢æº¢å‡ºï¼›å…¶ä¸­ llama ç³»åˆ—çš„æ¨¡å‹ä¸€èˆ¬ç”¨çš„æ˜¯ RMSnorm ä»¥åŠ pre-normï¼ˆä¸ºäº†ç¨³å®šæ€§ï¼‰
        # RMSnorm æ²¡æœ‰ä¸€ä¸ª recenter çš„æ“ä½œï¼Œè€Œ layernorm æ˜¯è®©æ¨¡å‹é‡æ–°å˜æˆ å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1
        # RMS ä½¿ç”¨ wå¹³æ–¹æ ¹å‡å€¼è¿›è¡Œå½’ä¸€åŒ– $\sqrt{\frac{1}{n} \sum_{1}^{n}{a_i^2} }$
        self.layernorm_att = nn.LayerNorm(hidden_dim, eps=0.00001)

        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)
        self.o_proj = nn.Linear(hidden_dim, hidden_dim)
        self.drop_att = nn.Dropout(self.dropout)

        # for ffn å‡†å¤‡
        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4)
        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim)
        self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=0.00001)
        self.act_fn = nn.ReLU()
        
        self.drop_ffn = nn.Dropout(self.dropout)

    def attention_output(self, query, key, value, attention_mask=None):
        # è®¡ç®—ä¸¤è€…ç›¸å…³æ€§
        key = key.transpose(2, 3)  # (batch, num_head, head_dim, seq)
        att_weight = torch.matmul(query, key) / math.sqrt(self.head_dim)

        # attention mask è¿›è¡Œä¾æ¬¡è°ƒæ•´ï¼›å˜æˆ causal_attention
        if attention_mask is not None:
            # å˜æˆä¸‹ä¸‰è§’çŸ©é˜µ
            attention_mask = attention_mask.tril()
            att_weight = att_weight.masked_fill(attention_mask == 0, float("-1e20"))
        else:
            # äººå·¥æ„é€ ä¸€ä¸ªä¸‹ä¸‰è§’çš„ attention mask
            attention_mask = torch.ones_like(att_weight).tril()
            att_weight = att_weight.masked_fill(attention_mask == 0, float("-1e20"))

        att_weight = torch.softmax(att_weight, dim=-1)
        print(att_weight)

        att_weight = self.drop_att(att_weight)

        mid_output = torch.matmul(att_weight, value)
        # mid_output shape is: (batch, nums_head, seq, head_dim)

        mid_output = mid_output.transpose(1, 2).contiguous()
        batch, seq, _, _ = mid_output.size()
        mid_output = mid_output.view(batch, seq, -1)
        output = self.o_proj(mid_output)
        return output

    def attention_block(self, X, attention_mask=None):
        batch, seq, _ = X.size()
        query = self.q_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)
        key = self.k_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)
        value = self.v_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)

        output = self.attention_output(
            query,
            key,
            value,
            attention_mask=attention_mask,
        )
        return self.layernorm_att(X + output)

    def ffn_block(self, X):
        up = self.act_fn(
            self.up_proj(X),
        )
        down = self.down_proj(up)

        # æ‰§è¡Œ dropout
        down = self.drop_ffn(down)

        # è¿›è¡Œ norm æ“ä½œ
        return self.layernorm_ffn(X + down)

    def forward(self, X, attention_mask=None):
        # X ä¸€èˆ¬å‡è®¾æ˜¯å·²ç»ç»è¿‡ embedding çš„è¾“å…¥ï¼Œ (batch, seq, hidden_dim)
        # attention_mask ä¸€èˆ¬æŒ‡çš„æ˜¯ tokenizer åè¿”å›çš„ mask ç»“æœï¼Œè¡¨ç¤ºå“ªäº›æ ·æœ¬éœ€è¦å¿½ç•¥
        # shape ä¸€èˆ¬æ˜¯ï¼š (batch, nums_head, seq)

        att_output = self.attention_block(X, attention_mask=attention_mask)
        ffn_output = self.ffn_block(att_output)
        return ffn_output


# æµ‹è¯•

x = torch.rand(3, 4, 64)
net = SimpleDecoder(64, 8)
mask = (
    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])
    .unsqueeze(1)
    .unsqueeze(2)
    .repeat(1, 8, 4, 1)
)

net(x, mask).shape
```


## äº¤ä¸ªæœ‹å‹ğŸ¤£
æœ€åæ¬¢è¿å…³æ³¨æˆ‘ï¼ŒåŸºæœ¬å…¨ç½‘åŒå [chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://yuanchaofa.com/)
- å…¬ä¼—å·ï¼š ![chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png)
- [Bç«™-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://space.bilibili.com/12420432)
- [YouTube-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://www.youtube.com/@bbruceyuan)
- [chaofa çš„ notion ç®€ä»‹](https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54)
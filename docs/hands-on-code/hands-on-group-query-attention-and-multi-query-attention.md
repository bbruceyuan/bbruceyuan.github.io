---
title: æ‰‹å†™å¤§æ¨¡å‹ç»„ä»¶ä¹‹Group Query Attentionï¼Œä» MHAï¼ŒMQA åˆ° GQA
date: 2024-12-08T22:00:00
star: true
tag:
  - transformer
  - LLM
category:
  - hands-on-code
description: äº†è§£æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ï¼ŒåŒ…æ‹¬MHAï¼ˆMulti-Head Attentionï¼‰ã€MQAï¼ˆMulti-Query Attentionï¼‰å’ŒGQAï¼ˆGroup Query Attentionï¼‰ã€‚é€šè¿‡æ‰‹å†™ä»£ç å®ç°ï¼Œæ¢è®¨ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶çš„å¼‚åŒï¼Œä»¥åŠGQAåœ¨æ¨ç†æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„ä¼˜åŠ¿ã€‚
publish: true
permalink: /hands-on-code/hands-on-group-query-attention-and-multi-query-attention.html
banner: https://yuanchaofa.com/img/huggingface.png
---

- GQAï¼ˆGroup Query Attentionï¼‰çš„ä¼˜ç‚¹ï¼šæ•ˆæœæŸå¤±å°ï¼Œæ¨ç†çš„æ—¶å€™å¯ä»¥åŠ é€Ÿï¼ˆæ¥è‡ªäºkvcacheå°ï¼Œå†…å­˜å–æ•°å°‘ï¼‰ã€‚
- ä»”ç»†é˜…è¯» MHA, MQA å’Œ GQAçš„åŒºåˆ«ï¼Œå°±ä¼šå‘ç° MHA å’Œ MQA éƒ½æ˜¯ GQA çš„ç‰¹æ®Šè¡¨è¾¾å½¢å¼
    - ä¸‰è€…å¯ä»¥ç”¨åŒä¸€å¥—ä»£ç ï¼Œåªéœ€è¦ä¿®æ”¹ã€GQAã€‘ä»£ç é‡Œé¢çš„ `nums_key_value_head` å‚æ•°å°±å¯
    - `nums_key_value_head` è®¾ç½®ç­‰äº 1 å°±æ˜¯ MQA
    - `nums_key_value_head` è®¾ç½®ç­‰äº `nums_head` å°±æ˜¯ MHA



> å¦‚æœä¸å–œæ¬¢çœ‹æ–‡å­—çš„åŒå­¦å¯ä»¥æŸ¥çœ‹ [Bç«™](https://space.bilibili.com/12420432) æˆ–è€… [YouTube](https://www.youtube.com/@bbruceyuan) è§†é¢‘ã€‚
> 
> Bç«™ï¼š[https://www.bilibili.com/video/BV1ZmqpYfEGY/](https://www.bilibili.com/video/BV1ZmqpYfEGY/)
> 
> YouTube: [https://www.youtube.com/watch?v=1jBW7qcyd7A&t=1s](https://www.youtube.com/watch?v=1jBW7qcyd7A&t=1s)


## multi-head self-attention
> å¤‡æ³¨ï¼šä¹Ÿå¯ä»¥ç›´æ¥ç”± GQA ä¸­ä¿®æ”¹å‚æ•°å¾—åˆ°ã€‚ä½†æ˜¯æœ¬ä»£ç æ›´å®Œæ•´ä¸€äº›

```python
import math
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_dim, nums_head) -> None:
        super().__init__()
        self.nums_head = nums_head

        # ä¸€èˆ¬æ¥è¯´ï¼Œ
        self.head_dim = hidden_dim // nums_head
        self.hidden_dim = hidden_dim

        # ä¸€èˆ¬é»˜è®¤æœ‰ biasï¼Œéœ€è¦æ—¶åˆ»æ³¨æ„ï¼Œhidden_dim = head_dim * nums_headï¼Œæ‰€ä»¥æœ€ç»ˆæ˜¯å¯ä»¥ç®—æˆæ˜¯ n ä¸ªçŸ©é˜µ
        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)

        # gpt2 å’Œ bert ç±»éƒ½æœ‰ï¼Œä½†æ˜¯ llama å…¶å®æ²¡æœ‰
        self.att_dropout = nn.Dropout(0.1)
        # è¾“å‡ºæ—¶å€™çš„ proj
        self.o_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, X, attention_mask=None):
        # éœ€è¦åœ¨ mask ä¹‹å‰ masked_fill
        # X shape is (batch, seq, hidden_dim)
        # attention_mask shape is (batch, seq)

        batch_size, seq_len, _ = X.size()

        Q = self.q_proj(X)
        K = self.k_proj(X)
        V = self.v_proj(X)

        # shape å˜æˆ ï¼ˆbatch_size, num_head, seq_len, head_dimï¼‰
        q_state = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).permute(
            0, 2, 1, 3
        )
        k_state = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(
            1, 2
        )
        v_state = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(
            1, 2
        )
        # æ³¨æ„è¿™é‡Œéœ€è¦ç”¨ head_dimï¼Œè€Œä¸æ˜¯ hidden_dim
        attention_weight = (
            q_state @ k_state.transpose(-1, -2) / math.sqrt(self.head_dim)
        )
        print(type(attention_mask))
        if attention_mask is not None:
            attention_weight = attention_weight.masked_fill(
                attention_mask == 0, float("-1e20")
            )

        # ç¬¬å››ä¸ªç»´åº¦ softmax
        attention_weight = torch.softmax(attention_weight, dim=3)
        print(attention_weight)

        attention_weight = self.att_dropout(attention_weight)
        output_mid = attention_weight @ v_state

        # é‡æ–°å˜æˆ (batch, seq_len, num_head, head_dim)
        # è¿™é‡Œçš„ contiguous() æ˜¯ç›¸å½“äºè¿”å›ä¸€ä¸ªè¿ç»­å†…å­˜çš„ tensorï¼Œä¸€èˆ¬ç”¨äº† permute/tranpose éƒ½è¦è¿™ä¹ˆæ“ä½œ
        # å¦‚æœåé¢ç”¨ Reshape å°±å¯ä»¥ä¸ç”¨è¿™ä¸ª contiguous()ï¼Œå› ä¸º view åªèƒ½åœ¨è¿ç»­å†…å­˜ä¸­æ“ä½œ
        output_mid = output_mid.transpose(1, 2).contiguous()

        # å˜æˆ (batch, seq, hidden_dim),
        output = output_mid.view(batch_size, seq_len, -1)
        output = self.o_proj(output)
        return output


attention_mask = (
    torch.tensor(
        [
            [0, 1],
            [0, 0],
            [1, 0],
        ]
    )
    .unsqueeze(1)
    .unsqueeze(2)
    .expand(3, 8, 2, 2)
)

x = torch.rand(3, 2, 128)
net = MultiHeadAttention(128, 8)
net(x, attention_mask).shape
```


## Group Query Attention
> å¤‡æ³¨ï¼šä»¥ä¸‹ä»£ç çœç•¥äº† attention_dropout attention_maskç­‰æƒ…å†µçš„å¤„ç†ï¼ŒçœŸå®å®ç°è¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘ã€‚
```python
import torch
import torch.nn as nn
import math

# å¿½ç•¥äº† attention_mask, attention_dropout; 
class GroupQueryAttention(nn.Module):
    def __init__(self, hidden_dim, nums_head, nums_key_value_head):
        super().__init__()
        assert hidden_dim % nums_head == 0 # å¯ä»¥æ•´é™¤
        assert nums_head % nums_key_value_head == 0  # N ä¸ª query head ä¸ºä¸€ç»„

        self.hidden_dim = hidden_dim
        self.nums_head = nums_head
        self.nums_key_value_head = nums_key_value_head
        self.head_dim = hidden_dim // nums_head

        # åˆå§‹åŒ– qkv o
        self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim)  # out feature_size (nums_head * head_dim)
        # k v out shape (nums_key_value_head * head_dim)
        self.k_proj = nn.Linear(hidden_dim, nums_key_value_head * self.head_dim)
        self.v_proj = nn.Linear(hidden_dim, nums_key_value_head * self.head_dim)

        self.o_proj = nn.Linear(hidden_dim, hidden_dim) # input_size nums_head * head_dim

    def forward(self, X, attention_mask=None):
        # X shape (batch, seq, hidden_dim)
        batch_size, seq, _ = X.size()

        # qkv projection
        q = self.q_proj(X)  # ï¼ˆbatch, seq, hidden_dim)
        k = self.k_proj(X)
        v = self.v_proj(X) 

        # attention_weight ç›®æ ‡shape æ˜¯ (batch, nums_head, seq, seq)
        q = q.view(batch_size, seq, self.nums_head, self.head_dim)
        k = k.view(batch_size, seq, self.nums_key_value_head, self.head_dim)
        v = v.view(batch_size, seq, self.nums_key_value_head, self.head_dim)

        # å…³æ³¨: nums_head å’Œ nums_key_value_head çš„å…³ç³»
        q = q.transpose(1, 2) # (b, nums_head, seq, head_dim)
        k = k.transpose(1, 2) # (b, nums_key_value_head, seq, head_dim)
        v = v.transpose(1, 2)  # (b, nums_key_value_head, seq, head_dim)

        # k v repeatï¼› ï¼ˆå¹¿æ’­æ“ä½œï¼‰
        k = k.repeat_interleave(self.nums_head // self.nums_key_value_head, dim=1)
        v = v.repeat_interleave(self.nums_head // self.nums_key_value_head, dim=1)

        attention_score = (q @ k.transpose(2, 3)) / math.sqrt(self.head_dim)

        attention_weight = torch.softmax(attention_score, dim=-1)
        # ï¼ˆattention_mask å¿½ç•¥ï¼‰ # å¯ä»¥çœ‹å‰é¢çš„è§†é¢‘

        output = attention_weight @ v  # (b, nums_head, seq, head_dim)

        # output projection å˜æˆ (b, seq, hidden_dim)
        output = output.transpose(1, 2).contiguous()
        final_output = self.o_proj(output.view(batch_size, seq, -1))

        return final_output

# æµ‹è¯•
x = torch.rand(3, 2, 128)
net = GroupQueryAttention(128, 8, 4)
net(x).shape

```

## Multi Query Attention
ç”±äº MQA æ˜¯ GQA çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œå› æ­¤åªè¦åœ¨å‚æ•°è®¾ç½®çš„æ—¶å€™å°† nums_key_value_head = 1 å°±æ˜¯ Multi Query Self-Attentionã€‚


## äº¤ä¸ªæœ‹å‹ğŸ¤£
æœ€åæ¬¢è¿å…³æ³¨æˆ‘ï¼ŒåŸºæœ¬å…¨ç½‘åŒå [chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://yuanchaofa.com/)
- å…¬ä¼—å·ï¼š ![chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://yuanchaofa.com/llms-zero-to-hero/chaofa-wechat-official-account.png)
- [Bç«™-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://space.bilibili.com/12420432)
- [YouTube-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://www.youtube.com/@bbruceyuan)
- [chaofa çš„ notion ç®€ä»‹](https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54)
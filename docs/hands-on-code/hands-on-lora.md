---
title: LoRA åŸç†å’Œ PyTorch ä»£ç å®ç°
date: 2024-11-09T21:56:00
star: true
tag:
  - transformer
  - LLM
category:
  - hands-on-code
description: ç”¨ PyTorch å®ç°ä»é›¶å®ç° LoRA, ç†è§£ LoRA çš„åŸç†ï¼Œä¸»è¦æ˜¯ä¸ºäº†å±•ç¤ºä¸€ä¸ª LoRA å®ç°çš„ç»†èŠ‚
publish: true
permalink: /hands-on-code/hands-on-lora.html
---



## èƒŒæ™¯

æ— è®ºæ˜¯ç«çƒ­çš„å¤§æ¨¡å‹ï¼ˆLLMï¼‰è¿˜æ˜¯æ–‡ç”Ÿå›¾æ¨¡å‹ï¼ˆStable  Diffusionï¼‰å¾®è°ƒçš„æ—¶å€™ï¼Œéƒ½éœ€è¦å¤§é‡çš„GPUæ˜¾å­˜ï¼Œä¸ªäººçš„æ˜¾å¡ä¸Šå¾ˆéš¾å®ç°ï¼Œå› æ­¤å„ç§å‚æ•°é«˜æ•ˆï¼ˆParameter-Efficientï¼‰çš„æ–¹æ³•å±‚å‡ºä¸ç©·ï¼Œæœ€å—å¤§å®¶æ¬¢è¿çš„å°±æ˜¯ **LoRA**[ã€ŠLoRA: Low-Rank Adaptation of Large Language Modelsã€‹](https://papers.cool/arxiv/2106.09685)ã€‚

LoRA æœ‰å¾ˆå¤šçš„ä¼˜ç‚¹ï¼ŒèŠ‚çº¦æ˜¾å­˜ï¼Œè®­ç»ƒå¿«ï¼Œæ•ˆæœæŸå¤±è¾ƒå°ï¼ˆç›¸å¯¹äºå…¨å‚æ•°å¾®è°ƒï¼‰ï¼Œæ¨ç†çš„æ—¶å€™ä¸å¢åŠ è€—æ—¶ï¼Œå¯ä»¥åšä¸€ä¸ªæ’å…¥å¼ç»„ä»¶ä½¿ç”¨ã€‚ç¼ºç‚¹å½“ç„¶ä¹Ÿæœ‰ï¼Œé‚£å°±æ˜¯è¿˜æ˜¯ä¼šæœ‰ä¸€äº›æ•ˆæœçš„æŸå¤±ï¼ˆç¬‘ï¼‰ã€‚

> å‡å°‘æ˜¾å­˜å ç”¨çš„ä¸»è¦åŸå› æ˜¯è®­ç»ƒå‚æ•°å˜å°äº†ï¼ˆæ¯”å¦‚åªå¯¹ qkv å±‚åš LoRAï¼‰


>
> ä¸å–œæ¬¢çœ‹æ–‡å­—çš„åŒå­¦å¯ä»¥çœ‹ [Bç«™è§†é¢‘-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://www.bilibili.com/video/BV1fHmkYyE2w/),
> 
> æˆ–è€…è§†é¢‘å·ï¼šchaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹


## æ ¸å¿ƒåŸç†

æ ¸å¿ƒåŸç†éå¸¸çš„ç®€å•ï¼Œä»»æ„ä¸€ä¸ªçŸ©é˜µ $W_0$ï¼Œéƒ½å¯ä»¥å¯¹å®ƒè¿›è¡Œä½ç§©åˆ†è§£ï¼ŒæŠŠä¸€ä¸ªå¾ˆå¤§çš„çŸ©é˜µæ‹†åˆ†æˆä¸¤ä¸ªå°çŸ©çŸ©é˜µ[^1]ï¼ˆ$A,B$ï¼‰ï¼Œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä¸å»æ”¹å˜ $W_0$ å‚æ•°ï¼Œè€Œæ˜¯å»æ”¹å˜ $A B$ã€‚å…·ä½“å¯ä»¥è¡¨ç¤ºä¸º

$$W_{new} = W_0 + AB \tag{1}$$

æœ€ç»ˆåœ¨è®­ç»ƒè®¡ç®—çš„æ—¶å€™æ˜¯

$$h = W_0x + ABx = (W_0 + AB)x\tag{2}$$

ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼ŒAB ä¼šè¿›è¡Œä¸€å®šçš„ç¼©æ”¾ï¼Œä½¿ç”¨ $\frac{\alpha}{r}$ ä½œä¸ºç¼©æ”¾å› å­ï¼Œæ‰€ä»¥æœ€ç»ˆä¼šå†™æˆ

$$h = (W_0 + \frac{\alpha}{r}AB)x\tag{3}$$

$$\text{s.t.} \quad W_0 \in \mathbb{R}^{n \times m}, \; A \in \mathbb{R}^{n \times r}, \; B \in \mathbb{R}^{r \times m}$$

å…¶ä¸­ $r << n \text{ and } r << m$ï¼Œ$r$ ç”šè‡³å¯ä»¥è®¾ç½®æˆ 1ã€‚


- ä¸ºä»€ä¹ˆè¯´åªä¼˜åŒ– AB ä¸¤ä¸ªçŸ©é˜µå°±å¯ä»¥äº†å‘¢ï¼Ÿè¿™é‡Œé¢çš„å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿ
- $W$ ä¸æ˜¯æ»¡ç§©çš„ï¼Œé‡Œé¢æœ‰å¤§é‡å‚æ•°æ˜¯å†—ä½™çš„ï¼Œé‚£ä¹ˆå…¶å®å¯ä»¥ç”¨æ›´æ¥è¿‘æ»¡ç§©çš„çŸ©é˜µ AB ä»£æ›¿ã€‚
> çŸ©é˜µéƒ½å¯ä»¥è¡¨ç¤ºä¸ºè‹¥å¹²ä¸ªçº¿æ€§æ— å…³å‘é‡ï¼Œæœ€å¤§çš„çº¿æ€§æ— å…³å‘é‡ä¸ªæ•°å°±æ˜¯ç§©


## PyTorch ä»£ç å®ç°
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class LinearLoRALayer(nn.Module):
    def __init__(self, 
        in_features, 
        out_features,
        merge=False,
        rank=8,
        lora_alpha=16,
        dropout=0.1,
    ):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.merge = merge
        self.rank = rank

        # linear weight çš„ Shape æ˜¯ (out_features, in_features), æ­£ç¡®çš„åšæ³•æ˜¯ xW^T
        self.linear = nn.Linear(in_features, out_features)
        # è¿™é‡Œéå¸¸çš„é‡è¦ï¼Œè¿™é‡Œæ˜¯å®ç°çš„å°ç»†èŠ‚
        
        if rank > 0:
            # è¿™é‡Œæ˜¯ä¸ºäº†æ ‡è®° lora_a å’Œ lora_b æ˜¯å¯è®­ç»ƒçš„å‚æ•°
            self.lora_a = nn.Parameter(
                torch.zeros(out_features, rank)
            )
            # lora_a éœ€è¦åˆå§‹åŒ–ä¸º é«˜æ–¯åˆ†å¸ƒ
            # @æ˜¥é£å½’æ— æœŸ æé†’æˆ‘ @ç”¨ä»£ç æ‰“ç‚¹é…±æ²¹çš„chaofa : åœ¨è°ƒç”¨å‡¯æ˜åˆå§‹åŒ–çš„æ—¶å€™æ³¨é‡Šé‡Œå†™çš„é«˜æ–¯åˆ†å¸ƒï¼Œè°ƒç”¨çš„å´æ˜¯å‡åŒ€åˆ†å¸ƒï¼Œè€Œä¸”å‚æ•°açš„å€¼è®¾ç½®çš„æ˜¯æ ¹å·5ï¼Œä½†aè¡¨ç¤ºçš„æ˜¯leaky reluçš„è´Ÿæ–œç‡ç³»æ•°ï¼Œä¸€èˆ¬æ˜¯0.01è¿™æ ·çš„å°å€¼ï¼Œä¸å¯èƒ½è¶…è¿‡1
            nn.init.kaiming_normal_(self.lora_a, a=0.01)

            self.lora_b = nn.Parameter(
                torch.zeros(rank, in_features)
            )
            self.scale = lora_alpha / rank

            # linear éœ€è¦è®¾ç½®ä¸ºä¸å¯ä»¥è®­ç»ƒ
            self.linear.weight.requires_grad = False
            self.linear.bias.requires_grad = False
        
        self.dropout = nn.Dropout(
            dropout
        ) if dropout > 0 else nn.Identity()

        # å¦‚æœé‡‡ç”¨ merge è¿›è¡Œæ¨ç†ï¼Œ
        # é‚£ä¹ˆä¼šæŠŠ lora_a å’Œ lora_b ä¸¤ä¸ªå°çŸ©é˜µçš„å‚æ•°ç›´æ¥æ”¾åˆ° linear.weight ä¸­
        if merge:
            self.merge_weight()

    
    def forward(self, X):
        # X shape is (batch, seq_len, in_feature)
        # lora_a æ˜¯ out_features * rank
        if self.rank > 0 and not self.merge:
            output = self.linear(X) + self.scale * ( X @ (self.lora_a @ self.lora_b).T )
        elif self.rank > 0 and self.merge:
            output = self.linear(X)
        else:
            output = self.linear(X)
        
        return self.dropout(output)

    def merge_weight(self, ):
        if self.merge and self.rank > 0:
            self.linear.weight.data += self.scale * (self.lora_a @ self.lora_b)
    
    def unmerge_weight(self, ):
        if self.rank > 0:
            self.linear.weight.data -= self.scale * (self.lora_a @ self.lora_b)


# å†™ä¸€æ®µæµ‹è¯•ä»£ç 
# Test the LoRALinear layer
batch_size = 32
seq_len = 128
in_features = 768
out_features = 512
rank = 8
lora_alpha = 16
dropout = 0.1

# Create a test input
x = torch.randn(batch_size, seq_len, in_features)

# Test regular mode (no merge)
lora_layer = LinearLoRALayer(
    in_features=in_features,
    out_features=out_features,
    rank=rank,
    lora_alpha=lora_alpha,
    dropout=dropout,
    merge=False
)

# Forward pass
output = lora_layer(x)
print(f"Output shape (no merge): {output.shape}")  # Should be [batch_size, seq_len, out_features]

# Test merged mode
lora_layer_merged = LinearLoRALayer(
    in_features=in_features,
    out_features=out_features,
    rank=rank,
    lora_alpha=lora_alpha,
    dropout=dropout,
    merge=True
)

# Forward pass with merged weights
output_merged = lora_layer_merged(x)
print(f"Output shape (merged): {output_merged.shape}")  # Should be [batch_size, seq_len, out_features]

# Test weight merging/unmerging
lora_layer.merge_weight()
output_after_merge = lora_layer(x)
lora_layer.unmerge_weight()
output_after_unmerge = lora_layer(x)

print("Max difference after merge/unmerge cycle:", 
      torch.max(torch.abs(output - output_after_unmerge)).item())


```

- Q: å¤§æ¨¡å‹çš„ LoRA å®ç°çœŸçš„è¿™ä¹ˆç®€å•å—ï¼Ÿ
- A: åŸç†æ˜¯è¿™ä¹ˆç®€å•ï¼Œä½†æ˜¯å®é™…å®ç°è¿‡ç¨‹ä¸­å› ä¸ºå±‚å¾ˆå¤šï¼Œä¼šæœ‰ä¸€äº›é…ç½®ï¼Œæ¯”å¦‚ QKV å±‚åš LoRA è¿˜æ˜¯ FFN å±‚åš LoRAï¼Œè¿™äº›éƒ½ä¼šå¢åŠ ä»£ç çš„å¤æ‚æ€§ï¼Œä½†æ˜¯æ ¸å¿ƒåŸç†å°±æ˜¯ä¸Šé¢çš„ä»£ç ã€‚


## References

[^1]: è¿™é‡Œå’ŒPCA,SVD æœ‰ä¸€äº›å·®åˆ«ã€‚å‰è€…æ˜¯ä¸ºäº†æ®é™ç»´/å‹ç¼©ï¼Œåè€…ä»…ä»…æ˜¯ä¸ºäº†å­¦ä¹ ä½ç§©çš„çŸ©é˜µï¼ˆå‚æ•°å¯ä»¥æ›´æ–°æ”¹å˜ï¼‰

æ„Ÿå…´è¶£å¯ä»¥é˜…è¯»æˆ‘çš„å…¶ä»–æ–‡ç« ï¼š
- [ä» self-attention åˆ° multi-head self-attention](/hands-on-code/from-self-attention-to-multi-head-self-attention.html)
- [æ‰‹å†™ transformer decoderï¼ˆCausalLMï¼‰](/hands-on-code/hands-on-causallm-decoder.html)
- [LLM å¤§æ¨¡å‹è®­ç»ƒ-æ¨ç†æ˜¾å­˜å ç”¨åˆ†æ](/post/llm-train-infer-memoery-usage-calculation.html)
- [æ‰‹å†™å¤§æ¨¡å‹ç»„ä»¶ä¹‹Group Query Attentionï¼Œä» MHAï¼ŒMQA åˆ° GQA](https://bruceyuan.com/hands-on-code/hands-on-group-query-attention-and-multi-query-attention.html)


## äº¤ä¸ªæœ‹å‹ğŸ¤£
æœ€åæ¬¢è¿å…³æ³¨æˆ‘ï¼ŒåŸºæœ¬å…¨ç½‘åŒå [chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://bruceyuan.com/)
- å…¬ä¼—å·ï¼š ![chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://bruceyuan.com/llms-zero-to-hero/chaofa-wechat-official-account.png)
- [Bç«™-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://space.bilibili.com/12420432)
- [YouTube-chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://www.youtube.com/@bbruceyuan)
- [chaofa çš„ notion ç®€ä»‹](https://chaofa.notion.site/11a569b3ecce49b2826d679f5e2fdb54)
